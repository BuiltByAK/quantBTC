---
title: "kdakd"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).

BTC, ETH, XRP, BNB, SOL, DOGE, ADA, TRX, STETH, WBTC, TON, LINK, LEO, AVAX, XLM

```{r}
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(lubridate)
library(readr)

# Define function to fetch each metric
get_metric <- function(endpoint) {
  url <- paste0("https://blockchain.info/q/", endpoint)
  response <- GET(url)
  value <- as.numeric(content(response, "text", encoding = "UTF-8"))
  return(value)
}

# Pull metrics from Blockchain.com API
timestamp <- Sys.time()

block_height <- get_metric("getblockcount")
hash_rate <- get_metric("hashrate")              # GH/s
difficulty <- get_metric("getdifficulty")
market_cap <- get_metric("marketcap") / 1e9      # Convert to billions USD
total_btc <- get_metric("totalbc") / 1e8         # BTC in circulation
unconfirmed_tx <- get_metric("unconfirmedcount")
tx_count_24h <- get_metric("24hrtransactioncount")

# Create a data frame with this snapshot
snapshot <- data.frame(
  timestamp = timestamp,
  block_height = block_height,
  hash_rate_GHs = hash_rate,
  difficulty = difficulty,
  market_cap_BUSD = market_cap,
  total_btc = total_btc,
  unconfirmed_tx = unconfirmed_tx,
  tx_count_24h = tx_count_24h
)

# View it
print(snapshot)

# Load existing data if available
file_path <- "btc_network_stats.csv"
if (file.exists(file_path)) {
  existing_data <- read_csv(file_path)
  updated_data <- bind_rows(existing_data, snapshot)
} else {
  updated_data <- snapshot
}

# Save updated data
write_csv(updated_data, file_path)

```

```         
ETH, XRP, BNB, SOL, DOGE, ADA, TRX, STETH, WBTC, TON, LINK, LEO, AVAX, XLM, SUI, WSTETH, USDS, SHIB, HBAR, LTC
```

OM, ICP, INJ

```{r}
##devtools::install_github("coinmetrics/api-client-r")
#library(coinmetrics)
#No SOL, no SELTH, no ton, no leo, no avax, no SUI, no WSTETH, no USDS, no SHIB, no HBAR





btc_data <- get_asset_metrics(
  assets = c("btc", "eth", "xrp", "bnb", "doge", "ada", "trx", "wbtc", "link", "ltc", "xlm", "inj"),
  metrics = "PriceUSD",
  start_time = "2013-01-01",
  end_time = Sys.Date()
)

all_symbols_cm
```

# **Final look:**

```{r}
# --- 0. Setup: Install and Load Libraries ---



# Ensure necessary packages are installed, e.g., install.packages("tidyverse")


library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma)
library(lubridate)
library(httr)
library(jsonlite)
library(coinmetrics)
library(slider)
library(PerformanceAnalytics)
library(gridExtra)
library(doParallel)
library(foreach)
library(scales)
library(crypto2)

# Set global timezone to UTC for consistency
Sys.setenv(TZ='UTC')

# --- 1. Parameters ---

# --- Data & Timeframe Settings ---
N_TOP_COINS          <- 100   # Number of top coins (excluding BTC and stablecoins) to consider
HISTORY_YEARS        <- 6     # Total years of historical data to fetch
BACKTEST_YEARS       <- 2     # Years of data reserved for backtesting
END_DATE             <- Sys.Date() # End date for data fetching (today)
START_DATE           <- END_DATE - years(HISTORY_YEARS) # Calculate start date
BACKTEST_START_DATE  <- END_DATE - years(BACKTEST_YEARS) # Calculate backtest start date
TRAINING_END_DATE    <- BACKTEST_START_DATE - days(1) # End date for the training period

# --- Filtering Settings ---
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # Symbols to exclude

# Explicit list of assets (lowercase) known NOT to be well-supported by CoinMetrics API
# Add symbols here if they consistently cause fetch errors or lack required metrics (e.g., AdrActCnt)
CM_UNSUPPORTED_ASSETS <- c(
    "sol", "steth", "ton", "leo", "avax", "sui", "wsteth", "usds", "pi",
    "weth", "usde", "bsc-usd", "hype", "weeth", "wbt", "susds", "cbtc",
    "susde", "buidl", "lbtc", "kas", "pol", "s", "ftn", "arb", "ip",
    "solvbtc", "rseth", "move", "bnsol", "wld", "dexe", "bonk", "sei",
    "usdt0", "reth", "usd0", "bera", "usdc.e", "flr", "solvbtc.bbn", "pyusd",
    "wbnb", "om", "inj" # <<< Added om, inj
)

# --- API Key Configuration ---
# CoinMetrics API Key: Set as environment variable CM_API_KEY
# The R client primarily uses the environment variable.
CM_API_KEY <- Sys.getenv("CM_API_KEY")
if (CM_API_KEY == "") {
  warning("CoinMetrics API Key (CM_API_KEY) not found or is blank. Using free tier access.\nRate limits and metric availability will be restricted.", immediate. = TRUE)
} else {
  cat("Attempting to use CoinMetrics API Key found in environment variable CM_API_KEY.\n")
}

# --- Analysis Thresholds ---
VIF_THRESHOLD          <- 5     # Variance Inflation Factor threshold (for multicollinearity - not used in this script version)
ADF_P_VALUE_THRESHOLD  <- 0.05  # Augmented Dickey-Fuller test p-value threshold for stationarity (mean-reversion)
HURST_THRESHOLD        <- 0.5   # Hurst exponent threshold (values < 0.5 suggest mean-reversion)

# --- Core Strategy Settings ---
INITIAL_ROLLING_WINDOW <- 30    # Initial rolling window for Z-score calculation (days)
INITIAL_ZSCORE_ENTRY   <- 1.5   # Initial Z-score threshold to enter a trade
INITIAL_ZSCORE_EXIT    <- 0.5   # Initial Z-score threshold to exit a trade
SD_OFFSET              <- 1e-9  # Small offset to avoid division by zero with standard deviation

# --- Position Sizing Settings ---
INITIAL_MAX_POSITION_SCALAR <- 2.0 # Initial max multiplier for position size based on Z-score magnitude
INITIAL_SCALING_FACTOR      <- 0.5 # Initial factor determining how much Z-score magnitude scales position size

# --- Regime Filter Settings ---
REGIME_FILTER_ENABLED <- TRUE   # Master switch for the regime filter
REGIME_VOL_WINDOW     <- 60    # Rolling window for BTC volatility calculation (days)
REGIME_ADR_WINDOW     <- 30    # Rolling window for BTC active address momentum calculation (days)
REGIME_VOL_PERCENTILE <- 0.75  # Percentile threshold for high volatility regime
REGIME_ADR_PERCENTILE <- 0.25  # Percentile threshold for low address momentum regime (potential risk-off)

# --- Optimization Settings ---
OPTIMIZE_PARAMETERS <- TRUE   # Whether to run parameter optimization on the training set
OPTIMIZATION_METRIC <- "Sharpe" # Metric to optimize ("Sharpe" or "Calmar")
STARTING_BALANCE    <- 500000 # Hypothetical starting balance for equity curve plots

# --- Performance & Caching Settings ---
N_CORES <- detectCores() - 1    # Number of CPU cores for parallel processing
if (N_CORES < 1) N_CORES <- 1
CACHE_DIR          <- "data_cache" # Directory to store cached data files
MAX_CACHE_AGE_DAYS <- 1          # Maximum age (in days) for cache files to be considered valid
FORCE_REFRESH      <- FALSE      # Set to TRUE to ignore existing cache and force data re-download

# Create cache directory if it doesn't exist
if (!dir.exists(CACHE_DIR)) {
  dir.create(CACHE_DIR, showWarnings = FALSE, recursive = TRUE)
  cat("Cache directory created:", file.path(getwd(), CACHE_DIR), "\n")
}

# --- Function Definition: calculate_safe_metrics ---

#' Calculate Performance Metrics Safely
#'
#' Calculates common performance metrics, handling cases with insufficient data
#' or zero standard deviation.
#'
#' @param returns_xts An xts object containing daily strategy returns.
#' @param trade_data A data frame containing trade signals/positions, aligned with `returns_xts`.
#'                   Must contain a 'position' column if Win%/PctInMarket are desired.
#' @param scale Annualization factor (default: 252 trading days).
#' @param Rf Risk-free rate (default: 0).
#'
#' @return A list containing: AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn.
#'         Metrics that cannot be calculated reliably (e.g., due to insufficient data or zero SD) will be NA.
calculate_safe_metrics <- function(returns_xts, trade_data = NULL, scale = 252, Rf = 0) {
    # Initialize metrics list with NAs
    metrics <- list(
        AnnReturn = NA_real_, AnnStdDev = NA_real_, Sharpe = NA_real_,
        MaxDD = NA_real_, Calmar = NA_real_, WinPerc = NA_real_,
        PctInMarket = NA_real_, CumReturn = NA_real_
    )

    # Basic check for sufficient data
    if (is.null(returns_xts) || !inherits(returns_xts, "xts") || nrow(returns_xts) < 20) {
        # Attempt to calculate cumulative return even with few data points
        tryCatch({
            metrics$CumReturn <- as.numeric(PerformanceAnalytics::Return.cumulative(returns_xts, geometric = FALSE))
        }, error = function(e) {
             # message("Note: Could not calculate CumReturn due to insufficient data.") # Less verbose
        })
        return(metrics)
    }

    # Pre-calculate standard deviation
    sd_val <- sd(coredata(returns_xts), na.rm = TRUE)

    # Calculate core metrics (less sensitive to zero SD)
    tryCatch({
        metrics$MaxDD <- as.numeric(PerformanceAnalytics::maxDrawdown(returns_xts))
    }, error = function(e) {
        warning("Error calculating Max Drawdown: ", e$message, call. = FALSE)
    })
    tryCatch({
        tbl <- PerformanceAnalytics::table.AnnualizedReturns(returns_xts, scale = scale, geometric = FALSE)
        metrics$AnnReturn <- as.numeric(tbl[1, 1])
        metrics$AnnStdDev <- as.numeric(tbl[2, 1])
    }, error = function(e) {
        warning("Error calculating Annualized Return/StdDev: ", e$message, call. = FALSE)
    })
    tryCatch({
        metrics$CumReturn <- as.numeric(PerformanceAnalytics::Return.cumulative(returns_xts, geometric = FALSE))
    }, error = function(e) {
        warning("Error calculating Cumulative Return: ", e$message, call. = FALSE)
    })

    # Calculate trade-based metrics if trade_data is valid and aligns
    if (!is.null(trade_data) && "position" %in% names(trade_data) && nrow(trade_data) == nrow(returns_xts)) {
         positions <- trade_data$position[!is.na(trade_data$position)]
         returns_coredata <- coredata(returns_xts) # Ensure alignment
         # Check if lengths still match after potential NA removal in returns/positions
         if (length(positions) == length(returns_coredata)) {
              returns_in_market <- returns_coredata[positions != 0]
              if (length(returns_in_market) > 0) {
                  metrics$WinPerc <- mean(returns_in_market > 0, na.rm = TRUE)
              } else {
                  metrics$WinPerc <- NA_real_ # No trades taken
              }
              metrics$PctInMarket <- mean(positions != 0, na.rm = TRUE)
         } else {
              # warning("Length mismatch between returns and positions after NA handling. Cannot calculate Win%/PctInMarket.", call. = FALSE) # Less verbose
              metrics$WinPerc <- NA_real_
              metrics$PctInMarket <- NA_real_
         }
    } else if (!is.null(trade_data)) {
         # warning("Trade data provided but invalid or misaligned. Cannot calculate Win%/PctInMarket.", call. = FALSE) # Less verbose
    }

    # Calculate metrics sensitive to standard deviation (only if SD > offset)
    if (!is.na(sd_val) && sd_val > SD_OFFSET) {
        tryCatch({
            shp <- PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = Rf, scale = scale, geometric = FALSE)
            metrics$Sharpe <- as.numeric(shp[1, ])
        }, error = function(e) {
            warning("Error calculating Sharpe Ratio: ", e$message, call. = FALSE)
        })
        tryCatch({
            metrics$Calmar <- as.numeric(PerformanceAnalytics::CalmarRatio(returns_xts, scale = scale))
        }, error = function(e) {
            warning("Error calculating Calmar Ratio: ", e$message, call. = FALSE)
        })
    } else {
        # Handle zero/tiny SD case
        metrics$Sharpe <- NA_real_ # Sharpe is undefined with zero SD
        # Attempt manual Calmar calculation if AnnReturn and MaxDD are valid
        if (!is.na(metrics$AnnReturn) && !is.na(metrics$MaxDD) && metrics$MaxDD != 0) {
           tryCatch({
               metrics$Calmar <- metrics$AnnReturn / abs(metrics$MaxDD)
           }, error = function(e) {
               metrics$Calmar <- NA_real_
           })
        } else {
           metrics$Calmar <- NA_real_
        }
        # message("Note: Zero or near-zero standard deviation detected. Sharpe/Calmar set to NA or calculated manually.") # Less verbose
    }
    return(metrics)
}
# --- END Function Definition ---


# --- 2. Data Acquisition Part 1: Basic Price Data (CryptoCompare via httr) ---
cat("--- Step 2: Acquiring Basic Price Data ---\n")

# --- Get Top Coins Ranking (CoinGecko) ---
cg_cache_file <- file.path(CACHE_DIR, paste0("coingecko_ranking_cache_N", N_TOP_COINS, ".rds"))
top_coins_df <- NULL

if (!FORCE_REFRESH && file.exists(cg_cache_file)) {
    cat("Loading CoinGecko ranking from cache:", cg_cache_file, "\n")
    top_coins_df <- tryCatch(readRDS(cg_cache_file), error = function(e) {
        cat(" Error loading CoinGecko cache:", e$message, "\n"); NULL
    })
}

if (is.null(top_coins_df)) {
    cat("Fetching coin list and market caps from CoinGecko...\n")
    tryCatch({
        # Request slightly more coins to ensure enough candidates after filtering
        cg_per_page <- min(250, N_TOP_COINS + length(STABLECOINS) + 30)
        cg_url <- "https://api.coingecko.com/api/v3/coins/markets"
        cg_params <- list(vs_currency = "usd", order = "market_cap_desc",
                          per_page = cg_per_page, page = 1, sparkline = "false")
        cg_response <- httr::GET(cg_url, query = cg_params)
        Sys.sleep(2) # Be polite to the API

        if (httr::status_code(cg_response) == 200) {
            cg_content <- httr::content(cg_response, "text", encoding = "UTF-8")
            cg_data <- jsonlite::fromJSON(cg_content)
            top_coins_df <- cg_data %>%
                as_tibble() %>%
                select(symbol, id, market_cap) %>%
                mutate(symbol = toupper(symbol)) # Standardize to uppercase
            cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
            # Save to cache
            tryCatch(saveRDS(top_coins_df, file = cg_cache_file), error = function(e) {
                cat(" Error saving CoinGecko cache:", e$message, "\n")
            })
        } else {
            stop("Failed to fetch data from CoinGecko. Status: ", httr::status_code(cg_response))
        }
    }, error = function(e) {
        cat("Error during CoinGecko fetch:", e$message, "\n")
        stop("Cannot proceed without coin ranking.")
    })
}

# --- Filter for Initial Candidate Symbols ---
cat("Filtering for initial prime candidates...\n")
prime_candidates_info <- top_coins_df %>%
    filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% # Exclude stables and BTC itself
    slice_head(n = N_TOP_COINS - 1) # Keep top N-1 (since BTC is added separately)

initial_prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
initial_all_symbols_of_interest <- c(btc_symbol, initial_prime_candidate_symbols)

if (length(initial_prime_candidate_symbols) < (N_TOP_COINS * 0.8)) { # Check if significantly fewer candidates
    warning("Fetched fewer candidates than expected: ", length(initial_prime_candidate_symbols), immediate. = TRUE)
}
cat("Initial", length(initial_prime_candidate_symbols), "candidates identified for basic fetch:",
    paste(initial_prime_candidate_symbols, collapse=", "), "\n")

# --- Fetch Basic Historical Price Data (CryptoCompare) ---
basic_data_cache_file <- file.path(CACHE_DIR, paste0("basic_price_data_N", N_TOP_COINS, "_", format(START_DATE),"_",format(END_DATE),".rds"))
all_data_list_basic <- list()
cache_valid_basic <- FALSE

if (!FORCE_REFRESH && file.exists(basic_data_cache_file)) {
    cache_info <- file.info(basic_data_cache_file)
    cache_age <- Sys.time() - cache_info$mtime
    if (cache_age < duration(days = MAX_CACHE_AGE_DAYS)) {
        cat("Loading basic price data from cache:", basic_data_cache_file, "\n")
        all_data_list_basic <- tryCatch(readRDS(basic_data_cache_file), error = function(e) {
            cat(" Error loading basic price cache:", e$message, "\n"); list()
        })
        if (length(all_data_list_basic) > 0) {
            cache_valid_basic <- TRUE
        } else {
            all_data_list_basic <- list() # Ensure it's an empty list if cache read failed
        }
    } else {
        cat("Basic price cache is outdated.\n")
    }
}

if (!cache_valid_basic) {
    cat("Fetching", HISTORY_YEARS, "years historical PRICE data via httr/jsonlite for ~",
        length(initial_all_symbols_of_interest), "symbols...\n")
    not_found_symbols_basic <- c()
    base_url_cc <- "https://min-api.cryptocompare.com/data/v2/histoday"
    # CryptoCompare API limit per request (max 2000 data points)
    api_limit_cc <- min(2000, as.numeric(END_DATE - START_DATE) + 1) # +1 to be safe

    for (sym in initial_all_symbols_of_interest) {
        cat("Fetching price data for:", sym, "...")
        tryCatch({
            query_params <- list(fsym = sym, tsym = "USD", limit = api_limit_cc,
                                 toTs = as.numeric(as.POSIXct(END_DATE))) # Use END_DATE
            response <- httr::GET(base_url_cc, query = query_params)
            Sys.sleep(1.5) # Be polite

            if (httr::status_code(response) == 200) {
                content <- httr::content(response, "text", encoding = "UTF-8")
                json_data <- jsonlite::fromJSON(content)

                if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                    df <- json_data$Data$Data %>% as_tibble()
                    # Check essential columns exist
                    if ("time" %in% names(df) && "close" %in% names(df)) {
                        df_processed <- df %>%
                            mutate(
                                timestamp = lubridate::as_datetime(time, tz = "UTC"), # Ensure UTC
                                date = as.Date(timestamp),
                                symbol = sym
                            ) %>%
                            select(date, symbol, price = close) %>%
                            filter(date >= START_DATE & date <= END_DATE) %>% # Filter exact date range
                            arrange(date)

                        if (nrow(df_processed) > 0) {
                            all_data_list_basic[[sym]] <- df_processed
                            cat(" Success.\n")
                        } else {
                            cat(" No data within specified date range. Skipping.\n")
                            not_found_symbols_basic <- c(not_found_symbols_basic, sym)
                        }
                    } else {
                        cat(" Missing required columns (time/close). Skipping.\n")
                        not_found_symbols_basic <- c(not_found_symbols_basic, sym)
                    }
                } else {
                    cat(" API Error:", json_data$Message, ". Skipping.\n")
                    not_found_symbols_basic <- c(not_found_symbols_basic, sym)
                }
            } else {
                cat(" HTTP Error:", httr::status_code(response), ". Skipping.\n")
                not_found_symbols_basic <- c(not_found_symbols_basic, sym)
            }
        }, error = function(e) {
            cat(" Error:", e$message, ". Skipping.\n")
            not_found_symbols_basic <- c(not_found_symbols_basic, sym)
            Sys.sleep(1.5) # Wait after error
        })
    } # End symbol loop

    # Save to cache if data was fetched successfully, especially BTC
    if (length(all_data_list_basic) > 0 && ("BTC" %in% names(all_data_list_basic))) {
        tryCatch(saveRDS(all_data_list_basic, file = basic_data_cache_file), error = function(e) {
            cat(" Error saving basic data cache:", e$message, "\n")
        })
        cat("Saved basic price data to cache:", basic_data_cache_file, "\n")
    } else {
        cat("WARNING: Failed to fetch sufficient basic data. Cache not saved.\n")
        if (!("BTC" %in% names(all_data_list_basic))) {
            stop("CRITICAL: Failed to fetch BTC basic price data. Cannot proceed.")
        }
    }
}

# --- Finalize Symbol Lists Based on Fetched Basic Data ---
successfully_fetched_basic <- names(all_data_list_basic)
initial_prime_candidate_symbols <- intersect(initial_prime_candidate_symbols, successfully_fetched_basic)

if (!("BTC" %in% successfully_fetched_basic)) {
    stop("CRITICAL: BTC basic price data missing after loading/fetching.")
}
if (length(initial_prime_candidate_symbols) == 0) {
    stop("CRITICAL: No candidate price data available after loading/fetching.")
}

cat("Using basic price data for BTC and", length(initial_prime_candidate_symbols), "candidates.\n")

# Combine list into a single long-format tibble
all_data_long_basic <- dplyr::bind_rows(all_data_list_basic) %>%
    arrange(symbol, date)


# --- 3. Basic Data Preparation ---
cat("\n--- Step 3: Calculating Returns from Basic Data ---\n")

all_data_calculated_basic <- all_data_long_basic %>%
    mutate(price = as.numeric(price)) %>% # Ensure numeric
    group_by(symbol) %>%
    arrange(date) %>%
    mutate(return = log(price) - log(lag(price))) %>% # Calculate log returns
    ungroup() %>%
    filter(!is.na(return)) # Remove first row NA return per symbol

cat("Log returns calculated.\n")


# --- 4. Training Period Analysis (Identify Potential Pairs using Basic Data) ---
cat("\n--- Step 4: Analyzing Training Period for Pair Identification (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")

# Filter data for the training period
training_data_basic <- all_data_calculated_basic %>%
    filter(date <= TRAINING_END_DATE)

pairs_trading_analysis_train <- list() # Store ADF/Hurst results
potentially_mean_reverting_pairs <- c() # Store symbols of potential pairs
training_pair_data_list_basic <- list() # Store the joined dataframes for potential pairs (training period only)

for (candidate_symbol in initial_prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training - Pair ID)\n")

    # Prepare data for the specific pair
    btc_data_train <- training_data_basic %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_train <- training_data_basic %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)

    # Check for sufficient overlapping data
    # Need at least INITIAL_ROLLING_WINDOW + buffer for stable tests
    if (nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) {
        cat("  Insufficient overlapping data in training period. Skipping pair.\n")
        next # Skip to the next candidate
    }

    # Store the joined data for this pair (used later in optimization if pair qualifies)
    training_pair_data_list_basic[[candidate_symbol]] <- pair_data_train

    # Calculate log spread (ensure prices > 0)
    pair_data_train_spread <- pair_data_train %>%
        filter(candidate_price > 0, btc_price > 0) %>%
        mutate(spread = log(candidate_price) - log(btc_price))

    # Perform Mean-Reversion Tests (ADF & Hurst)
    if (nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) { # Need enough data points for tests
        spread_vector_train <- na.omit(pair_data_train_spread$spread)

        # ADF Test
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL)
        adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA

        # Hurst Exponent
        hurst_result_train <- tryCatch(
            pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display = FALSE),
            error = function(e) NULL
        )
        hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA

        # Check if potentially mean-reverting based on thresholds
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) ||
                                   (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)

        # Store results
        pairs_trading_analysis_train[[candidate_symbol]] <- list(
            adf_p_value = adf_p_value_train,
            hurst_exponent = hurst_value_train,
            is_potentially_mean_reverting = is_mean_reverting_train
        )

        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4),
            "| Hurst:", round(hurst_value_train, 3), "\n")

        if (is_mean_reverting_train) {
            cat("  >>> Pair BTC-", candidate_symbol, " identified as potentially mean-reverting. <<<\n")
            potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol)
        }
    } else {
        cat("  Insufficient spread data points for mean-reversion tests.\n")
    }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
} # End loop through candidate symbols

cat("--- Training Period Pair Identification Complete ---\n")
if (length(potentially_mean_reverting_pairs) == 0) {
    stop("CRITICAL: No potentially mean-reverting pairs identified based on training data analysis.")
} else {
    cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")
}


# --- 4.1 Data Acquisition Part 2: Indicator Data (CoinMetrics) ---
cat("\n--- Step 4.1: Acquiring Indicator Data for Potential Pairs from CoinMetrics ---\n")

# --- Filter potential pairs based on CM_UNSUPPORTED_ASSETS list ---
# Use lowercase symbols for the API call, keep uppercase list for internal use
symbols_to_fetch_cm_lower <- setdiff(tolower(c(btc_symbol, potentially_mean_reverting_pairs)), CM_UNSUPPORTED_ASSETS)
symbols_for_indicators <- character(0) # Initialize empty

if (!"btc" %in% symbols_to_fetch_cm_lower) {
    cat("WARNING: BTC is listed as unsupported or failed basic fetch. Cannot get BTC indicators for regime filter.\n")
    # Proceed without BTC indicators, regime filter will likely be disabled later
    symbols_to_fetch_cm_lower <- setdiff(symbols_to_fetch_cm_lower, "btc")
    if (length(symbols_to_fetch_cm_lower) == 0) {
         cat("WARNING: No potential pairs remain after excluding unsupported assets and BTC.\n")
    } else {
         symbols_for_indicators <- toupper(symbols_to_fetch_cm_lower) # Keep uppercase list
         cat("Proceeding to fetch indicators for candidates only (no BTC):", paste(symbols_for_indicators, collapse=", "), "\n")
    }
} else if (length(symbols_to_fetch_cm_lower) <= 1 && "btc" %in% symbols_to_fetch_cm_lower) {
    cat("WARNING: Only BTC remains after filtering unsupported assets. No pairs to fetch indicators for.\n")
    symbols_for_indicators <- toupper(symbols_to_fetch_cm_lower) # Only BTC
} else {
    symbols_for_indicators <- toupper(symbols_to_fetch_cm_lower) # Convert back to UPPERCASE for internal consistency
    cat("Will attempt to fetch indicators for:", paste(symbols_for_indicators, collapse=", "), "\n")
}
# --- End Filter ---

# Define desired indicator metrics from CoinMetrics
metrics_indicators_fetch <- c("AdrActCnt", "TxCnt") # Active Addresses, Transaction Count
indicator_data_long <- NULL

# --- Caching for Indicator Data ---
indicator_cache_file <- file.path(CACHE_DIR, paste0("indicator_data_N", N_TOP_COINS, "_", format(START_DATE),"_",format(END_DATE),".rds"))
cache_valid_indicator <- FALSE

if (!FORCE_REFRESH && file.exists(indicator_cache_file)) {
    cache_info <- file.info(indicator_cache_file)
    cache_age <- Sys.time() - cache_info$mtime
    if (cache_age < duration(days = MAX_CACHE_AGE_DAYS)) {
        cat("Loading indicator data from cache:", indicator_cache_file, "\n")
        indicator_data_long <- tryCatch(readRDS(indicator_cache_file), error = function(e) {
            cat(" Error loading indicator cache:", e$message, "\n"); NULL
        })
        if (!is.null(indicator_data_long)) {
             # Minimal check: Does cache contain data for expected symbols?
             cached_symbols <- unique(indicator_data_long$symbol)
             if(all(symbols_for_indicators %in% cached_symbols)) {
                  cache_valid_indicator <- TRUE
                  # Subset cached data if needed (e.g., if N_TOP_COINS changed)
                  indicator_data_long <- indicator_data_long %>% filter(symbol %in% symbols_for_indicators)
             } else {
                  cat(" Indicator cache doesn't contain all required symbols. Refetching.\n")
                  indicator_data_long <- NULL # Invalidate partial cache
             }
        } else {
            indicator_data_long <- NULL # Ensure NULL if cache read failed
        }
    } else {
        cat("Indicator cache is outdated.\n")
    }
}

# Fetch from CoinMetrics if cache is invalid or forced refresh
if (!cache_valid_indicator) {
    if (length(symbols_to_fetch_cm_lower) > 0) { # Only fetch if there are symbols left
        cat("Fetching indicators via CoinMetrics R client for:", paste(symbols_for_indicators, collapse=", "), "\n") # Use UPPERCASE list for message
        cat("Metrics:", paste(metrics_indicators_fetch, collapse=", "), "\n")

        indicator_data_cm <- tryCatch({
            # Use environment variable CM_API_KEY implicitly (do not pass api_key argument)
            coinmetrics::get_asset_metrics(
                 assets = symbols_to_fetch_cm_lower, # Use LOWERCASE list for API
                 metrics = metrics_indicators_fetch,
                 start_time = format(START_DATE),
                 end_time = format(END_DATE),
                 frequency = "1d",
                 paging_from = "start"
            )
        }, error = function(e) {
            cat("Error fetching indicator data from CoinMetrics API:", e$message, "\n")
            NULL # Return NULL on error
        })

        # Process fetched data if successful
        if (!is.null(indicator_data_cm) && nrow(indicator_data_cm) > 0) {
            cat("Processing fetched indicator data...\n")
            indicator_data_long <- indicator_data_cm %>%
                mutate(date = as.Date(time)) %>% # Convert time to Date
                select(-time) %>%
                mutate(symbol = toupper(asset)) %>% # Standardize symbol to uppercase
                select(date, symbol, everything(), -asset) %>%
                rename( # Rename columns to snake_case for consistency
                    adr_act_cnt = AdrActCnt,
                    tx_cnt = TxCnt
                ) %>%
                # Ensure metrics are numeric, coercing errors to NA
                mutate(across(-c(date, symbol), ~ suppressWarnings(as.numeric(.)))) %>%
                arrange(symbol, date)

            # Save the processed data to cache
            tryCatch(saveRDS(indicator_data_long, file = indicator_cache_file), error = function(e) {
                cat(" Error saving indicator data cache:", e$message, "\n")
            })
            cat("Saved indicator data to cache:", indicator_cache_file, "\n")
        } else {
            cat("WARNING: No indicator data returned from CoinMetrics API fetch.\n")
            indicator_data_long <- NULL # Ensure it's NULL if fetch failed
        }
    } else {
         cat("Skipping CoinMetrics indicator fetch as no supported/required symbols were identified.\n")
         indicator_data_long <- NULL # Ensure NULL if skipped
    }
}

# --- Verify fetched indicators & disable regime filter if necessary ---
fetched_indicator_symbols <- if (!is.null(indicator_data_long)) unique(indicator_data_long$symbol) else character(0)
fetched_indicator_metrics <- if (!is.null(indicator_data_long)) setdiff(names(indicator_data_long), c("date", "symbol")) else character(0)

if (length(fetched_indicator_metrics) > 0) {
    cat("Successfully loaded/fetched indicators:", paste(fetched_indicator_metrics, collapse=", "),
        "for symbols:", paste(fetched_indicator_symbols, collapse=", "), "\n")
} else {
    cat("No indicator data available after fetch/cache attempt.\n")
}

# Check conditions for disabling the regime filter
if (REGIME_FILTER_ENABLED) {
    btc_indicators_available <- "BTC" %in% fetched_indicator_symbols &&
                               "adr_act_cnt" %in% fetched_indicator_metrics # Need AdrActCnt specifically for BTC
    if (!btc_indicators_available) {
        cat("WARNING: Required BTC indicator data (AdrActCnt) missing. Disabling regime filter.\n")
        REGIME_FILTER_ENABLED <- FALSE
    }
    # Note: Volatility is calculated from basic returns, so it's always available if BTC price is.
}


# --- 4.2 Combine Basic Data with Indicator Data ---
cat("\n--- Step 4.2: Combining Basic and Indicator Data ---\n")

if (!is.null(indicator_data_long) && nrow(indicator_data_long) > 0) {
    # Perform a left join to keep all basic data and add indicators where available
    all_data_combined <- left_join(all_data_calculated_basic, indicator_data_long, by = c("date", "symbol"))
    cat("Indicator data joined with basic price/return data.\n")
} else {
    cat("No indicator data available to join. Proceeding with basic data only.\n")
    # Add NA columns for indicators to maintain structure
    all_data_combined <- all_data_calculated_basic %>%
        mutate(adr_act_cnt = NA_real_, tx_cnt = NA_real_)
    # Ensure regime filter is disabled if it was previously enabled but indicators are missing
    if (REGIME_FILTER_ENABLED) {
        cat("Disabling Regime filter as indicator data is unavailable.\n")
        REGIME_FILTER_ENABLED <- FALSE
    }
}


# --- 4.3 Calculate Rolling Indicators & Refine Pairs ---
cat("\n--- Step 4.3: Calculating Rolling Indicators & Refining Pairs ---\n")

# Calculate rolling volatility (always needed if regime filter *might* be used)
# Calculate rolling address metrics (only if adr_act_cnt is available)
all_data_combined <- all_data_combined %>%
    group_by(symbol) %>%
    arrange(date) %>%
    mutate(
        # Rolling volatility of log returns
        rolling_vol = slider::slide_dbl(return, ~sd(.x, na.rm = TRUE), .before = REGIME_VOL_WINDOW - 1, .complete = TRUE),
        # Smoothed active addresses
        adr_act_smooth = if ("adr_act_cnt" %in% names(.)) {
                            slider::slide_dbl(adr_act_cnt, ~mean(.x, na.rm = TRUE), .before = REGIME_ADR_WINDOW - 1, .complete = TRUE)
                         } else { NA_real_ },
        # Momentum of smoothed active addresses (e.g., % change over window)
        adr_act_mom = if ("adr_act_smooth" %in% names(.)) {
                         (adr_act_smooth / lag(adr_act_smooth, REGIME_ADR_WINDOW)) - 1
                      } else { NA_real_ }
    ) %>%
    ungroup()

cat("Rolling volatility and address metrics calculated.\n")

# Refine the list of pairs for backtesting based on data availability and regime filter status
pairs_for_backtesting <- c() # Final list of pairs to use
training_pair_data_list_combined <- list() # Store training data for the *final* pairs
regime_vol_threshold_value <- NA # Initialize thresholds
regime_adr_mom_threshold_value <- NA

# Filter data needed for threshold calculation (training period only)
training_data_combined <- all_data_combined %>% filter(date <= TRAINING_END_DATE)

if (REGIME_FILTER_ENABLED) {
    cat("Regime Filter is ENABLED. Refining pairs based on indicator availability in training data...\n")

    # Check if BTC has sufficient data in the training period to calculate thresholds
    btc_regime_data_train <- training_data_combined %>% filter(symbol == btc_symbol)
    can_calculate_thresholds <- nrow(btc_regime_data_train) > max(REGIME_VOL_WINDOW, REGIME_ADR_WINDOW * 2 + 1) && # Need enough data points
                                "rolling_vol" %in% names(btc_regime_data_train) && sum(!is.na(btc_regime_data_train$rolling_vol)) > 10 && # Check vol column exists and has values
                                "adr_act_mom" %in% names(btc_regime_data_train) && sum(!is.na(btc_regime_data_train$adr_act_mom)) > 10    # Check mom column exists and has values

    if (can_calculate_thresholds) {
        # Calculate thresholds using BTC's training data
        regime_vol_threshold_value <- quantile(btc_regime_data_train$rolling_vol,
                                               probs = REGIME_VOL_PERCENTILE, na.rm = TRUE, type = 8)
        regime_adr_mom_threshold_value <- quantile(btc_regime_data_train$adr_act_mom,
                                                   probs = REGIME_ADR_PERCENTILE, na.rm = TRUE, type = 8)

        # Check if quantile calculation was successful
        if (is.na(regime_vol_threshold_value) || is.na(regime_adr_mom_threshold_value)) {
            cat("WARNING: Failed to calculate valid regime thresholds (resulted in NA). Disabling filter.\n")
            REGIME_FILTER_ENABLED <- FALSE
        } else {
            cat(" Regime Thresholds Calculated (Training Data): Vol >", round(regime_vol_threshold_value, 6),
                " AND AdrMom <", round(regime_adr_mom_threshold_value, 4), "\n")
            # Regime filter remains enabled. Now filter pairs based on data overlap.
            for (sym in potentially_mean_reverting_pairs) {
                 # Re-join data for this pair using the combined dataset for training period
                 btc_data_train_comb <- training_data_combined %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, rolling_vol, adr_act_mom) # Include regime indicators
                 candidate_data_train_comb <- training_data_combined %>% filter(symbol == sym) %>% select(date, candidate_price = price, candidate_return = return)
                 joined_data <- inner_join(btc_data_train_comb, candidate_data_train_comb, by = "date") %>% arrange(date)

                 # Check sufficient overlap *after* joining (might lose dates if one asset has gaps)
                 if (nrow(joined_data) > INITIAL_ROLLING_WINDOW + 60) {
                     training_pair_data_list_combined[[sym]] <- joined_data # Store data needed for optimization
                     pairs_for_backtesting <- c(pairs_for_backtesting, sym) # Add to final list
                 } else {
                     cat(" Skipping pair", sym, "- insufficient overlap after joining combined training data.\n")
                 }
            }
        }
    } else {
        cat("WARNING: Not enough valid BTC indicator data in the training period to calculate regime thresholds. Disabling filter.\n")
        REGIME_FILTER_ENABLED <- FALSE
    }
}

# Fallback if Regime Filter was disabled or no pairs qualified with it enabled
if (!REGIME_FILTER_ENABLED || length(pairs_for_backtesting) == 0) {
    if (REGIME_FILTER_ENABLED && length(pairs_for_backtesting) == 0) { # Check if filter was on but yielded no pairs
         cat("WARNING: No pairs had sufficient data overlap with BTC indicators. Disabling Regime Filter and using all potentially mean-reverting pairs.\n")
         REGIME_FILTER_ENABLED <- FALSE # Explicitly turn off
    } else {
         cat("Regime filter is disabled. Using all potentially mean-reverting pairs found in Step 4.\n")
    }
    pairs_for_backtesting <- potentially_mean_reverting_pairs # Use the list from basic analysis
    # Populate the training data list using the *basic* data stored earlier
    for (sym in pairs_for_backtesting) {
        if (sym %in% names(training_pair_data_list_basic)) {
            training_pair_data_list_combined[[sym]] <- training_pair_data_list_basic[[sym]]
        } else {
             cat("WARNING: Basic training data missing for potentially mean-reverting pair:", sym, "- cannot optimize/backtest this pair.\n")
             pairs_for_backtesting <- setdiff(pairs_for_backtesting, sym) # Remove if data is somehow missing
        }
    }
}

# Final check
if (length(pairs_for_backtesting) == 0) {
    stop("CRITICAL: No pairs available for optimization/backtesting after all filtering steps.")
} else {
     cat("Final list of pairs for optimization/backtesting:", paste(pairs_for_backtesting, collapse=", "), "\n")
}


# --- 4.5 Parameter Optimization (on Training Data) ---
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")

# Define the grid of parameters to test
# Focus on core strategy parameters (window, entry/exit Z) for this example
param_grid <- expand.grid(
    roll_window = seq(20, 40, by = 10),
    entry_z     = seq(1.5, 2.0, by = 0.25),
    exit_z      = seq(0.25, 0.75, by = 0.25),
    max_scalar  = INITIAL_MAX_POSITION_SCALAR, # Keep sizing params fixed for this optimization
    scale_factor= INITIAL_SCALING_FACTOR,
    stringsAsFactors = FALSE
) %>% filter(exit_z < entry_z) # Ensure exit threshold is inside entry threshold

cat("Total parameter combinations to test:", nrow(param_grid), "\n")

# Define the evaluation function (run strategy on one pair for given params)
# This function simulates the backtest logic but only on the training data portion
evaluate_params_on_train <- function(pair_symbol, pair_training_data, params) {
    # Apply strategy logic using 'params' (params$roll_window, etc.)
    # Return a single performance metric (e.g., Sharpe or Calmar)

    # --- Simplified Backtest Logic (Mirroring Step 5 but on training data) ---
    # Ensure prices are positive
    pair_training_data_filtered <- pair_training_data %>% filter(btc_price > 0, candidate_price > 0)

    if (nrow(pair_training_data_filtered) < params$roll_window + 5) return(NA_real_)

    signals <- pair_training_data_filtered %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = params$roll_window - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = params$roll_window - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        filter(!is.na(zscore)) # Start after rolling window is populated

    if (nrow(signals) < 2) return(NA_real_)

    trades <- signals %>%
         mutate(
            raw_signal = case_when(
                zscore > params$entry_z ~ -1, # Short spread
                zscore < -params$entry_z ~ 1, # Long spread
                abs(zscore) < params$exit_z ~ 0, # Exit
                TRUE ~ NA_real_ # Maintain position otherwise
            ),
            raw_signal = zoo::na.locf(raw_signal, na.rm = FALSE, fromLast = FALSE), # Fill NAs forward
            raw_signal = ifelse(is.na(raw_signal), 0, raw_signal), # Fill initial NAs with 0
            # NOTE: Regime filter NOT applied during optimization on training data
            # NOTE: Position sizing NOT applied during optimization (using fixed position=signal)
            position = lag(raw_signal, default = 0) # Position based on *previous* day's signal
        ) %>%
        mutate(position = as.numeric(position))

    # Calculate returns
    returns_tibble <- trades %>%
        mutate(
            candidate_return = as.numeric(candidate_return),
            btc_return = as.numeric(btc_return),
            strategy_return = position * (candidate_return - btc_return),
            strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0)
        ) %>%
        select(date, strategy_return)

    if (nrow(returns_tibble) < 20) return(NA_real_) # Need enough returns for metrics

    returns_xts <- xts::xts(returns_tibble$strategy_return, order.by = returns_tibble$date)

    # Use the safe metric calculator
    metrics <- calculate_safe_metrics(returns_xts, trade_data = trades) # Pass trades for Win% calc if needed

    # Return the desired optimization metric
    metric_value <- if (OPTIMIZATION_METRIC == "Sharpe") {
        metrics$Sharpe
    } else if (OPTIMIZATION_METRIC == "Calmar") {
        metrics$Calmar
    } else {
        NA_real_ # Default or error case
    }

    # Ensure finite value is returned, otherwise NA
    return(if (is.finite(metric_value)) metric_value else NA_real_)
}


# Run the optimization or use initial parameters
if (OPTIMIZE_PARAMETERS && nrow(param_grid) > 0) {
    cat("Starting parallel optimization across", N_CORES, "cores...\n")
    cl <- makeCluster(N_CORES)
    registerDoParallel(cl)

    # Use foreach for parallel execution
    optimization_results_list <- foreach(
        i = 1:nrow(param_grid),
        .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts", "zoo", "pracma"), # Include needed packages
        .combine = 'list',
        .errorhandling = 'pass' # Capture errors without stopping
    ) %dopar% {
        current_params <- param_grid[i, ]
        # Evaluate current params on each pair in the final training list
        pair_metrics <- sapply(pairs_for_backtesting, function(sym) {
            if (sym %in% names(training_pair_data_list_combined)) {
                 # Call the evaluation function
                 evaluate_params_on_train(sym, training_pair_data_list_combined[[sym]], current_params)
            } else {
                NA_real_ # Should not happen based on earlier checks, but safeguard
            }
        })
        # Aggregate results across pairs (e.g., mean of the metric)
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]
        aggregate_metric <- if (length(valid_metrics) > 0) mean(valid_metrics) else NA_real_

        # Return results for this parameter combination
        list(
            params = current_params,
            aggregate_metric = aggregate_metric,
            num_valid_pairs = length(valid_metrics)
        )
    } # End foreach loop

    stopCluster(cl)
    cat("\nOptimization loop finished.\n")

    # Process results
    # Filter out any iterations that resulted in errors
    valid_results <- Filter(function(x) !inherits(x, "error") && is.list(x) && !is.null(x$aggregate_metric) && is.finite(x$aggregate_metric), optimization_results_list)

    if (length(valid_results) > 0) {
        # Combine results into a data frame
        optimization_results_df <- purrr::map_dfr(valid_results, ~bind_cols(as_tibble(.x$params), tibble(aggregate_metric = .x$aggregate_metric, num_valid_pairs = .x$num_valid_pairs)))

        # Find the best parameters based on the aggregate metric
        best_result_row <- optimization_results_df %>%
            arrange(desc(aggregate_metric)) %>% # Higher is better
            head(1)

        if (nrow(best_result_row) > 0 && is.finite(best_result_row$aggregate_metric)) {
            best_params <- list(
                roll_window = best_result_row$roll_window,
                entry_z = best_result_row$entry_z,
                exit_z = best_result_row$exit_z,
                max_scalar = best_result_row$max_scalar, # Carry over fixed params
                scale_factor = best_result_row$scale_factor
            )
            cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n")
            print(bind_rows(best_params))
            cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")

            # Update global parameters with optimized values
            ROLLING_WINDOW <- best_params$roll_window
            ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z
            ZSCORE_EXIT_THRESHOLD <- best_params$exit_z
            MAX_POSITION_SCALAR <- best_params$max_scalar
            SCALING_FACTOR <- best_params$scale_factor

        } else {
            cat("WARNING: Optimization resulted in non-finite or no valid best metric. Using INITIAL parameters.\n")
            ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW
            ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY
            ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT
            MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR
            SCALING_FACTOR <- INITIAL_SCALING_FACTOR
        }
    } else {
        cat("WARNING: Optimization failed or yielded no valid results. Using INITIAL parameters.\n")
        ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW
        ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY
        ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT
        MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR
        SCALING_FACTOR <- INITIAL_SCALING_FACTOR
    }
} else {
    cat("Skipping optimization (OPTIMIZE_PARAMETERS=FALSE or no grid). Using INITIAL parameters.\n")
    ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW
    ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY
    ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT
    MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR
    SCALING_FACTOR <- INITIAL_SCALING_FACTOR
}


# --- 5. Backtesting Period (using Combined Data & Regime Filter) ---
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW,
    " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD,
    " MaxScalar=", MAX_POSITION_SCALAR, " ScaleFactor=", SCALING_FACTOR, "\n")
if(REGIME_FILTER_ENABLED) {
    cat("Regime Filter: ENABLED (Vol >", round(regime_vol_threshold_value, 6),
        "AND AdrMom <", round(regime_adr_mom_threshold_value, 4), " considered High Risk)\n")
} else {
    cat("Regime Filter: DISABLED\n")
}

# Get combined data including buffer needed for rolling calculations
# Buffer needs to accommodate the largest window used (Z-score, Vol, ADR)
buffer_days <- max(ROLLING_WINDOW, REGIME_VOL_WINDOW, REGIME_ADR_WINDOW * 2, na.rm = TRUE) # *2 for ADR mom lag
backtest_data_full_period_raw <- all_data_combined %>%
    filter(symbol %in% c(btc_symbol, pairs_for_backtesting)) %>%
    filter(date >= (BACKTEST_START_DATE - days(buffer_days))) %>% # Include buffer before backtest start
    arrange(symbol, date)

# Add market regime state for the backtest period using thresholds calculated from TRAINING data
# Only apply if the filter is enabled AND thresholds are valid
if (REGIME_FILTER_ENABLED && !is.na(regime_vol_threshold_value) && !is.na(regime_adr_mom_threshold_value)) {
    # Calculate regime based on BTC's indicators ONLY
    btc_regime_data_backtest <- backtest_data_full_period_raw %>%
      filter(symbol == btc_symbol) %>%
      mutate(
            # Check conditions based on calculated training thresholds
            is_high_vol = rolling_vol > regime_vol_threshold_value,
            is_low_adr_mom = adr_act_mom < regime_adr_mom_threshold_value,
            # Determine regime: High Risk if both conditions met, otherwise Normal
            market_regime = case_when(
                !is.na(is_high_vol) & !is.na(is_low_adr_mom) & is_high_vol & is_low_adr_mom ~ "High Risk",
                TRUE ~ "Normal" # Default to Normal if conditions not met or data is NA
            )
      ) %>% select(date, market_regime)

    # Join the calculated BTC regime state back to the main dataset
    backtest_data_full_period <- left_join(backtest_data_full_period_raw, btc_regime_data_backtest, by = "date") %>%
      # Fill any potential remaining NAs in market_regime (e.g., before BTC data starts) - default to Normal
      mutate(market_regime = ifelse(is.na(market_regime), "Normal", market_regime))

} else {
    # If filter disabled or thresholds invalid, assign "Normal" regime everywhere
    backtest_data_full_period <- backtest_data_full_period_raw %>%
        mutate(market_regime = "Normal") # Add dummy column
}

# Store detailed results (signals, positions, returns) for each pair
backtest_details <- list()

# Loop through the final list of pairs to backtest
for (candidate_symbol in pairs_for_backtesting) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")

    # Filter data for the current pair for the full period (including buffer)
    btc_data_backtest <- backtest_data_full_period %>%
        filter(symbol == btc_symbol) %>%
        select(date, btc_price = price, btc_return = return, market_regime) # Keep regime column

    candidate_data_backtest <- backtest_data_full_period %>%
        filter(symbol == candidate_symbol) %>%
        select(date, candidate_price = price, candidate_return = return)

    # Join BTC and candidate data
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>%
        filter(btc_price > 0, candidate_price > 0) %>% # Ensure positive prices
        arrange(date)

    # Check for sufficient data after join and filtering
    if (nrow(pair_data_backtest) < ROLLING_WINDOW + 5) {
        cat("  Insufficient overlapping data for Z-score calculation. Skipping pair.\n")
        next # Skip to next pair
    }

    # Calculate Spread, Z-Score
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            # Handle zero or NA standard deviation
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        # Filter dates >= actual backtest start date AND where zscore is calculable
        filter(date >= BACKTEST_START_DATE, !is.na(zscore))

     if (nrow(pair_data_signals) < 2) {
         cat("  Insufficient data points after Z-score calculation within backtest period. Skipping pair.\n")
         next
     }

     # Calculate Signals, Apply Regime Filter, Determine Position Size
     pair_data_trades <- pair_data_signals %>%
         mutate(
            # 1. Raw Signal based on Z-score thresholds
            raw_signal = case_when(
                zscore > ZSCORE_ENTRY_THRESHOLD ~ -1,  # Enter Short Spread
                zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, # Enter Long Spread
                abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, # Exit (Signal = 0)
                TRUE ~ NA_real_                      # Maintain current position (signal NA)
            ),
            # Carry forward the last non-NA signal (maintain position)
            raw_signal = zoo::na.locf(raw_signal, na.rm = FALSE, fromLast = FALSE),
            # Fill initial NAs (before first signal) with 0 (no position)
            raw_signal = ifelse(is.na(raw_signal), 0, raw_signal),
            # Lag the raw signal to know the signal *before* the current day
            prev_raw_signal = lag(raw_signal, default = 0),

            # 2. Apply Regime Filter (if enabled)
            # Prevent *entering* a new position if regime is "High Risk"
            # Allow exits and holding existing positions during "High Risk"
            current_signal = if (REGIME_FILTER_ENABLED) {
                                 ifelse(
                                     # Condition: Trying to enter (prev signal was 0, current is not) AND regime is High Risk
                                     prev_raw_signal == 0 & raw_signal != 0 & market_regime == "High Risk",
                                     0, # Override entry signal to 0 (no position)
                                     raw_signal # Otherwise, keep the raw signal
                                 )
                             } else {
                                 raw_signal # If filter disabled, current_signal is just raw_signal
                             },

            # 3. Determine Position based on *previous day's filtered* signal
            # The position for day T is determined by the signal at the end of day T-1
            prev_signal = lag(current_signal, default = 0),

            # 4. Calculate Position Size Scalar
            # Need lagged Z-score and lagged signals to determine size for *current* position
            prev_zscore = lag(zscore, default = 0),
            prev_prev_signal = lag(prev_signal, default = 0), # Need signal from T-2 for entry detection

            # Calculate dynamic scalar based on Z-score magnitude at entry
            size_scalar_raw = case_when(
                # Entering Short: prev signal is -1, prev_prev was 0
                prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                # Entering Long: prev signal is 1, prev_prev was 0
                prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                # Holding position: Use the scalar calculated at entry (lag the calculated scalar)
                prev_signal != 0 & prev_signal == prev_prev_signal ~ lag(
                     # Re-calculate scalar from previous step to lag correctly
                     pmax(1.0, # Ensure scalar is at least 1 if holding
                           case_when( lag(prev_signal, default=0) == -1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (lag(prev_zscore,default=0) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                      lag(prev_signal, default=0) == 1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (abs(lag(prev_zscore,default=0)) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                      TRUE ~ 1.0 ) # Default inside lag if needed
                     ), default=1.0), # Default scalar is 1 if first day holding
                # Exiting or Flat: Scalar is irrelevant (position is 0)
                TRUE ~ 1.0 # Default scalar is 1
            ),
            # Final position size: Signal * Scalar (ensure scalar is >= 1 if in position)
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar_raw)), # If flat, scalar is 0; else >= 1
            position = prev_signal * size_scalar
        ) %>%
        mutate(position = as.numeric(position)) # Ensure numeric type

    # Calculate Strategy Returns for this pair
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate(
            candidate_return = as.numeric(candidate_return),
            btc_return = as.numeric(btc_return),
            # Strategy return = Position * (Leg1 Return - Leg2 Return)
            # Here: Position * (Candidate Return - BTC Return)
            # Position > 0 means long Candidate, short BTC (expect spread to increase)
            # Position < 0 means short Candidate, long BTC (expect spread to decrease)
            strategy_return = position * (candidate_return - btc_return),
            # Handle potential Inf/-Inf/NA returns (replace with 0)
            strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0),
            symbol = candidate_symbol # Add symbol column for aggregation later
        ) %>%
        select(date, symbol, strategy_return) # Keep only essential columns for returns

    # Store results for this pair
    backtest_details[[candidate_symbol]] <- list(
        trade_data = pair_data_trades,         # Contains signals, zscores, positions, regime
        returns_data = pair_strategy_returns_tibble # Contains daily strategy returns
    )
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
} # End loop through pairs_for_backtesting


# --- 6. Performance Metrics & Drawdown Calculation ---
cat("\n--- Step 6: Calculating Performance Metrics ---\n")

if (length(backtest_details) == 0) {
    stop("CRITICAL: No pairs were successfully backtested. Cannot calculate performance.")
}

# --- 6a. Aggregate Performance (Equal Weight Portfolio) ---
# Combine returns from all successfully backtested pairs
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))

# Calculate average daily return across all active pairs
portfolio_daily_returns <- all_strategy_returns_long %>%
    group_by(date) %>%
    # Simple average return (equal weight) - replace NAs with 0 if a pair had no return on a day
    summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>%
    arrange(date) %>%
    # If *all* pairs had NA on a given day (unlikely), result is NaN; replace with 0
    mutate(portfolio_return = ifelse(is.nan(portfolio_return), 0, portfolio_return))

# Calculate aggregate metrics
if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts_agg <- xts::xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts_agg) <- "Aggregate Strategy"

    cat("\n--- AGGREGATE Backtest Performance Metrics ---\n")
    # Use the safe calculation function (no trade_data needed for aggregate)
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts_agg, trade_data = NULL) # No position data for aggregate

    cat(" Annualized Return:     ", scales::percent(agg_metrics$AnnReturn, accuracy=0.01), "\n")
    cat(" Annualized Std Dev:    ", scales::percent(agg_metrics$AnnStdDev, accuracy=0.01), "\n")
    cat(" Cumulative Return:     ", scales::percent(agg_metrics$CumReturn, accuracy=0.01), "\n")
    cat(" Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n")
    cat(" Maximum Drawdown:      ", scales::percent(agg_metrics$MaxDD, accuracy=0.01), "\n")
    cat(" Calmar Ratio:          ", round(agg_metrics$Calmar, 3), "\n")
    # Note: Win% and PctInMarket are not meaningful for the aggregate in this simple calc

} else {
    cat("\nError: Aggregate portfolio returns are empty. Cannot calculate aggregate metrics.\n")
    # Allow script to continue to individual metrics if possible
}

# --- 6b. Individual Pair Performance & Drawdowns ---
cat("\n--- INDIVIDUAL Pair Backtest Performance & Drawdowns ---\n")
individual_metrics_list <- list()
individual_drawdown_tables <- list()

for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data
    pair_trade_data <- backtest_details[[candidate_symbol]]$trade_data # Get trade data for Win%/PctInMarket

    if (nrow(pair_returns_tibble) > 1) {
        # Ensure NAs are 0 before converting to xts
        pair_returns_tibble <- pair_returns_tibble %>%
            mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return))
        pair_returns_xts <- xts::xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol

        # Calculate metrics using the safe function, passing trade_data
        pair_metrics <- calculate_safe_metrics(pair_returns_xts, pair_trade_data)
        individual_metrics_list[[candidate_symbol]] <- pair_metrics

        # Print metrics
        cat(" Annualized Return:      ", scales::percent(pair_metrics$AnnReturn, accuracy=0.01), "\n")
        cat(" Annualized Std Dev:     ", scales::percent(pair_metrics$AnnStdDev, accuracy=0.01), "\n")
        cat(" Cumulative Return:      ", scales::percent(pair_metrics$CumReturn, accuracy=0.01), "\n")
        cat(" Win % (of active days): ", scales::percent(pair_metrics$WinPerc, accuracy=0.1), "\n")
        cat(" % Time In Market:       ", scales::percent(pair_metrics$PctInMarket, accuracy=0.1), "\n")
        cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n")
        cat(" Maximum Drawdown:       ", scales::percent(pair_metrics$MaxDD, accuracy=0.01), "\n")
        cat(" Calmar Ratio:           ", round(pair_metrics$Calmar, 3), "\n")

        # Calculate and print drawdown table
        dd_table <- tryCatch(
            PerformanceAnalytics::table.Drawdowns(pair_returns_xts, top = 5),
            error = function(e) {
                cat(" Could not calculate drawdown table:", e$message, "\n")
                NULL
            }
        )
        if (!is.null(dd_table)) {
            cat(" Top 5 Drawdowns:\n")
            print(dd_table)
            individual_drawdown_tables[[candidate_symbol]] <- dd_table
        }
        cat("\n") # Add space between pairs

    } else {
        cat(" Insufficient return data points for performance calculation:", candidate_symbol, "\n\n")
    }
}

# --- 6c. Create Summary Table ---
summary_df <- tryCatch({
    bind_rows(lapply(individual_metrics_list, as.data.frame), .id = "Candidate") %>%
    mutate(Pair = paste("BTC", Candidate, sep="-")) %>%
    select(Pair, AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn) %>%
    arrange(desc(Sharpe)) # Arrange by Sharpe ratio
}, error = function(e) {
    cat("Error creating summary performance table:", e$message, "\n")
    NULL
})

if (!is.null(summary_df)){
    cat("\n--- Performance Summary Table (Backtest Period) ---\n")
    print(summary_df, digits = 3)
}


# --- 7. Plotting ---
cat("\n--- Step 7: Plotting Results ---\n")

# Create directory for plots if it doesn't exist
plot_dir <- "strategy_plots"
if (!dir.exists(plot_dir)) {
    dir.create(plot_dir)
}
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")

# --- Plot Aggregate Performance ---
if (exists("portfolio_returns_xts_agg") && inherits(portfolio_returns_xts_agg, "xts")) {
    agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png")
    png(agg_plot_file, width = 800, height = 1000)
    tryCatch({
        PerformanceAnalytics::charts.PerformanceSummary(
            portfolio_returns_xts_agg,
            main = "Aggregate Strategy Performance (Equal Weight)",
            geometric = FALSE # Use arithmetic returns for aggregation
        )
    }, error = function(e) {
        cat("Error generating aggregate performance plot:", e$message, "\n")
        plot.new() # Create blank plot on error
        title("Error Generating Aggregate Plot")
    })
    dev.off()
    cat("Aggregate performance plot saved:", agg_plot_file, "\n")
} else {
    cat("Skipping aggregate performance plot (data missing or invalid).\n")
}


# --- Plot Individual Pair Details ---
# Get BTC prices for the backtest period (needed for strategy results plot)
btc_prices_backtest <- all_data_calculated_basic %>% # Use basic data for clean price series
    filter(symbol == btc_symbol, date >= BACKTEST_START_DATE) %>%
    select(date, btc_price = price)

# Loop through successfully backtested pairs
for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data
    pair_returns_data <- backtest_details[[candidate_symbol]]$returns_data

    # Check if data exists for plotting
    if (is.null(pair_plot_data) || nrow(pair_plot_data) < 2 || is.null(pair_returns_data) || nrow(pair_returns_data) < 2) {
        cat(" Insufficient data to generate plots for", candidate_symbol, "\n")
        next
    }

    # --- Plot 1: Spread + Z-Score + Entries + Regime Background ---
    # Identify entry points (where position goes from 0 to non-zero)
    trade_entries <- pair_plot_data %>%
        filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>%
        mutate(entry_type = ifelse(position > 0, "Long Spread Entry", "Short Spread Entry"))

    # Plot of the Log Spread
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) +
        geom_line(color = "black", size = 0.5) +
        labs(title = paste("BTC vs", candidate_symbol, ": Log Spread (Backtest Period)"), x = NULL, y = "Log Spread") +
        theme_minimal(base_size = 10) +
        theme(plot.title = element_text(size = 10))

    # Plot of the Z-Score with signals and regime background
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) +
        # Add shaded background for "High Risk" regime periods
        geom_rect(data = . %>% filter(market_regime == "High Risk"),
                  aes(xmin = date, xmax = lead(date, default = max(date) + days(1)), ymin = -Inf, ymax = Inf),
                  fill = "grey85", alpha = 0.4, inherit.aes = FALSE) +
        # Z-score line
        geom_line(color = "black", size = 0.5) +
        # Threshold lines
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") +
        geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") +
        geom_hline(yintercept = 0, color = "grey50", linetype = "solid") +
        # Entry points
        geom_point(data = trade_entries, aes(color = entry_type), size = 2, shape=17) +
        scale_color_manual(values = c("Long Spread Entry" = "darkgreen", "Short Spread Entry" = "darkred")) +
        labs(
            title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"),
            subtitle = ifelse(REGIME_FILTER_ENABLED, "Grey background indicates 'High Risk' regime (entry blocked)", "Regime filter disabled"),
            x = "Date", y = "Z-Score", color = "Entry Type"
        ) +
        theme_minimal(base_size = 10) +
        theme(legend.position = "bottom", plot.title = element_text(size = 10), plot.subtitle = element_text(size=8))

    # Arrange and save Spread/Z-score plots
    pair_plot_file_spread <- file.path(plot_dir, paste0("01_spread_zscore_", candidate_symbol, ".png"))
    tryCatch({
        ggsave(pair_plot_file_spread,
               gridExtra::grid.arrange(plot_spread, plot_zscore, ncol = 1, heights = c(1, 2)),
               width = 10, height = 7, dpi = 150)
        cat(" Spread/Z-score plot saved:", pair_plot_file_spread, "\n")
    }, error = function(e){
        cat(" Could not save spread/z-score plot for", candidate_symbol, ":", e$message, "\n")
    })


    # --- Plot 2: Strategy Results (Multi-Panel Equity Curve Style) ---
    # Prepare data for the multi-panel plot
    plot_data_strategy <- pair_plot_data %>%
        select(date, position, market_regime) %>%
        inner_join(pair_returns_data, by = "date") %>%
        inner_join(btc_prices_backtest, by = "date") %>% # Add BTC price for context
        arrange(date) %>%
        mutate(
            # Calculate cumulative equity (starting from hypothetical balance)
            strategy_return = ifelse(is.na(strategy_return), 0, strategy_return),
            CumEQ = STARTING_BALANCE * cumprod(1 + strategy_return),
            # Identify trade entry/exit points for plotting signals
            trade_signal_change = sign(position) - lag(sign(position), default = 0),
            trade_signal = case_when(
                trade_signal_change > 0 ~ 1, # Entered Long or Exited Short
                trade_signal_change < 0 ~ -1, # Entered Short or Exited Long
                TRUE ~ 0
            ),
             # Define start/end for position rectangles
            pos_start = date,
            pos_end = lead(date, default = max(date) + days(1)),
            # Use NA for zero positions so geom_rect doesn't draw them
            position_plot = ifelse(abs(position) < SD_OFFSET, NA_real_, position)
        )

    # Panel 1: Price (BTC Price for context)
    p_price <- ggplot(plot_data_strategy, aes(x = date, y = btc_price)) +
        geom_line(color = "darkgrey") +
        labs(y = "BTC Price (USD)", x = NULL) + # Label y-axis
        theme_minimal(base_size = 9) +
        theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
        ggtitle(paste("Strategy Results: BTC vs", candidate_symbol))

    # Panel 2: Trades (Entry/Exit markers)
    p_trades <- ggplot(plot_data_strategy %>% filter(trade_signal != 0), aes(x = date, y = 0)) +
        geom_point(aes(shape = factor(sign(position)), color = factor(sign(position))), size = 2.0) + # Shape/color by position *after* change
        scale_shape_manual(values = c("1" = 17, "-1" = 17), guide = "none") + # Triangles up/down
        scale_color_manual(values = c("1" = "darkgreen", "-1" = "darkred"), guide = "none") +
        labs(y = "Trades", x = NULL) +
        ylim(-1, 1) + # Keep y-axis tight
        theme_minimal(base_size = 9) +
        theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
              axis.text.y = element_blank(), axis.ticks.y = element_blank()) # No y-axis text/ticks

    # Panel 3: Position Size
    p_pos <- ggplot(plot_data_strategy %>% filter(!is.na(position_plot))) +
        # Draw rectangles for the duration of the position
        geom_rect(aes(xmin = pos_start, xmax = pos_end, ymin = 0, ymax = position_plot,
                      fill = factor(sign(position_plot))), alpha = 0.7) +
        geom_hline(yintercept = 0, color="black") + # Zero line
        # Colorblind-friendly blues: light for short, dark for long
        scale_fill_manual(values = c("1" = "steelblue", "-1" = "lightskyblue"), guide = "none") +
        labs(y = "Position Size", x = NULL) +
        theme_minimal(base_size = 9) +
        theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
        # Set y-limits based on max scalar +/- buffer
        ylim(-MAX_POSITION_SCALAR * 1.1, MAX_POSITION_SCALAR * 1.1)

    # Panel 4: Cumulative Equity
    p_cumeq <- ggplot(plot_data_strategy, aes(x = date, y = CumEQ)) +
        geom_line(color = "blue") +
        labs(y = "Portfolio Equity", x = "Date") +
        theme_minimal(base_size = 9) +
        scale_y_continuous(labels = scales::comma) # Format y-axis labels

    # Arrange and save the multi-panel strategy plot
    pair_plot_file_strategy <- file.path(plot_dir, paste0("02_strategy_results_", candidate_symbol, ".png"))
    tryCatch({
        ggsave(pair_plot_file_strategy,
               gridExtra::grid.arrange(p_price, p_trades, p_pos, p_cumeq, ncol = 1,
                                     heights = c(3, 0.5, 1.5, 2.5)), # Adjust relative heights
               width = 9, height = 7, dpi = 150)
        cat(" Strategy results plot saved:", pair_plot_file_strategy, "\n")
    }, error = function(e){
        cat(" Could not save strategy results plot for", candidate_symbol, ":", e$message, "\n")
    })

} # End loop plotting individual pairs


# --- 8. Notes on Bitcoin Fundamental Value Strategy ---
# (Code unchanged)
cat("\n--- Step 8: Notes on Bitcoin Fundamental Value Strategy --- \n")
# This section would contain commentary or references related to incorporating
# fundamental value models (like NVT ratio, stock-to-flow etc.) into the strategy.
# Example: Could be used as an additional filter or regime indicator.
# - High NVT might suggest overvaluation, potentially reducing risk appetite.
# - Deviation from S2F model could indicate entry/exit opportunities.
# Integration would require fetching relevant data (e.g., market cap, supply)
# and defining rules based on these fundamental indicators.

# --- 9. Notes on Dynamic Portfolio Rebalancing ---
# (Code unchanged)
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n")
# This section would discuss how to manage capital allocation across multiple pairs.
# Current approach uses equal weighting implicitly by averaging returns.
# More sophisticated approaches could involve:
# - Risk Parity: Allocate capital based on the volatility of each pair's spread or returns.
# - Signal Strength: Allocate more capital to pairs with stronger signals (e.g., higher |Z-score|).
# - Correlation: Reduce allocation to pairs whose spreads become highly correlated.
# - Dynamic Pair Selection: Activate/deactivate pairs based on rolling mean-reversion tests
#   or other criteria within the backtest period itself.
# Implementation would require tracking portfolio equity and dynamically adjusting
# position sizes based on the chosen allocation rules.

cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")

```
