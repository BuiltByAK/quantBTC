---
title: "kdakd"
format: html
editor: visual
---

# Investment Strategy: Relative Value Volatility Capture (RVVC)

## Strategy Objective:

To generate alpha by identifying and trading mispricings in selected emerging cryptocurrencies relative to Bitcoin, using a quantitative model that incorporates relative value spreads, market sentiment, momentum factors, and other indicators as we identify applied across a diversified portfolio of qualifying assets.

###  Phase 1: Asset Selection & Universe Definition

**Goal:** Identify a pool of tradable emerging crypto assets suitable for the strategy.

1.  **Market Capitalization:** Exclude top 20-30 assets (like BTC, ETH). Define a minimum market cap (e.g., \> \$50 Million) to ensure some level of establishment, but low enough to capture emerging opportunities.

2.  **Liquidity:** Minimum average daily trading volume ( \> \$1 Million USD) across selected reputable exchanges. Sufficient order book depth to allow execution without excessive slippage for intended position sizes.

3.  **Exchange Listing:** Traded on at least \[e.g., 2-3\] major exchanges with reliable API access and reasonable withdrawal/deposit processes.

4.  **Data Availability:** Sufficient historical price data (at least \[e.g., 1-2 years\]) for both the asset and its pairing against Bitcoin (or easily calculable). Availability of volume and potentially on-chain data.

5.  **Fundamental Screen (Qualitative Overlay):** Basic check for legitimacy – active development team, coherent whitepaper/documentation, active community, no obvious signs of being a scam or defunct project. This is a *filter*, not the primary driver.

**Measurements & Monitoring:** Regularly screen the market based on these criteria. Maintain a dynamic "Eligible Universe" list. Track liquidity, volume, and exchange status continuously.

### Phase 2: Core Model - Relative Value Spread Analysis

**Concept:** Model the price relationship of a selected Altcoin relative to Bitcoin (BTC), assuming BTC represents the broader crypto market's systematic movement. Deviations from the typical relationship present potential trading opportunities.

**Steps:**

1.  **Spread Calculation:** Define the spread: `Spread = Altcoin Price (in USD) / Bitcoin Price (in USD)`. Alternatively, use the direct Altcoin/BTC pair price if reliably available (`Spread = Altcoin Price in BTC`). This normalizes the altcoin's price relative to the market benchmark.

2.  **Distribution Modeling:**

    -   Analyze the historical time series of the calculated spread.

    -   Test for stationarity (e.g., using ADF test). If non-stationary, consider using returns or differences of the spread.

    -   Recognize that financial returns, especially crypto, exhibit fat tails (leptokurtosis) and potential skewness. Standard Normal distribution is often inadequate.

    -   **Fit a suitable distribution:**

        -   **Parametric:** Student's t-distribution (captures fat tails), Generalized Pareto Distribution (GPD - often used in Extreme Value Theory to model tails explicitly), Stable Distribution. Estimate parameters using Maximum Likelihood Estimation (MLE) or similar methods.

        -   **Non-Parametric:** Kernel Density Estimation (KDE). Provides flexibility but can be harder to interpret tail probabilities precisely.

    -   **Purpose:** The fitted distribution provides a probabilistic description of the spread's typical behavior and defines "extreme" deviations (e.g., movement beyond the 95th or 5th percentile). This distribution serves as the **Baseline Indicator** for potential mean reversion or trend continuation in the relative valuation.

### Phase 3: Signal Enhancement - Composite Indicator Overlay

**Goal:** Increase the robustness of buy/sell signals by requiring confirmation from additional factors beyond just the spread's statistical deviation.

**Indicators to Integrate:**

1.  **Market Sentiment:**

    -   **Indicator:** Crypto Fear & Greed Index (or similar aggregate sentiment measures).

    -   **Application:** Look for confirmation between spread extremes and sentiment extremes. E.g., require "Extreme Fear" (\< 20) to confirm a buy signal when the spread hits a statistical low; require "Extreme Greed" (\> 80) to confirm a sell signal when the spread hits a statistical high.

2.  **Momentum (of the Spread):**

    -   **Indicator:** Apply momentum indicators like RSI (Relative Strength Index) or MACD (Moving Average Convergence Divergence) directly to the *spread time series*.

    -   **Application:** Look for divergences (e.g., spread makes a new low, but RSI makes a higher low – potential bullish divergence confirming a buy signal) or confirmation (RSI crossing below 30 reinforces a buy signal).

3.  **Volume (Confirmation):**

    -   **Indicator:** Trading volume of the specific Altcoin/USD or Altcoin/BTC pair.

    -   **Application:** Increased volume accompanying a move to a spread extreme can confirm the validity of the move. Look for volume spikes or above-average volume when the spread crosses key thresholds derived from the distribution. Volume Profile analysis on the spread chart could identify key support/resistance levels.

4.  **Volatility (Regime Filter):**

    -   **Indicator:** Historical Volatility (e.g., standard deviation of returns over a rolling window) of the spread or the underlying altcoin.

    -   **Application:** The strategy might perform differently in high vs. low volatility regimes. Potentially adjust position sizing or signal thresholds based on the current volatility regime. For instance, require wider deviation thresholds during high volatility periods.

**Signal Generation Logic (Example):**

-   **Potential Buy Signal:** Spread drops below the \[e.g., 5th or 10th\] percentile of its fitted distribution.

-   **Confirmed Buy Signal:** Potential Buy Signal occurs AND Fear & Greed Index is below \[e.g., 25\] AND Spread RSI shows bullish divergence or is below \[e.g., 30\] AND Volume is above its \[e.g., 20-day\] moving average.

-   **Potential Sell/Short Signal:** Spread rises above the \[e.g., 90th or 95th\] percentile of its fitted distribution.

-   **Confirmed Sell/Short Signal:** Potential Sell/Short Signal occurs AND Fear & Greed Index is above \[e.g., 75\] AND Spread RSI shows bearish divergence or is above \[e.g., 70\].

-   **Exit Logic:** Can be based on the spread returning to its mean/median, hitting a predefined profit target, or an invalidation signal (e.g., stop-loss based on adverse spread movement, or a fundamental change in the asset).

### **Phase 4: Portfolio Construction & Management**

-   **Goal:** Apply the RVVC strategy across multiple qualifying assets simultaneously to achieve diversification and smoother returns.

-   **Considerations:**

    1.  **Correlation:** Altcoin spreads vs. Bitcoin can still be correlated. Monitor the correlation matrix of the spreads of the assets in the portfolio. High correlation might dilute diversification benefits.

    2.  **Capital Allocation:** Allocate capital across different opportunities. Methods:

        -   *Equal Weighting:* Simplest, but ignores differences in signal strength or volatility.

        -   *Risk Parity:* Allocate capital such that each position contributes equally to the overall portfolio's risk (requires estimating volatility/risk of each spread strategy). More complex but better risk balancing.

        -   *Signal Strength Weighting:* Allocate more capital to signals with higher conviction (e.g., multiple indicators strongly confirming).

    3.  **Dynamic Rebalancing:** Periodically rebalance the portfolio back to target allocations. Define frequency (e.g., weekly, monthly) or threshold-based rebalancing (when allocations drift significantly).

    4.  **Overall Portfolio Risk:** Monitor aggregate portfolio risk metrics (VaR, CVaR, Drawdown) and adjust overall leverage or exposure based on market conditions or risk limits.

### Phase 5: Integration with Other Asset Classes (Optional Enhancement)

**Goal:** Further enhance diversification, manage overall portfolio volatility, or potentially generate additional alpha streams.

**Potential Additions:**

1.  **Stablecoins (Risk Management / Liquidity):** Holding a portion of the portfolio in high-quality stablecoins (USDC, USDT - monitor their risks too) acts as a buffer during market downturns ("dry powder") and provides liquidity for new opportunities or margin requirements. Allocation can be dynamic based on overall market risk signals.

2.  **Bitcoin (Core Holding / Hedge):** While used as the benchmark, holding a core position in BTC could act as a partial hedge if altcoin spreads tend to compress (i.e., altcoins underperform BTC) during sharp market sell-offs. The strategy focuses on the *relative* value, but absolute P&L is impacted by BTC's price.

3.  **Tokenized Real-World Assets (RWAs) or Thematic Baskets (Future Potential):** As the market matures, incorporating tokenized assets with lower correlation to the general crypto market (e.g., tokenized bonds, real estate, commodities) could significantly improve diversification. Requires careful vetting of RWA protocols and liquidity.

4.  **Volatility Futures/Options (Advanced):** If expertise exists, trading volatility instruments (e.g., options on BTC/ETH, or potential future crypto VIX equivalents) could be used to hedge portfolio volatility or express views on market turbulence.

**Integration Approach:** Manage these as separate sleeves within the overall portfolio, with capital allocated based on top-down risk assessment or macroeconomic views. Alternatively, signals from these other assets (e.g., a spike in traditional market volatility like VIX) could be incorporated as another input into the composite indicator for the RVVC strategy (e.g., reduce risk during cross-asset volatility spikes).

### Phase 6: Backtesting & Refinement

-   Rigorous backtesting is crucial. Simulate the strategy over historical data, accounting for estimated transaction costs, slippage, and potential data biases.

<!-- -->

-   Analyze performance metrics: Omega ratio, Sortino Ratio, Maximum Drawdown, Calmar Ratio, win rate, average profit/loss per trade.

<!-- -->

-   Optimize parameters (distribution percentiles, indicator thresholds) carefully, avoiding overfitting (use walk-forward optimization or cross-validation).

<!-- -->

-   Continuously monitor live performance against backtested expectations and adapt the model as market dynamics evolve.

Systematic risk and idiosyncratic risks of crypto:

Systematic risks:

1.  Regulatory uncertainty (we cannot control it neither quantitatively measure it)
2.  Macroeconomic conditions: Interst rate hikes, inflation, recession can create fear which reduces risk appetite of investing. We need to get our hands on as many of these as possible.
3.  Technological risks: Vulnerabilites in blockchain infrastructure, attacks or protocol failtures
4.  Market Sentiment and herd behavior: majority of crypto investors are retail, between the ages of 18 - 35. They get most of their investment decisions from social media, panic (fear and greed) and impulsive reaction to news is the biggest reason for volatility associated to this asset class.

Idiosyncratic risk :

1.  Project failture,

2.  smart contract vulnerabilities (bugs expliots that can lead to hacks) causes users to lose confidence in the project,

3.  leadership/developer decisions can make or break a project, tokenomics and supply issues (mismanagement of token supply, inflation).

    These idiosyncratic risks are not alwyse priced in becuase of information asymmetry (investors dont understand it or it is not public information)

Possible things that can impact the price of crypto (specially BTC)

1.  Consumer confidence and traditional market indices: lets correlate most important indices with cryptos
2.  Energy consumption

BTC, ETH, XRP, BNB, SOL, DOGE, ADA, TRX, STETH, WBTC, TON, LINK, LEO, AVAX, XLM

```{r}
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(lubridate)
library(readr)

# Define function to fetch each metric
get_metric <- function(endpoint) {
  url <- paste0("https://blockchain.info/q/", endpoint)
  response <- GET(url)
  value <- as.numeric(content(response, "text", encoding = "UTF-8"))
  return(value)
}

# Pull metrics from Blockchain.com API
timestamp <- Sys.time()

block_height <- get_metric("getblockcount")
hash_rate <- get_metric("hashrate")              # GH/s
difficulty <- get_metric("getdifficulty")
market_cap <- get_metric("marketcap") / 1e9      # Convert to billions USD
total_btc <- get_metric("totalbc") / 1e8         # BTC in circulation
unconfirmed_tx <- get_metric("unconfirmedcount")
tx_count_24h <- get_metric("24hrtransactioncount")

# Create a data frame with this snapshot
snapshot <- data.frame(
  timestamp = timestamp,
  block_height = block_height,
  hash_rate_GHs = hash_rate,
  difficulty = difficulty,
  market_cap_BUSD = market_cap,
  total_btc = total_btc,
  unconfirmed_tx = unconfirmed_tx,
  tx_count_24h = tx_count_24h
)

# View it
print(snapshot)

# Load existing data if available
file_path <- "btc_network_stats.csv"
if (file.exists(file_path)) {
  existing_data <- read_csv(file_path)
  updated_data <- bind_rows(existing_data, snapshot)
} else {
  updated_data <- snapshot
}

# Save updated data
write_csv(updated_data, file_path)

```

```         
ETH, XRP, BNB, SOL, DOGE, ADA, TRX, STETH, WBTC, TON, LINK, LEO, AVAX, XLM, SUI, WSTETH, USDS, SHIB, HBAR, LTC
```

OM, ICP, INJ

```{r}
##devtools::install_github("coinmetrics/api-client-r")
#library(coinmetrics)
#No SOL, no SELTH, no ton, no leo, no avax, no SUI, no WSTETH, no USDS, no SHIB, no HBAR


library(coinmetrics)
Sys.setenv()


btc_data <- get_asset_metrics(
  assets = c("btc", "eth", "xrp", "bnb", "doge", "ada", "trx", "wbtc", "link", "ltc", "xlm", "inj"),
  metrics = "PriceUSD",
  start_time = "2013-01-01",
  end_time = Sys.Date()
)

all_symbols_cm
```

# Fear and greed index

```{r, Aftikhar_code}
library(httr)
library(jsonlite)
library(dplyr)

# Fetch Fear & Greed Index data
fg_data <- fromJSON("https://api.alternative.me/fng/?limit=5000")

# Convert to dataframe and process timestamps
fg_df <- fg_data$data %>%
  mutate(
    date = as.Date(as.POSIXct(as.numeric(timestamp))))


# Plot Fear & Greed Index over time
library(ggplot2)

ggplot(fg_df, aes(x = as.Date(date), y = as.numeric(value))) +
  geom_line(color = "#FF6B6B") +
  labs(title = "Crypto Fear & Greed Index", x = "Date", y = "Index Value") +
  theme_minimal()


library(quantmod)
getSymbols("BTC-USD", src = "yahoo", auto.assign = TRUE)

btc_price <- data.frame(date = index(`BTC-USD`), coredata(`BTC-USD`)) %>% select((date), BTC.USD.Adjusted) %>% rename(price_adj = BTC.USD.Adjusted) 

GnF_BTC <- fg_df %>% 
  left_join(
    btc_price,
    by = "date"
  ) %>%
  arrange(date) %>% select(date,value, price_adj, value_classification) %>% mutate(value = as.numeric(value))



library(ggplot2)
library(patchwork)
p1 <- GnF_BTC %>% ggplot(aes(x=date)) + 
  geom_line(aes(y=as.numeric(value))) 

p2 <- GnF_BTC %>% ggplot(aes(x=date, y = price_adj)) + geom_line()

(p1 / p2)




# Load necessary libraries
library(zoo)
library(ggplot2)

# Define your rolling window size
window_size <- 30

# Compute the rolling correlation between 'value' and 'price_adj'
rolling_corr <- rollapply(
  data = GnF_BTC[, c("value", "price_adj")],
  width = window_size,
  FUN = function(x) cor(x[, "value"], x[, "price_adj"], use = "complete.obs"),
  by.column = FALSE,
  fill = NA,
  align = "right"
)

# Add the rolling correlation to your data frame
GnF_BTC$rolling_corr <- as.numeric(rolling_corr)

# Plot the rolling correlation over time
ggplot(GnF_BTC, aes(x = date, y = rolling_corr)) +
  geom_line(color = "steelblue") +
  labs(title = "30-Day Rolling Correlation between Value and Price_adj",
       x = "Date",
       y = "Rolling Correlation") +
  theme_minimal()

```

```{r}
# --- 0. Setup: Load Libraries and Set Parameters ---

# Install missing packages if necessary
# install.packages(c("CoinMetricsR", "tidyverse", "quantmod", "tidyquant", "zoo",
#                   "PerformanceAnalytics", "car", "lmtest", "tseries", "pracma", "lubridate"))

# Load libraries
library(CoinMetricsR)
library(tidyverse)
library(tidyquant) # Using tidyquant for price data (can also use quantmod)
library(zoo)
library(PerformanceAnalytics)
library(car)       # For VIF
library(lmtest)    # For residual tests (Breusch-Pagan)
library(tseries)   # For ADF test
library(pracma)    # For Hurst exponent
library(lubridate) # For date manipulation

# --- Parameters ---
cm_api_key <- "YOUR_CM_API_KEY" # !!! REPLACE WITH YOUR KEY !!!
# cm_api_key <- Sys.getenv("CM_API_KEY") # Better practice: use environment variables

# Date Range
end_date <- Sys.Date()
start_date <- end_date - years(3) # Look back 3 years, adjust as needed

# Asset List (Replace with dynamic fetch later if needed)
# Placeholder: Top cryptos (excluding stablecoins, wrapped tokens etc.)
# You might need to curate this list based on CoinMetrics availability
# Use lowercase asset IDs as expected by CoinMetricsR
initial_asset_list <- c("btc", "eth", "sol", "xrp", "ada", "avax", "doge", "dot", "link", "matic") # Example List
bitcoin_id <- "btc"

# Correlation Parameters
rolling_corr_window <- 90 # Days for rolling correlation
corr_threshold <- 0.70    # Minimum rolling correlation to be considered a "prime candidate"

# Regression & VIF Parameters
vif_threshold <- 5 # Common threshold for VIF

# Metrics to Fetch from CoinMetrics (Choose relevant ones)
# See CoinMetrics documentation for available metrics: https://docs.coinmetrics.io/asset-metrics/
# Consider metrics related to: Activity, Value Transfer, Fees, Supply, Market, Mining/Staking
metrics_to_fetch <- c(
  "PriceUSD", # Already fetched via tidyquant, but good to have consistency or for other CM metrics
  "AdrActCnt", # Active Addresses
  "TxCnt", # Transaction Count
  "TxTfrValAdjUSD", # Adjusted Transfer Value (USD)
  "FeeTotNtv", # Total Fees (Native Units)
  "FeeTotUSD", # Total Fees (USD)
  "NVTAdj", # Adjusted NVT Ratio
  "SplyCur", # Current Supply
  "HashRate", # Hash Rate (for PoW coins)
  "StkCur", # Staked Supply (for PoS coins) - May not be available for all
  "ROI30d", # 30-day Return On Investment
  "VelAct1yr" # Active Address Velocity (1yr)
)

# Mean Reversion Parameters
adf_p_value_threshold <- 0.05
hurst_threshold_mr <- 0.5 # Hurst < 0.5 suggests mean reversion

# --- Helper Function for API Calls with Error Handling ---
safe_get_metric_data <- function(assets, metrics, start, end, api_key) {
  data <- NULL
  tryCatch({
    data <- CoinMetricsR::get_asset_metrics(
      assets = assets,
      metrics = metrics,
      start_time = format(start, "%Y-%m-%d"),
      end_time = format(end, "%Y-%m-%d"),
      frequency = "1d", # Daily frequency
      api_key = api_key
    )
    # Add slight delay to respect API rate limits if necessary
    # Sys.sleep(0.5)
  }, error = function(e) {
    warning(paste("Error fetching metrics for assets:", paste(assets, collapse=", "), "-", e$message))
  })
  return(data)
}

# --- 1. Data Acquisition ---

cat("--- 1. Data Acquisition ---\n")

# 1.1 Get Price Data using tidyquant (Yahoo Finance source often used)
# tidyquant symbols often need "-USD" appended
tq_symbols <- paste0(toupper(c(bitcoin_id, initial_asset_list[initial_asset_list != bitcoin_id])), "-USD")

cat("Fetching price data from", start_date, "to", end_date, "...\n")
price_data_raw <- tq_get(tq_symbols,
                         get = "stock.prices",
                         from = start_date,
                         to = end_date)

if(nrow(price_data_raw) == 0) {
  stop("Failed to fetch any price data. Check symbols and date range.")
}

# Clean and pivot price data
price_data <- price_data_raw %>%
  select(symbol, date, adjusted) %>%
  mutate(asset = tolower(str_remove(symbol, "-USD"))) %>% # Match CM asset IDs
  select(asset, date, price = adjusted) %>%
  pivot_wider(names_from = asset, values_from = price) %>%
  arrange(date) %>%
  drop_na() # Ensure all assets have price data for the dates

# Check if Bitcoin data is present
if(!bitcoin_id %in% colnames(price_data)) {
    stop("Bitcoin price data could not be fetched or processed. Check symbol '", bitcoin_id, "'.")
}

# Update asset list based on successfully fetched price data
available_assets <- colnames(price_data)[-1] # Exclude 'date' column
assets_to_analyze <- intersect(initial_asset_list, available_assets)

if(length(assets_to_analyze) <= 1) {
    stop("Insufficient assets with complete price data for analysis.")
}
cat("Assets with price data found:", paste(assets_to_analyze, collapse=", "), "\n")


# 1.2 Get CoinMetrics Data
cat("Fetching CoinMetrics data...\n")

# Fetch metrics for all available assets (including Bitcoin)
# Batch requests or loop if hitting API limits or errors
# This simple loop fetches one asset at a time to simplify error handling per asset
all_metrics_data_list <- list()
for (asset in assets_to_analyze) {
    cat("Fetching metrics for:", asset, "\n")
    asset_metrics <- safe_get_metric_data(asset, metrics_to_fetch, start_date, end_date, cm_api_key)
    if (!is.null(asset_metrics) && nrow(asset_metrics) > 0) {
        # Convert date column to Date object
        asset_metrics <- asset_metrics %>% mutate(date = as.Date(time)) %>% select(-time)
        all_metrics_data_list[[asset]] <- asset_metrics
    } else {
        warning("No metrics data returned for asset: ", asset)
        # Remove asset if no metrics found
        assets_to_analyze <- assets_to_analyze[assets_to_analyze != asset]
    }
    Sys.sleep(0.2) # Small delay
}

# Combine metrics data (handle missing metrics gracefully)
if (length(all_metrics_data_list) > 0) {
    # Use reduce with full_join to combine, then arrange
    metrics_data <- all_metrics_data_list %>%
      bind_rows() %>%
      select(asset, date, everything()) %>%
      arrange(asset, date)

    # Check for completely empty metrics columns across all assets fetched and remove them
    cols_to_remove <- metrics_data %>%
        summarise(across(everything(), ~all(is.na(.)))) %>%
        pivot_longer(everything()) %>%
        filter(value == TRUE) %>%
        pull(name)

    if(length(cols_to_remove) > 0) {
        cat("Removing metrics columns with all NA values:", paste(cols_to_remove, collapse=", "), "\n")
        metrics_data <- metrics_data %>% select(-all_of(cols_to_remove))
        metrics_to_fetch <- setdiff(metrics_to_fetch, cols_to_remove) # Update list of available metrics
    }

} else {
  stop("Failed to fetch sufficient metrics data from CoinMetrics for any asset.")
}


# --- 2. Data Preparation ---

cat("--- 2. Data Preparation ---\n")

# 2.1 Calculate Daily Returns (Log Returns are often preferred)
# Using prices fetched via tidyquant
returns_data <- price_data %>%
  mutate(across(-date, ~log(. / lag(.)), .names = "{.col}_ret")) %>%
  select(date, ends_with("_ret")) %>%
  drop_na() # Remove first row with NA returns

# Ensure consistent column names (remove _ret suffix for easier merging later)
colnames(returns_data) <- str_remove(colnames(returns_data), "_ret")

# 2.2 Align Price, Returns, and Metrics Data by Date
# Find common date range after calculating returns and fetching metrics
common_dates <- price_data %>% select(date) %>%
    inner_join(returns_data %>% select(date), by = "date") %>%
    inner_join(metrics_data %>% distinct(date), by = "date") %>%
    pull(date)

if(length(common_dates) < max(rolling_corr_window, 30)) { # Need enough data for rolling calcs/analysis
    stop("Insufficient overlapping data points after merging prices, returns, and metrics.")
}

start_date_analysis <- min(common_dates)
end_date_analysis <- max(common_dates)

cat("Analysis Date Range:", format(start_date_analysis), "to", format(end_date_analysis), "\n")

# Filter all datasets to the common date range
price_data <- price_data %>% filter(date >= start_date_analysis, date <= end_date_analysis)
returns_data <- returns_data %>% filter(date >= start_date_analysis, date <= end_date_analysis)
metrics_data <- metrics_data %>% filter(date >= start_date_analysis, date <= end_date_analysis)

# Prepare metrics data for regression: pivot wider
# Ensure no metrics have the same name as asset ids by prefixing
metrics_wide <- metrics_data %>%
  pivot_wider(names_from = asset,
              values_from = all_of(intersect(metrics_to_fetch, colnames(metrics_data))), # Use only metrics that were actually fetched
              names_glue = "{asset}_{.value}") %>% # e.g., btc_AdrActCnt
  arrange(date)

# Join returns and metrics
analysis_data <- returns_data %>%
  inner_join(metrics_wide, by = "date")

# --- 3. Identify Prime Candidates via Rolling Correlation ---

cat("--- 3. Rolling Correlation Analysis ---\n")

candidate_assets <- setdiff(assets_to_analyze, bitcoin_id)
rolling_correlations <- list()
prime_candidates <- c()

# Calculate rolling correlation for each candidate against Bitcoin
btc_returns <- analysis_data[[bitcoin_id]]

for (asset in candidate_assets) {
  if (asset %in% colnames(analysis_data)) {
    asset_returns <- analysis_data[[asset]]

    # Ensure vectors are long enough
    if (length(asset_returns) >= rolling_corr_window) {
      roll_corr <- rollapply(data = zoo(cbind(asset_returns, btc_returns)),
                           width = rolling_corr_window,
                           FUN = function(x) cor(x[,1], x[,2], use="pairwise.complete.obs"),
                           by.column = FALSE, align = "right", fill = NA)

      rolling_correlations[[asset]] <- tibble(
          date = analysis_data$date,
          rolling_corr = as.numeric(roll_corr)
        ) %>% drop_na() # Drop initial NAs

      # Check if the average or median correlation meets the threshold
      # Or check if correlation is consistently above threshold (e.g., > 75% of the time)
      avg_corr <- mean(rolling_correlations[[asset]]$rolling_corr, na.rm = TRUE)
      median_corr <- median(rolling_correlations[[asset]]$rolling_corr, na.rm = TRUE)

      cat("Asset:", asset, "- Avg Rolling Correlation:", round(avg_corr, 3),
          "- Median Rolling Correlation:", round(median_corr, 3), "\n")

      if (!is.na(median_corr) && median_corr >= corr_threshold) {
        prime_candidates <- c(prime_candidates, asset)
      }
    } else {
        cat("Asset:", asset, "- Insufficient data points for rolling correlation.\n")
    }
  }
}

if (length(prime_candidates) == 0) {
  stop("No prime candidates found meeting the correlation threshold of", corr_threshold)
}

cat("Prime candidates identified:", paste(prime_candidates, collapse = ", "), "\n")

# Optional: Plot rolling correlations
# rolling_corr_df <- bind_rows(rolling_correlations, .id = "asset")
# ggplot(rolling_corr_df, aes(x = date, y = rolling_corr, color = asset)) +
#   geom_line() +
#   geom_hline(yintercept = corr_threshold, linetype = "dashed", color = "red") +
#   labs(title = "Rolling Correlation with Bitcoin", y = "Correlation", x = "Date") +
#   theme_minimal()


# --- 4. Regression Analysis for Prime Candidates ---

cat("--- 4. Regression Analysis for Prime Candidates ---\n")

regression_results <- list()
selected_variables <- list() # To store variables passing VIF and significance checks for each candidate

# Define Bitcoin metrics columns (prefix added earlier)
btc_metrics_cols <- colnames(analysis_data)[startsWith(colnames(analysis_data), paste0(bitcoin_id, "_"))]

for (candidate in prime_candidates) {
  cat("\n--- Analyzing Candidate:", candidate, "---\n")

  # Define candidate's return column and metrics columns
  candidate_ret_col <- candidate
  candidate_metrics_cols <- colnames(analysis_data)[startsWith(colnames(analysis_data), paste0(candidate, "_"))]

  # Combine relevant Bitcoin and candidate metrics as potential predictors
  # Also consider lagged metrics if theory suggests (adds complexity)
  predictor_cols <- c(btc_metrics_cols, candidate_metrics_cols)

  # Ensure predictor columns actually exist in the data
  predictor_cols <- intersect(predictor_cols, colnames(analysis_data))
  
  # Remove columns with near zero variance or too many NAs before regression
  valid_predictors <- predictor_cols[sapply(analysis_data[predictor_cols], function(x) {
    !all(is.na(x)) && var(x, na.rm = TRUE) > 1e-10 
  })]
  
  # Further check for high NA count (e.g. > 20% NA)
  na_threshold <- 0.20 
  predictors_final_list <- valid_predictors[sapply(analysis_data[valid_predictors], function(x) {
      sum(is.na(x)) / length(x) < na_threshold
  })]
  
  if (length(predictors_final_list) < 2) {
      cat("Skipping", candidate, "- insufficient valid predictor variables after filtering.\n")
      next
  }
  
  # Prepare data for regression (handle NAs - imputation or removal)
  # For simplicity, we'll use na.omit on the subset for regression
  regression_df <- analysis_data %>%
    select(date, all_of(candidate_ret_col), all_of(predictors_final_list)) %>%
    drop_na() # Removes rows with NA in EITHER predictors or response

  if (nrow(regression_df) < length(predictors_final_list) + 10) { # Need enough rows for regression
      cat("Skipping", candidate, "- insufficient non-NA data rows for regression.\n")
      next
  }
  
  # Dependent variable
  y_var <- candidate_ret_col

  # Initial formula
  current_predictors <- predictors_final_list
  formula_str <- paste(y_var, "~", paste(current_predictors, collapse = " + "))
  formula_obj <- as.formula(formula_str)

  # --- VIF Check and Iterative Removal ---
  cat("Performing VIF check...\n")
  vif_high <- TRUE
  while(vif_high && length(current_predictors) > 1) {
      model_vif <- lm(formula_obj, data = regression_df)
      vif_values <- tryCatch({
          vif(model_vif)
      }, error = function(e) {
          warning("Could not calculate VIF for ", candidate, ". Skipping VIF check for this iteration. Error: ", e$message)
          return(NULL) # Return NULL if VIF calculation fails
      })

      if(is.null(vif_values)) {
          break # Exit VIF loop if calculation failed
      }

      # Check if vif_values is a matrix (multiple predictors) or a single value
      if (is.matrix(vif_values)) {
          max_vif <- max(vif_values[,1], na.rm = TRUE) # VIF is usually the first column if GVIF is used
          vif_df <- data.frame(Variable = rownames(vif_values), VIF = vif_values[,1])
      } else if (is.numeric(vif_values) && length(vif_values) > 0) {
          max_vif <- max(vif_values, na.rm = TRUE)
          vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values)
      } else {
           warning("Unexpected VIF result format for ", candidate, ". Skipping VIF reduction.")
           break # Exit loop if VIF format is unexpected
      }

      if (max_vif > vif_threshold) {
          # Find variable with highest VIF
          var_to_remove <- vif_df$Variable[which.max(vif_df$VIF)]
          cat("  Removing variable '", var_to_remove, "' due to high VIF (", round(max_vif, 2), ")\n")
          current_predictors <- setdiff(current_predictors, var_to_remove)
          
          if (length(current_predictors) < 1) {
              cat("  No predictors left after VIF removal for", candidate, "\n")
              vif_high = FALSE # Stop loop
              break 
          }
          
          formula_str <- paste(y_var, "~", paste(current_predictors, collapse = " + "))
          formula_obj <- as.formula(formula_str)
      } else {
          vif_high <- FALSE # All VIFs are below threshold
          cat("  VIF check passed. Max VIF:", round(max_vif, 2), "\n")
      }
  } # End VIF while loop

  if(length(current_predictors) == 0) {
      cat("No predictors remained after VIF analysis for", candidate, ". Skipping regression.\n")
      next
  }

  # --- Final Regression with Selected Predictors ---
  cat("Running final regression for", candidate, "with predictors:", paste(current_predictors, collapse=", "), "\n")
  final_model <- lm(formula_obj, data = regression_df)
  model_summary <- summary(final_model)

  # Store results
  regression_results[[candidate]] <- list(
    model = final_model,
    summary = model_summary,
    predictors = current_predictors,
    vif = if(exists("vif_df")) vif_df else NULL # Store last VIF values if calculated
  )

  cat("R-squared:", round(model_summary$r.squared, 3),
      "Adj. R-squared:", round(model_summary$adj.r.squared, 3), "\n")

  # --- Residual Analysis ---
  cat("Performing residual analysis...\n")
  residuals_df <- data.frame(
    date = regression_df$date, # Ensure date is kept
    residuals = residuals(final_model),
    fitted = fitted(final_model)
  )

  # 1. Plot Residuals vs Fitted
  p_res_fit <- ggplot(residuals_df, aes(x = fitted, y = residuals)) +
    geom_point(alpha = 0.6) +
    geom_smooth(method = "loess", color = "red", se = FALSE) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = paste(candidate, "- Residuals vs Fitted Values"), x = "Fitted Values", y = "Residuals") +
    theme_minimal()
  print(p_res_fit) # Display the plot

  # 2. Plot Residuals over Time (Check for autocorrelation)
  p_res_time <- ggplot(residuals_df, aes(x = date, y = residuals)) +
      geom_line(alpha = 0.7) +
      geom_hline(yintercept = 0, linetype = "dashed") +
      labs(title = paste(candidate, "- Residuals Over Time"), x = "Date", y = "Residuals") +
      theme_minimal()
  print(p_res_time) # Display the plot

  # 3. QQ Plot (Check for Normality)
  p_qq <- ggplot(residuals_df, aes(sample = residuals)) +
    stat_qq() +
    stat_qq_line() +
    labs(title = paste(candidate, "- Normal Q-Q Plot of Residuals")) +
    theme_minimal()
  print(p_qq)

  # 4. Normality Test (Shapiro-Wilk) - Often fails for financial data
  shapiro_test <- tryCatch(shapiro.test(residuals_df$residuals), error = function(e) list(p.value=NA))
  cat("  Shapiro-Wilk Normality Test p-value:", shapiro_test$p.value, "\n")

  # 5. Autocorrelation Test (Breusch-Godfrey or Durbin-Watson)
  # Using Breusch-Godfrey test from lmtest
  bg_test <- tryCatch(bgtest(final_model), error = function(e) list(p.value=NA)) # Tests for serial correlation
  cat("  Breusch-Godfrey Autocorrelation Test p-value:", bg_test$p.value, "\n")

  # 6. Heteroscedasticity Test (Breusch-Pagan)
  bp_test <- tryCatch(bptest(final_model), error = function(e) list(p.value=NA)) # Tests for non-constant variance
  cat("  Breusch-Pagan Heteroscedasticity Test p-value:", bp_test$p.value, "\n")

  # --- Identify "Best" Variables for this Candidate ---
  # Criteria: Statistically significant (e.g., p < 0.05) in the final model
  significant_vars <- model_summary$coefficients %>%
    as.data.frame() %>%
    rownames_to_column("Variable") %>%
    filter(Variable != "(Intercept)" & `Pr(>|t|)` < 0.05) %>% # Adjust p-value threshold if needed
    pull(Variable)

  cat("  Significant variables (p < 0.05):", paste(significant_vars, collapse = ", "), "\n")
  selected_variables[[candidate]] <- significant_vars

} # End loop through prime candidates


# --- 5. Create Composite Indicator (Example) ---

cat("\n--- 5. Composite Indicator Construction ---\n")

# Consolidate selected variables across all candidates
# This part requires careful thought: Do you want one indicator for all pairs,
# or one per pair? How to weigh variables?
# Simplest approach: Identify variables that appear frequently as significant and have low VIF.

all_significant_vars <- unlist(selected_variables)
var_frequency <- sort(table(all_significant_vars), decreasing = TRUE)

cat("Frequency of significant variables across candidates:\n")
print(var_frequency)

# Example: Choose top N variables or those appearing in > M candidates
# Let's choose variables appearing for at least 2 candidates (example threshold)
composite_vars <- names(var_frequency[var_frequency >= 1]) # Adjust threshold

if (length(composite_vars) == 0) {
    warning("No variables were consistently significant across candidates. Composite indicator cannot be built with this criteria.")
    # Handle this case - maybe use all selected vars, or stop
} else {
    cat("Variables selected for composite indicator:", paste(composite_vars, collapse=", "), "\n")

    # Construct the indicator: Simple average of scaled variables
    # Scaling is important because variables have different units/magnitudes
    composite_indicator_data <- analysis_data %>%
      select(date, all_of(composite_vars)) %>%
      mutate(across(-date, ~scale(.), .names = "{.col}_scaled")) %>% # Scale (center and standardize)
      select(date, ends_with("_scaled"))

    # Calculate the composite indicator (simple average of scaled values)
    # Ensure rowwise operation and handle potential NAs if scaling failed for a column
    composite_indicator_data$composite_indicator <- rowMeans(composite_indicator_data %>% select(-date), na.rm = TRUE)

    # Add to main analysis data frame
    analysis_data <- analysis_data %>%
      left_join(composite_indicator_data %>% select(date, composite_indicator), by = "date")

    cat("Composite indicator created.\n")

    # Optional: Plot composite indicator
    # ggplot(analysis_data, aes(x = date, y = composite_indicator)) +
    #   geom_line() +
    #   labs(title = "Composite Indicator Over Time", y = "Indicator Value") +
    #   theme_minimal()
}


# --- 6. Pairs Trading Analysis: Spread Mean Reversion ---

cat("\n--- 6. Pairs Trading Analysis ---\n")

pairs_trading_results <- list()

for (candidate in prime_candidates) {
  cat("\n--- Analyzing Spread for Pair: BTC -", candidate, "---\n")

  # Calculate the spread. Log prices are common for ratio stability.
  # Spread = log(Price_BTC) - log(Price_Candidate) or log(Price_Candidate / Price_BTC)
  # Using price_data which is aligned and filtered
  spread_df <- price_data %>%
    select(date, all_of(bitcoin_id), all_of(candidate)) %>%
    mutate(
      log_price_btc = log(.data[[bitcoin_id]]),
      log_price_candidate = log(.data[[candidate]]),
      spread_log = log_price_btc - log_price_candidate
      # Or: spread_ratio = .data[[bitcoin_id]] / .data[[candidate]]
    ) %>%
    select(date, spread_log) %>%
    drop_na()

  if (nrow(spread_df) < 50) { # Need sufficient points for tests
      cat("Skipping", candidate, "- insufficient spread data points.\n")
      next
  }

  # Visualize the spread
  p_spread <- ggplot(spread_df, aes(x = date, y = spread_log)) +
    geom_line() +
    labs(title = paste("Log Spread:", bitcoin_id, "-", candidate), y = "Log Price Spread") +
    theme_minimal()
  print(p_spread)

  # Test for Stationarity (Mean Reversion) using Augmented Dickey-Fuller (ADF) Test
  # Null Hypothesis (H0): The series has a unit root (is non-stationary).
  # Alternative Hypothesis (H1): The series is stationary.
  # We want to REJECT H0 (low p-value) for mean reversion.
  adf_test_result <- tryCatch(adf.test(spread_df$spread_log, alternative = "stationary"),
                              error = function(e) {warning("ADF test failed for ", candidate); NULL})

  if(!is.null(adf_test_result)) {
      cat("  ADF Test Results for", candidate, "spread:\n")
      # print(adf_test_result)
      cat("    p-value:", adf_test_result$p.value, "\n")
      is_stationary <- adf_test_result$p.value < adf_p_value_threshold
      cat("    Spread appears", ifelse(is_stationary, "STATIONARY (Mean-Reverting)", "NON-STATIONARY"), "\n")
  } else {
      is_stationary <- FALSE
  }


  # Calculate Hurst Exponent
  # H < 0.5: Mean-reverting (anti-persistent)
  # H = 0.5: Random walk (geometric Brownian motion)
  # H > 0.5: Trending (persistent)
  hurst_result <- tryCatch(hurstexp(spread_df$spread_log, display = FALSE),
                           error = function(e) {warning("Hurst calculation failed for ", candidate); NULL})

  if(!is.null(hurst_result)) {
      hurst_value <- hurst_result$He
      cat("  Hurst Exponent:", round(hurst_value, 3), "\n")
      suggests_mr <- hurst_value < hurst_threshold_mr
      cat("    Hurst suggests", ifelse(suggests_mr, "MEAN REVERSION", "TRENDING or RANDOM WALK"), "\n")
  } else {
      suggests_mr <- FALSE
      hurst_value <- NA
  }

  # Store results
  pairs_trading_results[[candidate]] <- list(
    adf_p_value = adf_test_result$p.value,
    is_stationary = is_stationary,
    hurst_exponent = hurst_value,
    suggests_mr_hurst = suggests_mr,
    spread_data = spread_df
  )

  # Optional: Overlay Composite Indicator with Spread Z-Score
  # Requires composite indicator to be calculated successfully
  if (exists("composite_indicator_data") && "composite_indicator" %in% colnames(analysis_data)) {
      spread_df <- spread_df %>%
          mutate(spread_zscore = (spread_log - mean(spread_log)) / sd(spread_log)) %>%
          left_join(analysis_data %>% select(date, composite_indicator), by="date") %>%
          drop_na()

      # Requires scaling indicator for plotting on same axis or use secondary axis
      scale_factor <- max(abs(spread_df$spread_zscore), na.rm=TRUE) / max(abs(spread_df$composite_indicator), na.rm=TRUE)
      scale_factor <- ifelse(is.finite(scale_factor), scale_factor, 1) # Handle division by zero if indicator is flat

      p_spread_indicator <- ggplot(spread_df, aes(x = date)) +
        geom_line(aes(y = spread_zscore, color = "Spread Z-Score")) +
        geom_line(aes(y = composite_indicator * scale_factor, color = "Composite Indicator (Scaled)")) +
        scale_color_manual(values = c("Spread Z-Score" = "blue", "Composite Indicator (Scaled)" = "red")) +
        labs(title = paste(candidate, "- Spread Z-Score vs Composite Indicator"), y = "Value") +
        theme_minimal() +
        theme(legend.position = "bottom")
      print(p_spread_indicator)
  }


} # End loop for pairs trading analysis


# --- 7. Conclusion / Next Steps ---
cat("\n--- 7. Analysis Complete ---\n")

cat("Prime candidates analyzed:", paste(prime_candidates, collapse=", "), "\n")

cat("\nPotential Pairs for Mean Reversion Trading (based on ADF & Hurst):\n")
for (candidate in names(pairs_trading_results)) {
    res <- pairs_trading_results[[candidate]]
    if (res$is_stationary && res$suggests_mr_hurst) {
        cat("  *", candidate, "(ADF p-val:", round(res$adf_p_value, 4), ", Hurst:", round(res$hurst_exponent, 3), ")\n")
    }
}

cat("\nNext Steps:\n")
cat("1. Review Regression Results: Examine coefficients, R-squared, and residual plots for each candidate.\n")
cat("2. Refine Composite Indicator: Experiment with variable selection, weighting schemes (e.g., based on t-stats), or dimensionality reduction (PCA).\n")
cat("3. Backtest Trading Strategy: Develop specific entry/exit rules for pairs trading based on spread deviation (e.g., Z-score) and potentially filtered/confirmed by the composite indicator. Test performance rigorously.\n")
cat("4. Parameter Tuning: Adjust lookback windows, thresholds (correlation, VIF, p-value), and metric lists.\n")
cat("5. Dynamic Asset List: Implement fetching top cryptos dynamically via an external API.\n")
cat("6. Robustness Checks: Test across different time periods and market conditions.\n")

# --- End of Script ---
```

```{r}
# --- 0. Setup: Install and Load Libraries ---
# install.packages(c("crypto2", "tidyverse", "zoo", "PerformanceAnalytics", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite")) # Install if needed

library(crypto2)
library(tidyverse)
library(zoo)
library(xts) # Explicitly load xts
library(PerformanceAnalytics)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(TTR)


# --- 1. Parameters ---
N_TOP_COINS <- 15         # Number of top coins to consider (including BTC initially)
ROLLING_WINDOW <- 30      # Rolling window for correlation and Z-score (days)
START_DATE <- Sys.Date() - years(2) # Analysis start date (e.g., 2 years back)
END_DATE <- Sys.Date()      # Analysis end date
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # Common stablecoins to exclude
VIF_THRESHOLD <- 5        # Threshold for Variance Inflation Factor
ADF_P_VALUE_THRESHOLD <- 0.05 # Threshold for ADF test significance
HURST_THRESHOLD <- 0.5    # Threshold for Hurst exponent (H < 0.5 suggests mean reversion)
ZSCORE_THRESHOLD <- 1.5   # Z-score threshold for potential trading signals


# --- 2. Data Acquisition ---
# --- 2. Data Acquisition ---
cat("--- Step 2: Acquiring Data ---\n")

# ... (CoinGecko fetching and filtering code remains the same) ...

# --- Revised Data Fetching using httr/jsonlite ---
cat("Fetching historical data for", paste(all_symbols_of_interest, collapse=", "), "using direct API calls...\n")
all_data_list <- list()
not_found_symbols <- c()
base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
# Calculate 'limit' based on date range for the API call
# CryptoCompare limit is number of days *including* start date
api_limit <- as.numeric(END_DATE - START_DATE)
# The API limit seems to be capped around 2000 points for free tier histoday.
# Adjust if needed, or implement multiple calls for longer periods.
if(api_limit > 2000) {
    cat("Warning: Date range exceeds typical CryptoCompare API limit (2000 days). Truncating fetch.\n")
    api_limit <- 2000
    # Adjust START_DATE if truncating - fetch most recent 2000 days ending at END_DATE
    START_DATE <- END_DATE - days(api_limit)
    cat("Adjusted START_DATE to:", format(START_DATE), "\n")
}


Sys.setenv(TZ='UTC') # Set timezone to UTC for consistency

for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "...\n")
    hist_data_xts <- NULL # Initialize

    tryCatch({
        # Construct API URL
        query_params <- list(
            fsym = sym,       # Symbol we want data for
            tsym = "USD",     # Quote currency
            limit = api_limit, # Number of days up to END_DATE
            toTs = as.numeric(as.POSIXct(END_DATE)) # End date as Unix timestamp
        )

        # Make the API request
        response <- httr::GET(base_url, query = query_params)
        Sys.sleep(1.5) # Pause *after* the call

        # Check HTTP status
        if (httr::status_code(response) == 200) {
            # Parse JSON content
            content <- httr::content(response, "text", encoding = "UTF-8")
            json_data <- jsonlite::fromJSON(content)

            # Check API-level response
            if (json_data$Response == "Success") {
                if (!is.null(json_data$Data$Data) && length(json_data$Data$Data) > 0 && nrow(json_data$Data$Data) > 0) {

                    # Extract the relevant data frame
                    df <- json_data$Data$Data

                    # Ensure required columns exist (names might vary slightly)
                    # Common names: time, close, volumeto
                    if (all(c("time", "close", "volumeto") %in% names(df))) {

                        # Convert Unix timestamp ('time') to Date
                        # Use lubridate::as_datetime for robustness
                        df$timestamp <- lubridate::as_datetime(df$time, tz = "UTC")
                        df$date <- as.Date(df$timestamp) # Keep only the date part

                        # Select, rename, and create xts object
                        # Important: Filter by date *after* conversion, ensure it's within desired range
                        # The API might return slightly different start points based on 'limit'
                        df_filtered <- df %>%
                            select(date, close, volumeto) %>%
                            filter(date >= START_DATE & date <= END_DATE) %>%
                            arrange(date) # Ensure chronological order

                         if(nrow(df_filtered) > 0) {
                             hist_data_xts <- xts(df_filtered[, c("close", "volumeto")], order.by = df_filtered$date)
                             colnames(hist_data_xts) <- paste(sym, c("Price", "Volume"), sep = ".")
                             all_data_list[[sym]] <- hist_data_xts
                             cat(" Success. Rows fetched:", nrow(hist_data_xts), "\n")
                         } else {
                             cat(" Success, but no data returned within the specified date range after filtering for", sym, ". Skipping.\n")
                             not_found_symbols <- c(not_found_symbols, sym)
                         }

                    } else {
                        cat(" API Success, but returned data frame is missing required columns (time, close, volumeto) for", sym, ". Skipping.\n")
                        print(head(df)) # Print head to see structure
                        not_found_symbols <- c(not_found_symbols, sym)
                    }
                } else {
                     cat(" API Success, but Data$Data field is empty or null for", sym, ". Skipping.\n")
                     not_found_symbols <- c(not_found_symbols, sym)
                }
            } else {
                # API returned an error message
                cat(" API returned error for", sym, ":", json_data$Message, ". Skipping.\n")
                not_found_symbols <- c(not_found_symbols, sym)
            }
        } else {
            # HTTP request failed
            cat(" HTTP Error:", httr::status_code(response), "when fetching data for", sym, ". Skipping.\n")
            # Optional: print content for more details
            # print(httr::content(response, "text", encoding = "UTF-8"))
            not_found_symbols <- c(not_found_symbols, sym)
        }

    }, error = function(e) {
        # Catch errors during GET, parsing, or processing
        cat(" Error during API call or processing for", sym, ":", e$message, ". Skipping.\n")
        not_found_symbols <- c(not_found_symbols, sym)
        Sys.sleep(1.5) # Pause even after error
    })
}

# --- End of Revised Data Fetching ---

# Add an explicit check after the loop before proceeding
if (length(all_data_list) == 0) {
    stop("Failed to fetch historical data for ANY symbol using direct API calls. Exiting.")
}
if (!("BTC" %in% names(all_data_list))) {
     stop("Failed to fetch Bitcoin data using direct API calls. Exiting.")
}
# Update prime_candidate_symbols based on successful fetches
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list))
if (length(prime_candidate_symbols) == 0) {
    stop("Data fetched for BTC, but failed for all prime candidates using direct API calls. Exiting.")
}
cat("Successfully fetched data for:", paste(names(all_data_list), collapse=", "), "\n")


# Merge data into a single xts object
cat("Merging historical data...\n")
all_data_xts <- do.call(merge, c(all_data_list, fill = na.locf))
# Ensure data starts from START_DATE and handle NAs
# It's possible the earliest date fetched is slightly after START_DATE if data wasn't available
effective_start_date <- max(START_DATE, start(all_data_xts))
all_data_xts <- all_data_xts[paste0(effective_start_date, "/")]
# Remove any leading NAs that might remain after merging/filtering/na.locf
all_data_xts <- na.omit(all_data_xts)

if (nrow(all_data_xts) == 0) {
    stop("No overlapping data available for the selected symbols after merging and NA removal. Check date ranges and symbol availability.")
}

cat("Data merging complete. Date range:", format(start(all_data_xts)), "to", format(end(all_data_xts)), "\n")
cat("Dimensions of merged data:", dim(all_data_xts), "\n")


# --- Subsequent Steps (3 onwards) should now work ---

# --- 3. Data Preparation ---
cat("--- Step 3: Preparing Data ---\n")

# Calculate Daily Log Returns
cat("Calculating daily log returns...\n")
prices_xts <- all_data_xts[, grep("\\.Price$", colnames(all_data_xts))]
returns_xts <- Return.calculate(prices_xts, method = "log")
colnames(returns_xts) <- gsub("\\.Price$", ".Return", colnames(prices_xts))
returns_xts <- returns_xts[-1,] # Remove first row (NA after return calculation)

# Prepare Volume data (e.g., log change in volume)
cat("Calculating log change in volume...\n")
volumes_xts <- all_data_xts[, grep("\\.Volume$", colnames(all_data_xts))]
# Add small constant to avoid log(0) issues if volume is ever 0
volumes_xts[volumes_xts <= 0] <- 1e-6
log_volume_change_xts <- Return.calculate(volumes_xts, method="log") # Using log returns for volume change
colnames(log_volume_change_xts) <- gsub("\\.Volume$", ".LogVolChange", colnames(volumes_xts))
log_volume_change_xts <- log_volume_change_xts[-1,]

# Align all analysis data (Returns and Volume Changes)
analysis_data <- merge(returns_xts, log_volume_change_xts, join = "inner") # Use inner join to ensure all data exists
analysis_data <- na.omit(analysis_data) # Remove any remaining NAs

btc_return_col <- "BTC.Return"
candidate_return_cols <- paste0(prime_candidate_symbols, ".Return")


# --- 4. Rolling Correlation Analysis ---
cat("--- Step 4: Analyzing Rolling Correlations ---\n")

rolling_correlations <- list()
btc_returns_vec <- coredata(analysis_data[, btc_return_col])

for (candidate_ret_col in candidate_return_cols) {
    if(candidate_ret_col %in% colnames(analysis_data)) {
        candidate_symbol <- gsub("\\.Return$", "", candidate_ret_col)
        cat("Calculating rolling correlation for:", candidate_symbol, "\n")

        candidate_returns_vec <- coredata(analysis_data[, candidate_ret_col])

        # Ensure vectors are long enough for the window
        if(length(btc_returns_vec) >= ROLLING_WINDOW && length(candidate_returns_vec) >= ROLLING_WINDOW) {
             # Using runCor from TTR package (via PerformanceAnalytics) is efficient
             cor_xts <- tryCatch({
                 roll_cor <- runCor(btc_returns_vec, candidate_returns_vec, n = ROLLING_WINDOW)
                 xts(roll_cor, order.by = index(analysis_data)[(ROLLING_WINDOW):nrow(analysis_data)])
             }, error = function(e) {
                 cat(" Could not calculate rolling correlation for", candidate_symbol, ":", e$message, "\n")
                 NULL
             })

             if (!is.null(cor_xts)) {
                colnames(cor_xts) <- paste0("BTC_", candidate_symbol, "_RollingCor", ROLLING_WINDOW)
                rolling_correlations[[candidate_symbol]] <- cor_xts

                # Optional: Plotting rolling correlations
                # plot(rolling_correlations[[candidate_symbol]], main=paste("BTC vs", candidate_symbol, "Rolling Correlation"), ylab="Correlation", ylim=c(-1,1))
             }
        } else {
             cat(" Insufficient data points for rolling correlation calculation for", candidate_symbol, "\n")
        }
    }
}
cat("Rolling correlation analysis complete.\n")


# --- 5. Explanatory Variable Analysis (Regression) ---
cat("--- Step 5: Regression Analysis for Explanatory Variables ---\n")

regression_results <- list()
significant_variables <- list()

# Define potential predictors (lagged BTC return, lagged own return, lagged log volume change)
# We use lag(k=1) to predict today's return based on yesterday's information
predictors_base <- c(btc_return_col) # Always include lagged BTC return
# Add candidate-specific lagged predictors
analysis_data_lagged <- lag(analysis_data)
analysis_data_merged <- merge(analysis_data, analysis_data_lagged, join = "inner")
analysis_data_merged <- na.omit(analysis_data_merged) # Remove NAs introduced by lagging/merging

if (nrow(analysis_data_merged) < 20) { # Need sufficient data for regression
    warning("Insufficient data after lagging and merging for regression analysis. Skipping.")
} else {
    for (candidate_symbol in prime_candidate_symbols) {
        cat("Running regression for:", candidate_symbol, "\n")
        candidate_ret_col <- paste0(candidate_symbol, ".Return")
        candidate_lag_ret_col <- paste0(candidate_symbol, ".Return.1") # Lagged return column name from merge
        candidate_lag_vol_col <- paste0(candidate_symbol, ".LogVolChange.1") # Lagged vol change column name

        # Check if columns exist after lagging and merging
        required_cols <- c(candidate_ret_col, paste0(btc_return_col, ".1"), candidate_lag_ret_col, candidate_lag_vol_col)
        if (!all(required_cols %in% colnames(analysis_data_merged))) {
            cat(" Missing required columns for regression on", candidate_symbol, ". Skipping.\n")
            next
        }

        # Construct the formula dynamically
        # Predicting today's return (col) using lagged predictors (col.1)
        predictor_cols <- c(paste0(btc_return_col, ".1"), candidate_lag_ret_col, candidate_lag_vol_col)
        formula_str <- paste(candidate_ret_col, "~", paste(predictor_cols, collapse = " + "))

        tryCatch({
            model <- lm(as.formula(formula_str), data = analysis_data_merged)
            model_summary <- summary(model)

            # Check VIF
            vif_values <- tryCatch(vif(model), error = function(e) { cat(" Could not calculate VIF. "); NULL })
            high_vif_vars <- if (!is.null(vif_values)) names(vif_values[vif_values > VIF_THRESHOLD]) else character(0)

            # Check Residuals (Durbin-Watson test for autocorrelation)
            dw_test <- dwtest(model)

            results <- list(
                symbol = candidate_symbol,
                formula = formula_str,
                summary = model_summary,
                r_squared = model_summary$r.squared,
                adj_r_squared = model_summary$adj.r.squared,
                vif = vif_values,
                high_vif_vars = high_vif_vars,
                dw_test_p_value = dw_test$p.value
            )
            regression_results[[candidate_symbol]] <- results

            cat("  R-squared:", round(results$adj_r_squared, 3),
                "| High VIF:", paste(high_vif_vars, collapse=", "),
                "| DW p-value:", round(results$dw_test_p_value, 3), "\n")

             # Identify significant variables (e.g., p < 0.05) excluding intercept, ignoring high VIF vars for now
             coef_summary <- coef(model_summary)
             sig_vars <- rownames(coef_summary)[coef_summary[, "Pr(>|t|)"] < 0.05 & rownames(coef_summary) != "(Intercept)"]
             significant_variables[[candidate_symbol]] <- sig_vars

            # Optional: Plot residuals
            # plot(residuals(model), main=paste("Residuals for", candidate_symbol), ylab="Residual")
            # acf(residuals(model), main=paste("ACF of Residuals for", candidate_symbol))

        }, error = function(e) {
            cat(" Error running regression for", candidate_symbol, ":", e$message, "\n")
        })
    }
}
cat("Regression analysis complete.\n")


# --- 6. Identify Best Variables / Composite Indicator ---
cat("--- Step 6: Identifying Potentially Useful Variables ---\n")

# Consolidate significant variables across all models (simple approach)
all_significant_vars <- unlist(significant_variables)
variable_counts <- table(all_significant_vars)

cat("Frequency of significant variables (p<0.05) across models:\n")
print(sort(variable_counts, decreasing = TRUE))

# ** Placeholder for Composite Indicator **
# Based on the above, you might choose variables that appear frequently or have strong theoretical backing.
# Example: If 'BTC.Return.1' and 'Candidate.LogVolChange.1' are often significant,
# a composite indicator *could* be a weighted sum of these factors, but defining weights requires
# more rigorous analysis (e.g., PCA, optimization).
# For now, we just identify them.
potential_composite_vars <- names(variable_counts)[variable_counts > length(prime_candidate_symbols) * 0.3] # Example: significant in >30% of models
cat("\nVariables appearing frequently:", paste(potential_composite_vars, collapse=", "), "\n")


# --- 7. Pairs Trading Analysis (Spread Mean Reversion) ---
cat("--- Step 7: Pairs Trading Spread Analysis ---\n")

pairs_trading_analysis <- list()

# Use the original merged price data (all_data_xts)
prices_for_pairs <- prices_xts # Use the price subset from earlier

for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing spread for BTC vs", candidate_symbol, "\n")
    candidate_price_col <- paste0(candidate_symbol, ".Price")
    btc_price_col <- "BTC.Price"

    if (candidate_price_col %in% colnames(prices_for_pairs) && btc_price_col %in% colnames(prices_for_pairs)) {
        pair_prices <- prices_for_pairs[, c(btc_price_col, candidate_price_col)]
        pair_prices <- na.omit(pair_prices) # Ensure no NAs

        if (nrow(pair_prices) > ROLLING_WINDOW) {
             # Calculate spread: Log ratio is often preferred for stability
             spread_xts <- log(pair_prices[, candidate_price_col]) - log(pair_prices[, btc_price_col])
             # Alternative: Price Ratio: spread_xts <- pair_prices[, candidate_price_col] / pair_prices[, btc_price_col]
             colnames(spread_xts) <- paste0(candidate_symbol, "_BTC_LogSpread")

             # Test for Stationarity (ADF Test) on the spread
             adf_result <- tryCatch(adf.test(na.omit(coredata(spread_xts))), error = function(e) NULL)
             adf_p_value <- if (!is.null(adf_result)) adf_result$p.value else NA

             # Calculate Hurst Exponent
             hurst_result <- tryCatch(hurstexp(na.omit(coredata(spread_xts)), d = ROLLING_WINDOW, display = FALSE), error = function(e) NULL)
             hurst_value <- if (!is.null(hurst_result)) hurst_result$He else NA


             # Calculate Rolling Z-Score of the spread
             spread_mean <- rollapply(spread_xts, width = ROLLING_WINDOW, FUN = mean, align = "right", fill = NA)
             spread_sd <- rollapply(spread_xts, width = ROLLING_WINDOW, FUN = sd, align = "right", fill = NA)
             zscore_xts <- (spread_xts - spread_mean) / spread_sd
             colnames(zscore_xts) <- paste0(candidate_symbol, "_BTC_Spread_ZScore")

             pairs_trading_analysis[[candidate_symbol]] <- list(
                 symbol = candidate_symbol,
                 spread = spread_xts,
                 zscore = zscore_xts,
                 adf_p_value = adf_p_value,
                 hurst_exponent = hurst_value,
                 is_potentially_mean_reverting = (!is.na(adf_p_value) && adf_p_value < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value) && hurst_value < HURST_THRESHOLD)
             )

             cat("  ADF p-value:", round(adf_p_value, 4), "| Hurst:", round(hurst_value, 3), "\n")

             # Optional: Plot Spread and Z-Score
             # par(mfrow=c(2,1))
             # plot(spread_xts, main=paste("Log Spread:", candidate_symbol, "vs BTC"))
             # plot(zscore_xts, main=paste("Spread Z-Score (", ROLLING_WINDOW, "-day)", sep=""))
             # abline(h=c(-ZSCORE_THRESHOLD, 0, ZSCORE_THRESHOLD), col=c("red", "blue", "red"), lty=2)
             # par(mfrow=c(1,1))

        } else {
             cat("  Insufficient data points for spread analysis for", candidate_symbol, "\n")
        }
    } else {
        cat("  Price columns not found for BTC or", candidate_symbol, "\n")
    }
}

cat("Pairs trading analysis complete.\n")


# --- 8. Summary and Potential Trading Signals ---
cat("--- Step 8: Summary ---\n")

cat("\n--- Regression Summary ---\n")
for (sym in names(regression_results)) {
    cat(sym, ": Adj R^2 =", round(regression_results[[sym]]$adj_r_squared, 3),
        "| High VIF:", paste(regression_results[[sym]]$high_vif_vars, collapse=", "),
        "| DW p-val:", round(regression_results[[sym]]$dw_test_p_value, 3), "\n")
    if (length(significant_variables[[sym]]) > 0) {
      cat("  Significant Vars (p<0.05):", paste(significant_variables[[sym]], collapse=", "), "\n")
    } else {
      cat("  No significant variables found.\n")
    }
}

cat("\n--- Pairs Trading Potential Summary ---\n")
mean_reverting_candidates <- c()
for (sym in names(pairs_trading_analysis)) {
    res <- pairs_trading_analysis[[sym]]
    cat(sym, ": Potentially Mean Reverting:", res$is_potentially_mean_reverting,
        "(ADF p-val:", round(res$adf_p_value, 4), ", Hurst:", round(res$hurst_exponent, 3), ")\n")
    if (res$is_potentially_mean_reverting) {
        mean_reverting_candidates <- c(mean_reverting_candidates, sym)
        # Check current Z-Score for potential signals (using the *last* calculated Z-score)
        last_zscore <- tail(na.omit(res$zscore), 1)
        if(length(last_zscore) > 0) {
             last_zscore_value <- coredata(last_zscore)[1,1]
             cat("  Latest Z-Score:", round(last_zscore_value, 2), "\n")
             if (last_zscore_value > ZSCORE_THRESHOLD) {
                 cat("  Potential Signal: Short", sym, "/ Long BTC (Spread likely to decrease)\n")
             } else if (last_zscore_value < -ZSCORE_THRESHOLD) {
                 cat("  Potential Signal: Long", sym, "/ Short BTC (Spread likely to increase)\n")
             }
        }
    }
}

cat("\nCandidates showing potential for mean-reverting spreads with BTC:", paste(mean_reverting_candidates, collapse=", "), "\n")

cat("\n--- Analysis Finished ---\n")


# --- END OF SCRIPT ---
```

```{r}
unique(p$symbol)
```

# Test2:

```{r}

# --- 0. Setup: Install and Load Libraries ---
# install.packages(c("crypto2", "tidyverse", "PerformanceAnalytics", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider")) # Install slider if needed

# library(crypto2) # No longer needed as we use httr directly
library(tidyverse)
library(crypto2)
# library(PerformanceAnalytics) # Less needed without xts, but keep for chart.Correlation maybe
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations

# --- 1. Parameters ---
N_TOP_COINS <- 15         # Number of top coins to consider (including BTC initially)
ROLLING_WINDOW <- 30      # Rolling window for correlation and Z-score (days)
START_DATE <- Sys.Date() - years(2) # Analysis start date (e.g., 2 years back)
END_DATE <- Sys.Date()      # Analysis end date
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # Common stablecoins to exclude
VIF_THRESHOLD <- 5        # Threshold for Variance Inflation Factor
ADF_P_VALUE_THRESHOLD <- 0.05 # Threshold for ADF test significance
HURST_THRESHOLD <- 0.5    # Threshold for Hurst exponent (H < 0.5 suggests mean reversion)
ZSCORE_THRESHOLD <- 1.5   # Z-score threshold for potential trading signals


# --- 2. Data Acquisition (Using httr/jsonlite - Modified for Tibble Output) ---
cat("--- Step 2: Acquiring Data ---\n")

# Get list of coins and their current market caps (CoinGecko API - same as before)
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"
    cg_params <- list(vs_currency = "usd",
                      order = "market_cap_desc",
                      per_page = N_TOP_COINS + length(STABLECOINS) + 5, # Fetch more to allow filtering
                      page = 1,
                      sparkline = "false")
    cg_response <- GET(cg_url, query = cg_params)
    Sys.sleep(2)

    if (status_code(cg_response) == 200) {
        cg_data <- fromJSON(content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>%
            as_tibble() %>%
            select(symbol, id, market_cap) %>%
            mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else {
        stop("Failed to fetch data from CoinGecko. Status code: ", status_code(cg_response))
    }
}, error = function(e) {
    cat("Error fetching CoinGecko data:", e$message, "\n")
    stop("Cannot proceed without initial coin ranking.")
})

# Filter for Prime Candidates
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>%
    filter(!symbol %in% STABLECOINS) %>%
    filter(symbol != "BTC") %>%
    slice_head(n = N_TOP_COINS -1)

prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")

# --- Fetching using httr/jsonlite, storing in a list of tibbles ---
cat("Fetching historical data for", paste(all_symbols_of_interest, collapse=", "), "using direct API calls...\n")
all_data_list_tibbles <- list() # Store tibbles here
not_found_symbols <- c()
base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE)
if(api_limit > 2000) {
    cat("Warning: Date range exceeds typical CryptoCompare API limit (2000 days). Truncating fetch.\n")
    api_limit <- 2000
    START_DATE <- END_DATE - days(api_limit)
    cat("Adjusted START_DATE to:", format(START_DATE), "\n")
}
Sys.setenv(TZ='UTC')

for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "...\n")
    tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params)
        Sys.sleep(1.5)

        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8")
            json_data <- jsonlite::fromJSON(content)

            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && length(json_data$Data$Data) > 0 && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble() # Convert to tibble

                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>%
                        mutate(
                            timestamp = lubridate::as_datetime(time, tz = "UTC"),
                            date = as.Date(timestamp),
                            symbol = sym # Add symbol column
                        ) %>%
                        select(date, symbol, price = close, volume = volumeto) %>% # Rename for clarity
                        filter(date >= START_DATE & date <= END_DATE) %>%
                        arrange(date)

                    if(nrow(df_processed) > 0) {
                        all_data_list_tibbles[[sym]] <- df_processed # Store the tibble
                        cat(" Success. Rows fetched:", nrow(df_processed), "\n")
                    } else {
                        cat(" Success, but no data within date range for", sym, ". Skipping.\n")
                        not_found_symbols <- c(not_found_symbols, sym)
                    }
                } else {
                    cat(" API Success, but missing columns for", sym, ". Skipping.\n")
                    not_found_symbols <- c(not_found_symbols, sym)
                }
            } else {
                cat(" API returned error for", sym, ":", json_data$Message, ". Skipping.\n")
                not_found_symbols <- c(not_found_symbols, sym)
            }
        } else {
            cat(" HTTP Error:", httr::status_code(response), "for", sym, ". Skipping.\n")
            not_found_symbols <- c(not_found_symbols, sym)
        }
    }, error = function(e) {
        cat(" Error during API call/processing for", sym, ":", e$message, ". Skipping.\n")
        not_found_symbols <- c(not_found_symbols, sym)
        Sys.sleep(1.5)
    })
}

# Check if data was fetched
if (length(all_data_list_tibbles) == 0) stop("Failed to fetch data for ANY symbol. Exiting.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed to fetch Bitcoin data. Exiting.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Data fetched for BTC, but failed for all prime candidates. Exiting.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")

# Combine list of tibbles into one large tibble (long format)
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>%
    arrange(symbol, date)

# --- 3. Data Preparation (Tidyverse) ---
cat("--- Step 3: Preparing Data ---\n")

# Calculate Daily Log Returns and Log Volume Changes
# Add small constant to volume before log to avoid log(0) or log(negative)
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    group_by(symbol) %>%
    mutate(
        return = log(price) - log(lag(price)),
        log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))
    ) %>%
    ungroup() %>%
    # Remove NAs created by lag (first observation for each symbol)
    filter(!is.na(return), !is.na(log_vol_change))

# Pivot to Wide Format for correlation and regression
# Select only necessary columns before pivoting
all_data_wide <- all_data_calculated %>%
    select(date, symbol, price, return, log_vol_change) %>%
    pivot_wider(
        names_from = symbol,
        values_from = c(price, return, log_vol_change),
        names_sep = "." # Separator between metric and symbol (e.g., price.BTC, return.ETH)
    ) %>%
    arrange(date) %>%
    # Optional: Remove rows with any NAs if symbols have different start dates after filtering
    na.omit() # Be careful, this removes dates where *any* symbol might be missing

if (nrow(all_data_wide) == 0) {
    stop("No overlapping date range found across all symbols after pivoting and NA removal. Consider adjusting symbols or date range.")
}

cat("Data preparation complete. Wide data dimensions:", dim(all_data_wide), "\n")
cat("Date range in wide data:", format(min(all_data_wide$date)), "to", format(max(all_data_wide$date)), "\n")

# Define column names based on the wide format
btc_return_col_name <- "return.BTC"
candidate_return_col_names <- paste0("return.", prime_candidate_symbols)
btc_price_col_name <- "price.BTC"
candidate_price_col_names <- paste0("price.", prime_candidate_symbols)


# --- 4. Rolling Correlation Analysis (Tidyverse using slider) ---
cat("--- Step 4: Analyzing Rolling Correlations ---\n")

rolling_correlations_list <- list()

# Check if BTC return column exists
if(!(btc_return_col_name %in% colnames(all_data_wide))) {
    stop("BTC return column not found in wide data. Check pivoting.")
}

# Use slider::slide2_dbl for rolling correlation
for (candidate_ret_col in candidate_return_col_names) {
    if(candidate_ret_col %in% colnames(all_data_wide)) {
        candidate_symbol <- sub("return.", "", candidate_ret_col)
        cat("Calculating rolling correlation for:", candidate_symbol, "\n")

        # Ensure enough data points for the window
        if(nrow(all_data_wide) >= ROLLING_WINDOW) {
            # slider calculates result for each position, result length matches input length
            # It pads with NA at the beginning by default
            rolling_cor_vec <- slider::slide2_dbl(
                .x = all_data_wide[[btc_return_col_name]], # Use double brackets to get vector
                .y = all_data_wide[[candidate_ret_col]],
                .f = ~cor(.x, .y, use = "pairwise.complete.obs"), # Use pairwise to handle potential NAs within window
                .before = ROLLING_WINDOW - 1, # .before specifies how many periods before current
                .complete = FALSE # Allow calculation even if window isn't full at start (gives NA)
            )

            # Store results (optional: create a tibble with date)
            cor_tibble <- tibble(
                date = all_data_wide$date,
                rolling_cor = rolling_cor_vec
            ) %>% filter(!is.na(rolling_cor)) # Remove leading NAs where window wasn't full

             colnames(cor_tibble)[2] <- paste0("BTC_", candidate_symbol, "_RollingCor", ROLLING_WINDOW)
             rolling_correlations_list[[candidate_symbol]] <- cor_tibble

            # Optional: Plotting
             # plot_title <- paste("BTC vs", candidate_symbol, "Rolling Correlation")
             # print(ggplot(cor_tibble, aes(x = date, y = !!sym(colnames(cor_tibble)[2]))) + geom_line() + labs(title = plot_title) + theme_minimal())

        } else {
             cat(" Insufficient data points (", nrow(all_data_wide), ") for rolling correlation calculation for", candidate_symbol, "\n")
        }
    } else {
        cat(" Return column", candidate_ret_col, "not found in wide data. Skipping.\n")
    }
}
cat("Rolling correlation analysis complete.\n")


# --- 5. Explanatory Variable Analysis (Regression - Tidyverse) ---
cat("--- Step 5: Regression Analysis for Explanatory Variables ---\n")

regression_results <- list()
significant_variables <- list()

# Prepare data with lagged predictors
# Use lag within mutate on the wide data frame
# Note: Lagging reduces the number of usable rows by 1
analysis_data_regression <- all_data_wide %>%
    # Lag BTC return
    mutate(across(all_of(btc_return_col_name), ~lag(.), .names = "{.col}_lag1")) %>%
    # Lag candidate returns and log_vol_changes
    mutate(across(starts_with(c("return.", "log_vol_change.")), ~lag(.), .names = "{.col}_lag1")) %>%
    # Remove the first row containing NAs from lagging
    filter(!is.na(!!sym(paste0(btc_return_col_name, "_lag1")))) # Check lag of BTC return

if (nrow(analysis_data_regression) < 20) { # Need sufficient data for regression
    warning("Insufficient data after lagging for regression analysis (", nrow(analysis_data_regression), " rows). Skipping.")
} else {
    for (candidate_symbol in prime_candidate_symbols) {
        cat("Running regression for:", candidate_symbol, "\n")
        candidate_ret_col <- paste0("return.", candidate_symbol)
        # Define lagged predictor column names
        lagged_btc_ret_col <- paste0(btc_return_col_name, "_lag1")
        lagged_candidate_ret_col <- paste0(candidate_ret_col, "_lag1")
        lagged_candidate_vol_col <- paste0("log_vol_change.", candidate_symbol, "_lag1")

        # Check if all needed columns exist in the lagged data frame
        required_cols_reg <- c(candidate_ret_col, lagged_btc_ret_col, lagged_candidate_ret_col, lagged_candidate_vol_col)
        if (!all(required_cols_reg %in% colnames(analysis_data_regression))) {
            cat(" Missing required columns for regression on", candidate_symbol, ". Required:", paste(required_cols_reg[!required_cols_reg %in% colnames(analysis_data_regression)], collapse=", "), ". Skipping.\n")
            next
        }

        # Construct the formula dynamically using backticks for special characters like '.'
        predictor_vars <- c(lagged_btc_ret_col, lagged_candidate_ret_col, lagged_candidate_vol_col)
        formula_str <- paste0("`", candidate_ret_col, "` ~ `", paste(predictor_vars, collapse = "` + `"), "`")

        tryCatch({
            # Run lm on the tibble
            model <- lm(as.formula(formula_str), data = analysis_data_regression)
            model_summary <- summary(model)

            # Check VIF
            vif_values <- tryCatch(vif(model), error = function(e) { cat(" Could not calculate VIF. "); NULL })
            high_vif_vars <- if (!is.null(vif_values)) names(vif_values[vif_values > VIF_THRESHOLD]) else character(0)

            # Check Residuals (Durbin-Watson test)
            dw_test <- dwtest(model)

            results <- list(
                symbol = candidate_symbol,
                formula = formula_str,
                summary = model_summary,
                r_squared = model_summary$r.squared,
                adj_r_squared = model_summary$adj.r.squared,
                vif = vif_values,
                high_vif_vars = high_vif_vars,
                dw_test_p_value = dw_test$p.value
            )
            regression_results[[candidate_symbol]] <- results

            cat("  Adj R-squared:", round(results$adj_r_squared, 3),
                "| High VIF:", paste(high_vif_vars, collapse=", "),
                "| DW p-value:", round(results$dw_test_p_value, 3), "\n")

             # Identify significant variables
             coef_summary <- coef(model_summary)
             # Clean predictor names (remove backticks added for formula) for matching
             rownames(coef_summary) <- gsub("`", "", rownames(coef_summary))
             sig_vars <- rownames(coef_summary)[coef_summary[, "Pr(>|t|)"] < 0.05 & rownames(coef_summary) != "(Intercept)"]
             significant_variables[[candidate_symbol]] <- sig_vars

            # Optional: Plot residuals using ggplot
            # residuals_df <- tibble(date = analysis_data_regression$date, residuals = residuals(model))
            # print(ggplot(residuals_df, aes(x=date, y=residuals)) + geom_line() + labs(title=paste("Residuals for", candidate_symbol)) + theme_minimal())
            # print(acf(residuals(model), main=paste("ACF of Residuals for", candidate_symbol))) # acf still works

        }, error = function(e) {
            cat(" Error running regression for", candidate_symbol, ":", e$message, "\n")
        })
    }
}
cat("Regression analysis complete.\n")

# --- 6. Identify Best Variables / Composite Indicator (Logic unchanged) ---
cat("--- Step 6: Identifying Potentially Useful Variables ---\n")
all_significant_vars <- unlist(significant_variables)
variable_counts <- table(all_significant_vars)
cat("Frequency of significant variables (p<0.05) across models:\n")
print(sort(variable_counts, decreasing = TRUE))
potential_composite_vars <- names(variable_counts)[variable_counts > length(prime_candidate_symbols) * 0.3]
cat("\nVariables appearing frequently:", paste(potential_composite_vars, collapse=", "), "\n")


# --- 7. Pairs Trading Analysis (Spread Mean Reversion - Tidyverse) ---
cat("--- Step 7: Pairs Trading Spread Analysis ---\n")

pairs_trading_analysis_list <- list() # Store results as a list of tibbles

# Use the wide data frame containing prices
prices_for_pairs <- all_data_wide %>% select(date, starts_with("price."))

if(!(btc_price_col_name %in% colnames(prices_for_pairs))) {
    stop("BTC price column not found for pairs trading analysis.")
}

for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing spread for BTC vs", candidate_symbol, "\n")
    candidate_price_col <- paste0("price.", candidate_symbol)

    if (candidate_price_col %in% colnames(prices_for_pairs)) {

        # Create a temporary tibble for the pair
        pair_data <- prices_for_pairs %>%
            select(date, all_of(c(btc_price_col_name, candidate_price_col))) %>%
            # Calculate log spread
            mutate(
                spread_col_name = paste0(candidate_symbol, "_BTC_LogSpread"),
                spread = log(.data[[candidate_price_col]]) - log(.data[[btc_price_col_name]])
            )

        if (nrow(pair_data) > ROLLING_WINDOW) {
             # Test for Stationarity (ADF Test) on the spread vector
             spread_vector <- pair_data$spread
             adf_result <- tryCatch(adf.test(na.omit(spread_vector)), error = function(e) NULL)
             adf_p_value <- if (!is.null(adf_result)) adf_result$p.value else NA

             # Calculate Hurst Exponent
             hurst_result <- tryCatch(hurstexp(na.omit(spread_vector), d = ROLLING_WINDOW, display = FALSE), error = function(e) NULL)
             hurst_value <- if (!is.null(hurst_result)) hurst_result$He else NA

             # Calculate Rolling Z-Score of the spread using slider
             pair_data_zscore <- pair_data %>%
                mutate(
                    rolling_mean = slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
                    rolling_sd = slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
                    zscore_col_name = paste0(candidate_symbol, "_BTC_Spread_ZScore"),
                    zscore = (spread - rolling_mean) / rolling_sd
                ) %>%
                # Select relevant columns and remove rows with NA in zscore
                select(date, spread, zscore) %>%
                filter(!is.na(zscore))

             pairs_trading_analysis_list[[candidate_symbol]] <- list(
                 symbol = candidate_symbol,
                 spread_data = pair_data_zscore, # Tibble with date, spread, zscore
                 adf_p_value = adf_p_value,
                 hurst_exponent = hurst_value,
                 is_potentially_mean_reverting = (!is.na(adf_p_value) && adf_p_value < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value) && hurst_value < HURST_THRESHOLD)
             )

             cat("  ADF p-value:", round(adf_p_value, 4), "| Hurst:", round(hurst_value, 3), "\n")

             # Optional: Plot Spread and Z-Score using ggplot
             # plot_spread <- ggplot(pair_data_zscore, aes(x=date, y=spread)) + geom_line() + labs(title=paste("Log Spread:", candidate_symbol, "vs BTC")) + theme_minimal()
             # plot_zscore <- ggplot(pair_data_zscore, aes(x=date, y=zscore)) + geom_line() +
             #   geom_hline(yintercept=c(-ZSCORE_THRESHOLD, 0, ZSCORE_THRESHOLD), linetype="dashed", color=c("red", "blue", "red")) +
             #   labs(title=paste("Spread Z-Score (", ROLLING_WINDOW, "-day)", sep="")) + theme_minimal()
             # print(plot_spread)
             # print(plot_zscore)

        } else {
             cat("  Insufficient data points (", nrow(pair_data), ") for spread analysis for", candidate_symbol, "\n")
        }
    } else {
        cat("  Price column not found for", candidate_symbol, "\n")
    }
}
cat("Pairs trading analysis complete.\n")


# --- 8. Summary and Potential Trading Signals (Adapted for list structure) ---
cat("--- Step 8: Summary ---\n")

cat("\n--- Regression Summary ---\n")
if (length(regression_results) > 0) {
    for (sym in names(regression_results)) {
        res <- regression_results[[sym]]
        cat(sym, ": Adj R^2 =", round(res$adj_r_squared, 3),
            "| High VIF:", paste(res$high_vif_vars, collapse=", "),
            "| DW p-val:", round(res$dw_test_p_value, 3), "\n")
        if (length(significant_variables[[sym]]) > 0) {
          cat("  Significant Vars (p<0.05):", paste(significant_variables[[sym]], collapse=", "), "\n")
        } else {
          cat("  No significant variables found.\n")
        }
    }
} else {
    cat("No regression results to display.\n")
}


cat("\n--- Pairs Trading Potential Summary ---\n")
mean_reverting_candidates <- c()
if (length(pairs_trading_analysis_list) > 0) {
    for (sym in names(pairs_trading_analysis_list)) {
        res <- pairs_trading_analysis_list[[sym]]
        cat(sym, ": Potentially Mean Reverting:", res$is_potentially_mean_reverting,
            "(ADF p-val:", round(res$adf_p_value, 4), ", Hurst:", round(res$hurst_exponent, 3), ")\n")
        if (res$is_potentially_mean_reverting) {
            mean_reverting_candidates <- c(mean_reverting_candidates, sym)
            # Check current Z-Score (using the *last* row of the spread_data tibble)
            last_spread_data <- tail(res$spread_data, 1)
            if(nrow(last_spread_data) > 0) {
                 last_zscore_value <- last_spread_data$zscore
                 cat("  Latest Z-Score (", format(last_spread_data$date), "):", round(last_zscore_value, 2), "\n")
                 if (last_zscore_value > ZSCORE_THRESHOLD) {
                     cat("  Potential Signal: Short", sym, "/ Long BTC (Spread likely to decrease)\n")
                 } else if (last_zscore_value < -ZSCORE_THRESHOLD) {
                     cat("  Potential Signal: Long", sym, "/ Short BTC (Spread likely to increase)\n")
                 }
            }
        }
    }
     cat("\nCandidates showing potential for mean-reverting spreads with BTC:", paste(mean_reverting_candidates, collapse=", "), "\n")
} else {
    cat("No pairs trading analysis results to display.\n")
}

cat("\n--- Analysis Finished ---\n")

# --- END OF SCRIPT ---
```

# **Test3:**

```{r}
# --- 0. Setup: Install and Load Libraries ---
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics"))
library(crypto2)
library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics

# --- 1. Parameters ---
N_TOP_COINS <- 15         # Number of top coins to consider (incl. BTC)
HISTORY_YEARS <- 4        # <<< Increased history
BACKTEST_YEARS <- 2       # <<< Years for backtesting (out-of-sample)
ROLLING_WINDOW <- 30      # Rolling window for correlation, Z-score (days) - potentially calibrate on training data
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS) # <<< Define backtest start
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)     # <<< Define training end

STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT")
VIF_THRESHOLD <- 5
ADF_P_VALUE_THRESHOLD <- 0.05 # Threshold for ADF test significance (on training data)
HURST_THRESHOLD <- 0.5    # Threshold for Hurst exponent (on training data)
ZSCORE_ENTRY_THRESHOLD <- 1.5 # Z-score threshold for entering a trade
ZSCORE_EXIT_THRESHOLD <- 0.5  # Z-score threshold for exiting (closer to mean) - Use 0 for full reversion


# --- 2. Data Acquisition (Using httr/jsonlite - Tidyverse) ---
cat("--- Step 2: Acquiring Data ---\n")
# ... (CoinGecko fetching and filtering code remains the same) ...
# Get list of coins and their current market caps (CoinGecko API)
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"
    cg_params <- list(vs_currency = "usd", order = "market_cap_desc",
                      per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- GET(cg_url, query = cg_params)
    Sys.sleep(2)
    if (status_code(cg_response) == 200) {
        cg_data <- fromJSON(content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed to fetch data from CoinGecko. Status: ", status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })

# Filter for Prime Candidates
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS -1)
prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")

# --- Fetching using httr/jsonlite, storing in a list of tibbles ---
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list()
not_found_symbols <- c()
base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE)
if(api_limit > 2000) {
    cat("Warning: Date range may exceed API limit (~2000 days). Fetching max possible up to END_DATE.\n")
    api_limit <- 2000 # Adjust limit, CryptoCompare might handle >2000 sometimes
}
Sys.setenv(TZ='UTC')

for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "...\n")
    tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params)
        Sys.sleep(1.5)
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8")
            json_data <- jsonlite::fromJSON(content)
            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>%
                        mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>%
                        select(date, symbol, price = close, volume = volumeto) %>%
                        # Filter here ensures we only keep data within the MAX requested range
                        filter(date >= START_DATE & date <= END_DATE) %>%
                        arrange(date)
                    if(nrow(df_processed) > 0) {
                        all_data_list_tibbles[[sym]] <- df_processed
                        cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else { cat(" No data within specified date range for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
                } else { cat(" API Success, but missing columns for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
            } else { cat(" API returned error for", sym, ":", json_data$Message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
        } else { cat(" HTTP Error:", httr::status_code(response), "for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
    }, error = function(e) { cat(" Error fetching/processing", sym, ":", e$message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym); Sys.sleep(1.5) })
}

# Check fetched data
if (length(all_data_list_tibbles) == 0) stop("Failed to fetch ANY data. Exiting.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed to fetch BTC data. Exiting.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Fetched BTC, but failed for all candidates. Exiting.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")

# Combine into one long tibble
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>% arrange(symbol, date)

# --- 3. Data Preparation (Tidyverse - Returns) ---
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    # Ensure price and volume are numeric
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>%
    group_by(symbol) %>%
    # Sort within group just to be sure before lagging
    arrange(date) %>%
    mutate(
        return = log(price) - log(lag(price)),
        log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))
    ) %>%
    ungroup() %>%
    filter(!is.na(return)) # Keep NAs for log_vol_change if volume was originally 0/NA

# --- 4. Training Period Analysis ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")

training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)

regression_results_train <- list()
pairs_trading_analysis_train <- list()
potentially_mean_reverting_pairs <- c() # Store symbols identified in training

for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training Period)\n")

    # --- 4a. Prepare Pair-Specific Data for Training Period ---
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, btc_log_vol_change = log_vol_change)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return, candidate_log_vol_change = log_vol_change)

    # Inner join ensures common dates for this specific pair in the training period
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)

    if(nrow(pair_data_train) < ROLLING_WINDOW + 5) { # Need sufficient overlap
         cat(" Insufficient overlapping data points (", nrow(pair_data_train), ") for BTC-", candidate_symbol, " in training period. Skipping pair.\n")
         next
    }

    # --- 4b. Rolling Correlation (Training Period) ---
    # (Optional - Not strictly needed for backtest but good context)
    # rolling_cor_train <- slider::slide2_dbl(...)

    # --- 4c. Regression Analysis (Training Period) ---
    pair_data_train_lagged <- pair_data_train %>%
        mutate(
            btc_return_lag1 = lag(btc_return),
            candidate_return_lag1 = lag(candidate_return),
            candidate_log_vol_change_lag1 = lag(candidate_log_vol_change)
        ) %>%
        filter(!is.na(btc_return_lag1)) # Remove NA row from lagging

    if (nrow(pair_data_train_lagged) >= 20) {
        formula_str_train <- "candidate_return ~ btc_return_lag1 + candidate_return_lag1 + candidate_log_vol_change_lag1"
        tryCatch({
            model_train <- lm(as.formula(formula_str_train), data = pair_data_train_lagged)
            # Store summary, VIF, DW test results if needed for analysis
            # regression_results_train[[candidate_symbol]] <- summary(model_train) # etc.
            # vif_train <- car::vif(model_train)
            # dw_train <- lmtest::dwtest(model_train)
            cat("  Regression successful for", candidate_symbol, "(Training).\n")
        }, error = function(e) { cat("  Regression failed for", candidate_symbol, "(Training):", e$message, "\n") })
    } else {
         cat("  Insufficient data after lagging for regression on", candidate_symbol, "(Training).\n")
    }

    # --- 4d. Pairs Trading Spread Analysis (Training Period) ---
    pair_data_train_spread <- pair_data_train %>%
        # Ensure prices are positive before taking log
        filter(candidate_price > 0, btc_price > 0) %>%
        mutate(spread = log(candidate_price) - log(btc_price))

    if(nrow(pair_data_train_spread) > ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        # ADF Test
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL)
        adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        # Hurst Exponent
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = ROLLING_WINDOW, display=FALSE), error = function(e) NULL)
        hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA

        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) ||
                                   (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)

        pairs_trading_analysis_train[[candidate_symbol]] <- list(
            adf_p_value = adf_p_value_train,
            hurst_exponent = hurst_value_train,
            is_potentially_mean_reverting = is_mean_reverting_train
        )
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) {
            cat("  >>> Pair BTC-", candidate_symbol, "identified as potentially mean-reverting in training period. <<<\n")
            potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol)
        }
    } else {
        cat("  Insufficient spread data for ADF/Hurst tests on", candidate_symbol, "(Training).\n")
    }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
} # End loop through candidate symbols for training analysis

cat("--- Training Period Analysis Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")

# --- 5. Backtesting Period ---
cat("--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")

if (length(potentially_mean_reverting_pairs) == 0) {
    stop("No potentially mean-reverting pairs identified in the training period. Cannot proceed with backtest.")
}

# Get full data for the identified pairs + BTC
backtest_data_long <- all_data_calculated %>%
    filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>%
    filter(date >= BACKTEST_START_DATE) # Filter for backtest dates

# Store daily returns for each pair's strategy
strategy_returns_list <- list()

for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")

    # --- 5a. Prepare Pair-Specific Data for Backtesting Period ---
    btc_data_backtest <- backtest_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_backtest <- backtest_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)

    # Inner join ensures common dates for the backtest period
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>%
        # Ensure prices > 0 for spread calculation
        filter(btc_price > 0, candidate_price > 0) %>%
        arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) {
         cat(" Insufficient overlapping data points (", nrow(pair_data_backtest), ") for BTC-", candidate_symbol, " in backtest period. Skipping pair.\n")
         next
    }

    # --- 5b. Calculate Spread and Z-Score (Backtest Period) ---
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            zscore = (spread - rolling_mean) / rolling_sd
        ) %>%
        # Remove initial rows where rolling calculations are NA
        filter(!is.na(zscore), !is.na(rolling_sd), rolling_sd > 1e-6) # Avoid division by zero/tiny SD

    if(nrow(pair_data_signals) < 2) {
         cat(" Insufficient data after Z-score calculation for BTC-", candidate_symbol, ". Skipping pair.\n")
         next
    }

    # --- 5c. Generate Trading Signals and Positions ---
    pair_data_trades <- pair_data_signals %>%
        mutate(
            # Signal based on Z-score thresholds
            signal = case_when(
                zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, # Short the spread (Short Candidate / Long BTC)
                zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1,  # Long the spread (Long Candidate / Short BTC)
                # Exit condition: cross back towards zero
                abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0,
                TRUE ~ NA_real_ # Otherwise, keep current signal/position (fill later)
            ),
            # Fill missing signals with the previous signal (hold position)
            signal = zoo::na.locf(signal, na.rm = FALSE),
            signal = ifelse(is.na(signal), 0, signal), # If initial rows were NA, start flat
            # Position is the signal from the *previous* day (trade based on yesterday's signal)
            position = lag(signal, default = 0)
        )

    # --- 5d. Calculate Strategy Returns ---
    # Return = position_candidate * return_candidate + position_btc * return_btc
    # Position = +1 => Long Candidate (+1), Short BTC (-1)
    # Position = -1 => Short Candidate (-1), Long BTC (+1)
    # Position = 0 => Flat (0)
    pair_strategy_returns <- pair_data_trades %>%
        mutate(
            strategy_return = case_when(
                position == 1 ~ (1 * candidate_return) + (-1 * btc_return),
                position == -1 ~ (-1 * candidate_return) + (1 * btc_return),
                TRUE ~ 0 # Position is 0 (flat)
            ),
            # --- FIX: Create the 'symbol' column correctly ---
            symbol = candidate_symbol
        ) %>%
        # Now select only the columns we need for the final result list
        select(date, symbol, strategy_return)

    strategy_returns_list[[candidate_symbol]] <- pair_strategy_returns
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")

} # End loop through candidate symbols for backtesting

# --- 6. Aggregate Strategy Returns and Performance Metrics ---
cat("--- Step 6: Aggregating Returns and Calculating Performance ---\n")

if (length(strategy_returns_list) == 0) {
    stop("Strategy returns could not be calculated for any pair during backtest.")
}

# Combine returns from all pairs
all_strategy_returns_long <- bind_rows(strategy_returns_list)

# Calculate equal-weighted portfolio return per day
# This assumes we deploy capital equally among all active pairs each day
# More sophisticated allocation would require portfolio value tracking
portfolio_daily_returns <- all_strategy_returns_long %>%
    group_by(date) %>%
    # Average the returns of all pairs traded on that day
    summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>%
    arrange(date)

# Convert to xts for PerformanceAnalytics
portfolio_returns_xts <- xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
colnames(portfolio_returns_xts) <- "Strategy"

# Calculate Performance Metrics
cat("\n--- Backtest Performance Metrics (Equal Weighted Pairs) ---\n")
stats_table <- table.AnnualizedReturns(portfolio_returns_xts, scale = 252, geometric = FALSE) # Use scale=252 for daily data
sharpe_ratio <- SharpeRatio.annualized(portfolio_returns_xts, Rf = 0, scale = 252, geometric = FALSE)
max_drawdown <- maxDrawdown(portfolio_returns_xts)
calmar_ratio <- CalmarRatio(portfolio_returns_xts, scale = 252)

cat("Annualized Return:", round(stats_table["Annualized Return",], 4), "\n")
cat("Annualized Std Dev:", round(stats_table["Annualized Stdev",], 4), "\n")
cat("Annualized Sharpe (Rf=0%):", round(sharpe_ratio, 3), "\n")
cat("Maximum Drawdown:", round(max_drawdown, 4), "\n")
cat("Calmar Ratio:", round(calmar_ratio, 3), "\n")

# Plot Cumulative Returns
charts.PerformanceSummary(portfolio_returns_xts, main = "Pairs Trading Strategy Performance (Backtest)")

# --- 7. Notes on Dynamic Portfolio Rebalancing ---
cat("\n--- Step 7: Notes on Dynamic Portfolio Rebalancing ---\n")
cat("Dynamic rebalancing involves adjusting positions periodically (e.g., daily, weekly) to maintain desired target weights or risk exposure.\n")
cat("Why it matters for Pairs Trading:\n")
cat(" 1. Maintaining Dollar Neutrality: If prices diverge significantly, a dollar-neutral pair (e.g., long $1000 BTC, short $1000 ETH) can become unbalanced.\n")
cat(" 2. Maintaining Target Ratio: If the pair is based on a specific price ratio, rebalancing might bring it back towards that ratio.\n")
cat(" 3. Risk Management: Prevents excessive exposure to one leg of the pair.\n")
cat("Implementation Considerations:\n")
cat(" - Frequency: How often to rebalance (daily, weekly, threshold-based)?\n")
cat(" - Thresholds: Rebalance only if the imbalance exceeds a certain percentage?\n")
cat(" - Transaction Costs: Frequent rebalancing incurs higher costs.\n")
cat(" - Mechanism: Calculate current value of long/short legs, determine necessary trades to restore balance.\n")
cat("Where it fits: The rebalancing logic would typically run *daily* within the backtest loop, *after* calculating the day's P&L based on the existing position, and *before* determining the *next* day's position based on the Z-score signal. It might adjust the sizes of the positions held.\n")
cat("NOTE: The current backtest does *not* implement dynamic rebalancing; it assumes fixed positions (+1/-1 or 0) determined solely by the Z-score signals.\n")


cat("\n--- Analysis and Backtesting Finished ---\n")
```

# Test4:

```{r}
# --- 0. Setup: Install and Load Libraries ---
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra")) # Added gridExtra for plotting

library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics
library(gridExtra) # For arranging plots

# --- 1. Parameters ---
N_TOP_COINS <- 15
HISTORY_YEARS <- 4
BACKTEST_YEARS <- 2
ROLLING_WINDOW <- 30
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS)
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT")
VIF_THRESHOLD <- 5
ADF_P_VALUE_THRESHOLD <- 0.05
HURST_THRESHOLD <- 0.5
ZSCORE_ENTRY_THRESHOLD <- 1.5
ZSCORE_EXIT_THRESHOLD <- 0.5
# Add a small value to SD to prevent division by zero in Z-score if SD is exactly 0
SD_OFFSET <- 1e-9

# --- 2. Data Acquisition (Using httr/jsonlite - Tidyverse) ---
# ... (Keep the existing code from Step 2 - Data Acquisition) ...
cat("--- Step 2: Acquiring Data ---\n")
# Get list of coins and their current market caps (CoinGecko API)
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"
    cg_params <- list(vs_currency = "usd", order = "market_cap_desc",
                      per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- GET(cg_url, query = cg_params)
    Sys.sleep(2)
    if (status_code(cg_response) == 200) {
        cg_data <- fromJSON(content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed to fetch data from CoinGecko. Status: ", status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })

# Filter for Prime Candidates
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS -1)
prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")

# Fetching using httr/jsonlite, storing in a list of tibbles
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list()
not_found_symbols <- c()
base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE)
if(api_limit > 2000) {
    cat("Warning: Date range may exceed API limit (~2000 days). Fetching max possible up to END_DATE.\n")
    api_limit <- 2000
}
Sys.setenv(TZ='UTC')

for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "...\n")
    tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params)
        Sys.sleep(1.5)
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8")
            json_data <- jsonlite::fromJSON(content)
            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>%
                        mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>%
                        select(date, symbol, price = close, volume = volumeto) %>%
                        filter(date >= START_DATE & date <= END_DATE) %>%
                        arrange(date)
                    if(nrow(df_processed) > 0) {
                        all_data_list_tibbles[[sym]] <- df_processed
                        cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else { cat(" No data within specified date range for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
                } else { cat(" API Success, but missing columns for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
            } else { cat(" API returned error for", sym, ":", json_data$Message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
        } else { cat(" HTTP Error:", httr::status_code(response), "for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
    }, error = function(e) { cat(" Error fetching/processing", sym, ":", e$message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym); Sys.sleep(1.5) })
}

# Check fetched data
if (length(all_data_list_tibbles) == 0) stop("Failed to fetch ANY data. Exiting.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed to fetch BTC data. Exiting.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Fetched BTC, but failed for all candidates. Exiting.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")

# Combine into one long tibble
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>% arrange(symbol, date)


# --- 3. Data Preparation (Tidyverse - Returns) ---
# ... (Keep the existing code from Step 3 - Calculating Returns) ...
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>%
    group_by(symbol) %>%
    arrange(date) %>%
    mutate(
        return = log(price) - log(lag(price)),
        log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))
    ) %>%
    ungroup() %>%
    filter(!is.na(return))


# --- 4. Training Period Analysis ---
# ... (Keep the existing code from Step 4 - Training Period Analysis) ...
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)
regression_results_train <- list()
pairs_trading_analysis_train <- list()
potentially_mean_reverting_pairs <- c()

for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training Period)\n")
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, btc_log_vol_change = log_vol_change)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return, candidate_log_vol_change = log_vol_change)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)

    if(nrow(pair_data_train) < ROLLING_WINDOW + 5) {
         cat(" Insufficient overlapping data points (", nrow(pair_data_train), ") for BTC-", candidate_symbol, " in training period. Skipping pair.\n")
         next
    }

    # Regression Analysis (Optional detail storing omitted for brevity)
    pair_data_train_lagged <- pair_data_train %>%
        mutate(across(ends_with(c("return", "log_vol_change")), lag, .names = "{.col}_lag1")) %>%
        filter(complete.cases(.)) # Ensure all lagged columns are non-NA

    if (nrow(pair_data_train_lagged) >= 20) {
        formula_str_train <- "candidate_return ~ btc_return_lag1 + candidate_return_lag1 + candidate_log_vol_change_lag1"
        tryCatch({
            model_train <- lm(as.formula(formula_str_train), data = pair_data_train_lagged)
            cat("  Regression successful for", candidate_symbol, "(Training).\n")
        }, error = function(e) { cat("  Regression failed for", candidate_symbol, "(Training):", e$message, "\n") })
    } else { cat("  Insufficient data after lagging for regression on", candidate_symbol, "(Training).\n") }

    # Pairs Trading Spread Analysis (Training Period)
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL)
        adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = ROLLING_WINDOW, display=FALSE), error = function(e) NULL)
        hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) {
            cat("  >>> Pair BTC-", candidate_symbol, "identified as potentially mean-reverting in training period. <<<\n")
            potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol)
        }
    } else { cat("  Insufficient spread data for ADF/Hurst tests on", candidate_symbol, "(Training).\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Analysis Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")

# --- 5. Backtesting Period ---
cat("--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")

if (length(potentially_mean_reverting_pairs) == 0) {
    stop("No potentially mean-reverting pairs identified. Cannot backtest.")
}

backtest_data_long <- all_data_calculated %>%
    filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>%
    filter(date >= BACKTEST_START_DATE)

# <<< MODIFICATION: Store detailed results for each pair >>>
backtest_details <- list()

for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    btc_data_backtest <- backtest_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_backtest <- backtest_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) {
         cat(" Insufficient overlapping data points for BTC-", candidate_symbol, " in backtest. Skipping pair.\n")
         next
    }

    # Calculate Spread, Z-Score, Signals, Position
    pair_data_trades <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            # Ensure rolling_sd is not zero or NA before calculating zscore
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        filter(!is.na(zscore)) # Remove initial rows where rolling calcs are NA

     if(nrow(pair_data_trades) < 2) {
         cat(" Insufficient data after Z-score calculation for BTC-", candidate_symbol, ". Skipping pair.\n")
         next
     }

     pair_data_trades <- pair_data_trades %>%
         mutate(
            signal = case_when(
                zscore > ZSCORE_ENTRY_THRESHOLD ~ -1,
                zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1,
                abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0,
                TRUE ~ NA_real_
            ),
            signal = zoo::na.locf(signal, na.rm = FALSE),
            signal = ifelse(is.na(signal), 0, signal),
            position = lag(signal, default = 0)
        )

    # Calculate Strategy Returns for this pair
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate(
            strategy_return = case_when(
                position == 1 ~ (1 * candidate_return) + (-1 * btc_return),
                position == -1 ~ (-1 * candidate_return) + (1 * btc_return),
                TRUE ~ 0
            ),
            symbol = candidate_symbol
        ) %>%
        select(date, symbol, strategy_return)

    # <<< Store details for this pair >>>
    backtest_details[[candidate_symbol]] <- list(
        trade_data = pair_data_trades, # Contains spread, zscore, position etc.
        returns_data = pair_strategy_returns_tibble
    )

    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
}

# --- 6. Aggregate & Individual Performance Metrics ---
cat("--- Step 6: Calculating Performance Metrics ---\n")

if (length(backtest_details) == 0) {
    stop("Strategy returns could not be calculated for any pair during backtest.")
}

# --- 6a. Aggregate Portfolio Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>%
    group_by(date) %>%
    summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>%
    arrange(date)
portfolio_returns_xts <- xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
colnames(portfolio_returns_xts) <- "Aggregate Strategy"

cat("\n--- AGGREGATE Backtest Performance Metrics (Equal Weighted Pairs) ---\n")
stats_table_agg <- table.AnnualizedReturns(portfolio_returns_xts, scale = 252, geometric = FALSE)
sharpe_agg <- SharpeRatio.annualized(portfolio_returns_xts, Rf = 0, scale = 252, geometric = FALSE)
max_dd_agg <- maxDrawdown(portfolio_returns_xts)
calmar_agg <- CalmarRatio(portfolio_returns_xts, scale = 252)

print(stats_table_agg)
cat("Annualized Sharpe (Rf=0%):", round(sharpe_agg[1,], 3), "\n") # Sharpe is often returned as xts
cat("Maximum Drawdown:", round(max_dd_agg, 4), "\n")
cat("Calmar Ratio:", round(calmar_agg, 3), "\n")

# --- 6b. Individual Pair Performance ---
cat("\n--- INDIVIDUAL Pair Backtest Performance Metrics ---\n")
individual_metrics_list <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data
    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_xts <- xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol

        stats_table_pair <- table.AnnualizedReturns(pair_returns_xts, scale = 252, geometric = FALSE)
        sharpe_pair <- SharpeRatio.annualized(pair_returns_xts, Rf = 0, scale = 252, geometric = FALSE)
        max_dd_pair <- maxDrawdown(pair_returns_xts)
        calmar_pair <- CalmarRatio(pair_returns_xts, scale = 252)

        print(stats_table_pair)
        cat(" Annualized Sharpe (Rf=0%):", round(sharpe_pair[1,], 3), "\n")
        cat(" Maximum Drawdown:", round(max_dd_pair, 4), "\n")
        cat(" Calmar Ratio:", round(calmar_pair, 3), "\n\n")

        # Store metrics
        individual_metrics_list[[candidate_symbol]] <- list(
            AnnualizedReturn = stats_table_pair["Annualized Return",],
            AnnualizedStdDev = stats_table_pair["Annualized Stdev",],
            AnnualizedSharpe = sharpe_pair[1,],
            MaxDrawdown = max_dd_pair,
            CalmarRatio = calmar_pair
        )

    } else {
        cat(" Insufficient return data to calculate metrics for", candidate_symbol, "\n\n")
    }
}

# --- 7. Plotting Spreads and Signals ---
cat("--- Step 7: Plotting Spreads and Signals ---\n")

for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data

    # Identify trade entries (where position changes from 0 to non-zero)
    trade_entries <- pair_plot_data %>%
        filter(position != 0 & lag(position, default = 0) == 0) %>%
        mutate(entry_type = ifelse(position == 1, "Long Spread", "Short Spread"))

    # Plot 1: Spread
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) +
        geom_line() +
        labs(title = paste("BTC vs", candidate_symbol, ": Log Spread (Backtest Period)"),
             x = "Date", y = "Log Spread") +
        theme_minimal()

    # Plot 2: Z-Score with Signals
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) +
        geom_line() +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") +
        geom_hline(yintercept = ZSCORE_EXIT_THRESHOLD, color = "blue", linetype = "dotted") +
        geom_hline(yintercept = -ZSCORE_EXIT_THRESHOLD, color = "blue", linetype = "dotted") +
        geom_hline(yintercept = 0, color = "grey", linetype = "solid") +
        # Add points for trade entries
        geom_point(data = trade_entries, aes(color = entry_type), size = 2.5) +
        scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("BTC vs", candidate_symbol, ": Spread Z-Score & Entries (Backtest Period)"),
             x = "Date", y = "Z-Score", color = "Entry Signal") +
        theme_minimal() +
        theme(legend.position = "bottom")

    # Print plots (use grid.arrange to show both)
    tryCatch({
      grid.arrange(plot_spread, plot_zscore, ncol = 1)
    }, error = function(e){cat("Could not arrange plots for", candidate_symbol, ":", e$message, "\n")})

    # Optionally: Plot cumulative return for individual pair
    pair_returns_xts <- xts(backtest_details[[candidate_symbol]]$returns_data$strategy_return, order.by = backtest_details[[candidate_symbol]]$returns_data$date)
    chart.CumReturns(pair_returns_xts, main = paste("Cumulative Return: BTC vs", candidate_symbol), geometric = FALSE)

    cat(" Plots generated for", candidate_symbol, "\n")
}

# --- 8. Notes on Dynamic Portfolio Rebalancing ---
# ... (Keep the existing code from the previous Step 7 - Notes on Dynamic Rebalancing) ...
cat("\n--- Step 8: Notes on Dynamic Portfolio Rebalancing ---\n")
cat("Dynamic rebalancing involves adjusting positions periodically (e.g., daily, weekly) to maintain desired target weights or risk exposure.\n")
cat("Why it matters for Pairs Trading:\n")
cat(" 1. Maintaining Dollar Neutrality: If prices diverge significantly, a dollar-neutral pair (e.g., long $1000 BTC, short $1000 ETH) can become unbalanced.\n")
cat(" 2. Maintaining Target Ratio: If the pair is based on a specific price ratio, rebalancing might bring it back towards that ratio.\n")
cat(" 3. Risk Management: Prevents excessive exposure to one leg of the pair.\n")
cat("Implementation Considerations:\n")
cat(" - Frequency: How often to rebalance (daily, weekly, threshold-based)?\n")
cat(" - Thresholds: Rebalance only if the imbalance exceeds a certain percentage?\n")
cat(" - Transaction Costs: Frequent rebalancing incurs higher costs.\n")
cat(" - Mechanism: Calculate current value of long/short legs, determine necessary trades to restore balance.\n")
cat("Where it fits: The rebalancing logic would typically run *daily* within the backtest loop, *after* calculating the day's P&L based on the existing position, and *before* determining the *next* day's position based on the Z-score signal. It might adjust the sizes of the positions held.\n")
cat("NOTE: The current backtest does *not* implement dynamic rebalancing; it assumes fixed positions (+1/-1 or 0) determined solely by the Z-score signals.\n")


cat("\n--- Analysis, Backtesting, and Plotting Finished ---\n")
```

**Test5:**

```{r}
# --- 0. Setup: Install and Load Libraries ---
# Ensure required packages are installed. Run this once if needed:
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra"))

library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics
library(gridExtra) # For arranging plots

# --- 1. Parameters ---
N_TOP_COINS <- 15         # Number of top coins to consider (incl. BTC)
HISTORY_YEARS <- 4        # Total history to fetch
BACKTEST_YEARS <- 2       # Years for backtesting (out-of-sample)
ROLLING_WINDOW <- 30      # Rolling window for correlation, Z-score (days)
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS) # Define backtest start
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)     # Define training end

STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # Common stablecoins to exclude

# Analysis Thresholds
VIF_THRESHOLD <- 5        # Threshold for Variance Inflation Factor in regression
ADF_P_VALUE_THRESHOLD <- 0.05 # Threshold for ADF test significance (on training data)
HURST_THRESHOLD <- 0.5    # Threshold for Hurst exponent (on training data, H < 0.5 suggests mean reversion)

# Trading Strategy Parameters
ZSCORE_ENTRY_THRESHOLD <- 1.5 # Z-score threshold for entering a trade
ZSCORE_EXIT_THRESHOLD <- 0.5  # Z-score threshold for exiting (closer to mean)
SD_OFFSET <- 1e-9           # Small value to prevent division by zero in Z-score

# Position Sizing Parameters
MAX_POSITION_SCALAR <- 2.0 # Max position size relative to base (e.g., 2.0 means up to 2x base size)
SCALING_FACTOR <- 0.5     # How much extra size per unit of Z beyond threshold (adjust as needed)


# --- 2. Data Acquisition (Using httr/jsonlite - Tidyverse) ---
cat("--- Step 2: Acquiring Data ---\n")
# Get list of coins and their current market caps (CoinGecko API)
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"
    cg_params <- list(vs_currency = "usd", order = "market_cap_desc",
                      per_page = N_TOP_COINS + length(STABLECOINS) + 10, # Fetch bit more for filtering
                      page = 1, sparkline = "false")
    cg_response <- httr::GET(cg_url, query = cg_params)
    Sys.sleep(2) # Be nice to API
    if (httr::status_code(cg_response) == 200) {
        cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>%
            as_tibble() %>%
            select(symbol, id, market_cap) %>%
            mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else {
        stop("Failed to fetch data from CoinGecko. Status: ", httr::status_code(cg_response))
    }
}, error = function(e) {
    cat("Error fetching CoinGecko data:", e$message, "\n")
    stop("Cannot proceed without initial coin ranking.")
})

# Filter for Prime Candidates
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>%
    filter(!symbol %in% STABLECOINS) %>%        # Exclude stablecoins
    filter(symbol != "BTC") %>%                 # Exclude Bitcoin itself
    slice_head(n = N_TOP_COINS - 1)             # Take the top N-1

prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")

# Fetching historical data using httr/jsonlite, storing in a list of tibbles
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list()
not_found_symbols <- c()
base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
# Calculate 'limit' based on date range for the API call
api_limit <- as.numeric(END_DATE - START_DATE)
# Check API limit (approx 2000 for free tier histoday)
if(api_limit > 2000) {
    cat("Warning: Date range may exceed typical CryptoCompare API limit (~2000 days). Fetching max possible up to END_DATE.\n")
    api_limit <- 2000 # Adjust limit, CryptoCompare might handle >2000 sometimes
}
Sys.setenv(TZ='UTC') # Set timezone to UTC for consistency

for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "...\n")
    tryCatch({
        # Construct API URL and Query
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit,
                             toTs = as.numeric(as.POSIXct(END_DATE))) # End date as Unix timestamp
        # Make API request
        response <- httr::GET(base_url, query = query_params)
        Sys.sleep(1.5) # Pause *after* the call

        # Check HTTP status and API response
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8")
            json_data <- jsonlite::fromJSON(content)

            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                # Check for required columns
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>%
                        mutate(
                            timestamp = lubridate::as_datetime(time, tz = "UTC"),
                            date = as.Date(timestamp),
                            symbol = sym # Add symbol column
                        ) %>%
                        select(date, symbol, price = close, volume = volumeto) %>% # Rename for clarity
                        # Filter here ensures we only keep data within the MAX requested range
                        filter(date >= START_DATE & date <= END_DATE) %>%
                        arrange(date)

                    if(nrow(df_processed) > 0) {
                        all_data_list_tibbles[[sym]] <- df_processed
                        cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else {
                        cat(" Success, but no data returned within the specified date range for", sym, ". Skipping.\n")
                        not_found_symbols <- c(not_found_symbols, sym)
                    }
                } else {
                    cat(" API Success, but returned data frame is missing required columns for", sym, ". Skipping.\n")
                    not_found_symbols <- c(not_found_symbols, sym)
                }
            } else {
                # API returned an error message
                cat(" API returned error for", sym, ":", json_data$Message, ". Skipping.\n")
                not_found_symbols <- c(not_found_symbols, sym)
            }
        } else {
            # HTTP request failed
            cat(" HTTP Error:", httr::status_code(response), "when fetching data for", sym, ". Skipping.\n")
            not_found_symbols <- c(not_found_symbols, sym)
        }
    }, error = function(e) {
        # Catch errors during GET, parsing, or processing
        cat(" Error fetching/processing", sym, ":", e$message, ". Skipping.\n")
        not_found_symbols <- c(not_found_symbols, sym)
        Sys.sleep(1.5) # Pause even after error
    })
}

# Check if data was fetched successfully
if (length(all_data_list_tibbles) == 0) stop("Failed to fetch historical data for ANY symbol. Exiting.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed to fetch Bitcoin data. Exiting.")
# Update prime_candidate_symbols based on successful fetches
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Data fetched for BTC, but failed for all prime candidates. Exiting.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")

# Combine list of tibbles into one large tibble (long format)
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>%
    arrange(symbol, date)


# --- 3. Data Preparation (Tidyverse - Returns) ---
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6 # Add small constant to volume before log to avoid log(0)
all_data_calculated <- all_data_long %>%
    # Ensure price and volume are numeric
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>%
    group_by(symbol) %>%
    # Sort within group just to be sure before lagging
    arrange(date) %>%
    mutate(
        return = log(price) - log(lag(price)),
        log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))
    ) %>%
    ungroup() %>%
    # Remove NAs created by lag (first observation for each symbol's return)
    filter(!is.na(return))


# --- 4. Training Period Analysis ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")

training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)

regression_results_train <- list() # Store regression results if needed later
pairs_trading_analysis_train <- list() # Store ADF/Hurst results
potentially_mean_reverting_pairs <- c() # Store symbols identified in training

for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training Period)\n")

    # --- 4a. Prepare Pair-Specific Data for Training Period ---
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, btc_log_vol_change = log_vol_change)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return, candidate_log_vol_change = log_vol_change)

    # Inner join ensures common dates for this specific pair in the training period
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)

    if(nrow(pair_data_train) < ROLLING_WINDOW + 5) { # Need sufficient overlap for rolling calculations + tests
         cat(" Insufficient overlapping data points (", nrow(pair_data_train), ") for BTC-", candidate_symbol, " in training period. Skipping pair.\n")
         next
    }

    # --- 4b. Rolling Correlation (Training Period) ---
    # (Optional analysis - Not strictly needed for backtest signal but good context)
    # rolling_cor_train <- slider::slide2_dbl(...)

    # --- 4c. Regression Analysis (Training Period) ---
    # Prepare lagged data for regression
    pair_data_train_lagged <- pair_data_train %>%
        mutate(
            btc_return_lag1 = lag(btc_return),
            candidate_return_lag1 = lag(candidate_return),
            # Use coalesce to handle potential NA if log_vol_change was NA
            candidate_log_vol_change_lag1 = lag(coalesce(candidate_log_vol_change, 0))
        ) %>%
        filter(!is.na(btc_return_lag1), !is.na(candidate_return_lag1), !is.na(candidate_log_vol_change_lag1)) # Ensure all predictors are valid

    if (nrow(pair_data_train_lagged) >= 20) { # Need sufficient data points for lm
        formula_str_train <- "candidate_return ~ btc_return_lag1 + candidate_return_lag1 + candidate_log_vol_change_lag1"
        tryCatch({
            model_train <- lm(as.formula(formula_str_train), data = pair_data_train_lagged)
            # Store results if needed: summary(model_train), car::vif(model_train), lmtest::dwtest(model_train)
            # regression_results_train[[candidate_symbol]] <- summary(model_train) # Example
            cat("  Regression successful for", candidate_symbol, "(Training).\n")
        }, error = function(e) { cat("  Regression failed for", candidate_symbol, "(Training):", e$message, "\n") })
    } else {
         cat("  Insufficient data after lagging for regression on", candidate_symbol, "(Training).\n")
    }

    # --- 4d. Pairs Trading Spread Analysis (Training Period) ---
    pair_data_train_spread <- pair_data_train %>%
        # Ensure prices are positive before taking log
        filter(candidate_price > 0, btc_price > 0) %>%
        mutate(spread = log(candidate_price) - log(btc_price))

    if(nrow(pair_data_train_spread) > ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        # ADF Test
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) {cat(" ADF Error:", e$message); NULL})
        adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        # Hurst Exponent
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = ROLLING_WINDOW, display=FALSE), error = function(e) {cat(" Hurst Error:", e$message); NULL})
        hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA

        # Check if potentially mean reverting based on thresholds
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) ||
                                   (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)

        pairs_trading_analysis_train[[candidate_symbol]] <- list(
            adf_p_value = adf_p_value_train,
            hurst_exponent = hurst_value_train,
            is_potentially_mean_reverting = is_mean_reverting_train
        )
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) {
            cat("  >>> Pair BTC-", candidate_symbol, "identified as potentially mean-reverting in training period. <<<\n")
            potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol)
        }
    } else {
        cat("  Insufficient spread data for ADF/Hurst tests on", candidate_symbol, "(Training).\n")
    }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
} # End loop through candidate symbols for training analysis

cat("--- Training Period Analysis Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")


# --- 5. Backtesting Period ---
cat("--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")

if (length(potentially_mean_reverting_pairs) == 0) {
    stop("No potentially mean-reverting pairs identified in the training period. Cannot proceed with backtest.")
}

# Get full data for the identified pairs + BTC for the backtest period
backtest_data_long <- all_data_calculated %>%
    filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>%
    filter(date >= BACKTEST_START_DATE)

# Store detailed results for each pair during the backtest
backtest_details <- list()

for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")

    # --- 5a. Prepare Pair-Specific Data for Backtesting Period ---
    btc_data_backtest <- backtest_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_backtest <- backtest_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)

    # Inner join ensures common dates for the backtest period for this pair
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>%
        # Ensure prices > 0 for spread calculation
        filter(btc_price > 0, candidate_price > 0) %>%
        arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) {
         cat(" Insufficient overlapping data points (", nrow(pair_data_backtest), ") for BTC-", candidate_symbol, " in backtest period. Skipping pair.\n")
         next
    }

    # --- 5b. Calculate Spread and Z-Score (Backtest Period) ---
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            # Add offset to SD and check for NA before calculating Z-score
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        # Remove initial rows where rolling calculations are NA
        filter(!is.na(zscore))

    if(nrow(pair_data_signals) < 2) {
         cat(" Insufficient data after Z-score calculation for BTC-", candidate_symbol, ". Skipping pair.\n")
         next
    }

    # --- 5c. Generate Signals and Scaled Positions ---
    pair_data_trades <- pair_data_signals %>%
        mutate(
            # Determine signal direction based on Z-score crossing thresholds
            current_signal = case_when(
                zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, # Potential Short Spread
                zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1,  # Potential Long Spread
                abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, # Exit condition
                TRUE ~ NA_real_ # Maintain signal if between entry/exit
            ),
            # Carry forward the signal if between thresholds
            current_signal = zoo::na.locf(current_signal, na.rm = FALSE),
            current_signal = ifelse(is.na(current_signal), 0, current_signal), # Start flat if NA initially

            # --- Position Sizing Logic (Simplified Entry Scaling) ---
            # Position depends on *yesterday's* signal and Z-score
            prev_signal = lag(current_signal, default = 0),
            prev_zscore = lag(zscore, default = 0),
            prev_prev_signal = lag(prev_signal, default=0), # Signal from two days ago

            # Calculate size scalar based on Z-score at the time of entry
            # This is a basic scaling applied only when entering a position from flat
            size_scalar = case_when(
                 # Entering short (prev_signal is -1, prev_prev_signal was 0)
                 prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                 # Entering long (prev_signal is 1, prev_prev_signal was 0)
                 prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                 # Holding position (non-zero signal same as day before), use lag of scalar (needs careful check - simplified to 1 here)
                 # A robust implementation might recalculate or carry forward the entry scalar. For simplicity, we revert to 1 when holding.
                 prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, # Simplified: Hold at base size 1
                 # Exiting or flat
                 TRUE ~ 1.0 # Default base size (scalar won't matter if prev_signal is 0)
            ),
            # Ensure scalar is at least 1 if holding a position
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)),

            # Final position size and direction (based on previous day)
            position = prev_signal * size_scalar
        ) %>%
        mutate(position = as.numeric(position)) # Ensure numeric

    # --- 5d. Calculate Strategy Returns for this pair ---
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate(
            # Ensure returns are numeric
            candidate_return = as.numeric(candidate_return),
            btc_return = as.numeric(btc_return),
            # Calculate return: position * (spread daily return)
            # Approx spread return for logs: candidate_return - btc_return
            strategy_return = position * (candidate_return - btc_return),
            # Handle potential NA/Inf returns
            strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0),
            symbol = candidate_symbol
        ) %>%
        select(date, symbol, strategy_return)

    # Store details (trade data with positions, returns data)
    backtest_details[[candidate_symbol]] <- list(
        trade_data = pair_data_trades,
        returns_data = pair_strategy_returns_tibble
    )

    cat(" Backtesting calculations complete for", candidate_symbol, "\n")

} # End loop through candidate symbols for backtesting


# --- 6. Aggregate & Individual Performance Metrics ---
cat("--- Step 6: Calculating Performance Metrics (with Position Sizing) ---\n")

if (length(backtest_details) == 0) {
    stop("Strategy returns could not be calculated for any pair during backtest.")
}

# --- 6a. Aggregate Portfolio Performance ---
# Combine returns from all successfully backtested pairs
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))

# Calculate equal-weighted portfolio return per day
portfolio_daily_returns <- all_strategy_returns_long %>%
    group_by(date) %>%
    summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>%
    arrange(date)

# Convert to xts for PerformanceAnalytics
portfolio_returns_xts <- xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
colnames(portfolio_returns_xts) <- "Aggregate Strategy (Sized)"

cat("\n--- AGGREGATE Backtest Performance Metrics (Equal Weighted, Sized Positions) ---\n")
stats_table_agg <- PerformanceAnalytics::table.AnnualizedReturns(portfolio_returns_xts, scale = 252, geometric = FALSE)
# Use Rf = 0 for crypto as risk-free rate is ambiguous and usually small relative to crypto vol
sharpe_agg <- PerformanceAnalytics::SharpeRatio.annualized(portfolio_returns_xts, Rf = 0, scale = 252, geometric = FALSE)
max_dd_agg <- PerformanceAnalytics::maxDrawdown(portfolio_returns_xts)
calmar_agg <- PerformanceAnalytics::CalmarRatio(portfolio_returns_xts, scale = 252)

print(stats_table_agg)
# SharpeRatio.annualized might return xts, access first element
cat("Annualized Sharpe (Rf=0%):", round(sharpe_agg[1,], 3), "\n")
cat("Maximum Drawdown:", round(max_dd_agg, 4), "\n")
cat("Calmar Ratio:", round(calmar_agg, 3), "\n")

# --- 6b. Individual Pair Performance ---
cat("\n--- INDIVIDUAL Pair Backtest Performance Metrics (Sized Positions) ---\n")
individual_metrics_list <- list() # To store calculated metrics per pair

for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data

    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_xts <- xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol

        # Calculate metrics for the individual pair
        stats_table_pair <- PerformanceAnalytics::table.AnnualizedReturns(pair_returns_xts, scale = 252, geometric = FALSE)
        sharpe_pair <- PerformanceAnalytics::SharpeRatio.annualized(pair_returns_xts, Rf = 0, scale = 252, geometric = FALSE)
        max_dd_pair <- PerformanceAnalytics::maxDrawdown(pair_returns_xts)
        calmar_pair <- PerformanceAnalytics::CalmarRatio(pair_returns_xts, scale = 252)

        # Print metrics
        print(stats_table_pair)
        cat(" Annualized Sharpe (Rf=0%):", round(sharpe_pair[1,], 3), "\n")
        cat(" Maximum Drawdown:", round(max_dd_pair, 4), "\n")
        cat(" Calmar Ratio:", round(calmar_pair, 3), "\n\n")

        # Store metrics (optional)
        individual_metrics_list[[candidate_symbol]] <- list(
            AnnualizedReturn = stats_table_pair["Annualized Return",],
            AnnualizedStdDev = stats_table_pair["Annualized Stdev",],
            AnnualizedSharpe = sharpe_pair[1,],
            MaxDrawdown = max_dd_pair,
            CalmarRatio = calmar_pair
        )

    } else {
        cat(" Insufficient return data to calculate metrics for", candidate_symbol, "\n\n")
    }
}

# --- 7. Plotting Spreads and Signals ---
cat("--- Step 7: Plotting Spreads and Signals (with Position Sizing) ---\n")

for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data

    # Add a column for plotting position (NA if position is zero)
    pair_plot_data <- pair_plot_data %>% mutate(position_plot = ifelse(abs(position) < SD_OFFSET, NA, position))

    # Identify trade entries (where position changes from ~0 to non-zero)
    trade_entries <- pair_plot_data %>%
        filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>%
        mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread")) # Based on sign of non-zero position

    # Plot 1: Spread
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) +
        geom_line(color="black") +
        labs(title = paste("BTC vs", candidate_symbol, ": Log Spread (Backtest Period)"),
             x = NULL, y = "Log Spread") + # Remove x-axis label for stacking
        theme_minimal()

    # Plot 2: Z-Score with Signals
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) +
        geom_line(color="black") +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") +
        geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") +
        geom_hline(yintercept = 0, color = "grey", linetype = "solid") +
        # Add points for trade entries
        geom_point(data = trade_entries, aes(color = entry_type), size = 2.5, alpha=0.8) +
        scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Max Scalar:", MAX_POSITION_SCALAR,", Factor:", SCALING_FACTOR,")"),
             x = NULL, y = "Z-Score", color = "Entry Signal") + # Remove x-axis label
        theme_minimal() +
        theme(legend.position = "bottom")

     # Plot 3: Position Size
     plot_position <- ggplot(pair_plot_data, aes(x = date, y = position_plot)) +
         geom_step(na.rm = TRUE) + # Use geom_step and remove NAs for plotting
         geom_hline(yintercept = 0, color = "grey") +
         labs(title = "Position Size (Scaled, 0=Flat)",
              x = "Date", y = "Position") + # Keep x-axis label here
         theme_minimal()

    # Arrange and print plots
    # Ensure plots are valid before arranging
    plot_list <- list(plot_spread, plot_zscore, plot_position)
    valid_plots <- sum(sapply(plot_list, function(p) !is.null(p) && inherits(p, "ggplot")))
    if (valid_plots == 3){
        tryCatch({
          gridExtra::grid.arrange(plot_spread, plot_zscore, plot_position, ncol = 1, heights = c(2, 2, 1.5))
        }, error = function(e){cat("Could not arrange plots for", candidate_symbol, ":", e$message, "\n")})
    } else {
         cat("Could not generate all plots for", candidate_symbol, "\n")
    }


    # Plot cumulative return for individual pair
    pair_returns_xts <- xts(backtest_details[[candidate_symbol]]$returns_data$strategy_return,
                            order.by = backtest_details[[candidate_symbol]]$returns_data$date)
    PerformanceAnalytics::chart.CumReturns(pair_returns_xts,
                                           main = paste("Cumulative Return: BTC vs", candidate_symbol, "(Sized Position)"),
                                           geometric = FALSE, # Use arithmetic for sum of log returns approx
                                           ylab = "Cumulative Return")

    cat(" Plots generated for", candidate_symbol, "\n")
}

# --- 8. Notes on Dynamic Portfolio Rebalancing ---
cat("\n--- Step 8: Notes on Dynamic Portfolio Rebalancing ---\n")
cat("Dynamic rebalancing involves adjusting positions periodically (e.g., daily, weekly) to maintain desired target weights or risk exposure.\n")
cat("Why it matters for Pairs Trading:\n")
cat(" 1. Maintaining Dollar Neutrality: If prices diverge significantly, a dollar-neutral pair (e.g., long $1000 BTC, short $1000 ETH) can become unbalanced.\n")
cat(" 2. Maintaining Target Ratio: If the pair is based on a specific price ratio, rebalancing might bring it back towards that ratio.\n")
cat(" 3. Risk Management: Prevents excessive exposure to one leg of the pair.\n")
cat("Implementation Considerations:\n")
cat(" - Frequency: How often to rebalance (daily, weekly, threshold-based)?\n")
cat(" - Thresholds: Rebalance only if the imbalance exceeds a certain percentage?\n")
cat(" - Transaction Costs: Frequent rebalancing incurs higher costs.\n")
cat(" - Mechanism: Calculate current value of long/short legs, determine necessary trades to restore balance.\n")
cat("Where it fits: The rebalancing logic would typically run *daily* within the backtest loop, *after* calculating the day's P&L based on the existing position, and *before* determining the *next* day's position based on the Z-score signal. It might adjust the sizes of the positions held.\n")
cat("NOTE: The current backtest does *not* implement dynamic rebalancing; it assumes scaled positions determined at entry and held until exit signal.\n")


cat("\n--- Analysis, Backtesting, and Plotting Finished ---\n")
```

# Test5:

```{r}
# --- 0. Setup: Install and Load Libraries ---
# Ensure required packages are installed. Run this once if needed:
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra", "doParallel", "foreach"))

# Load Libraries
library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics
library(gridExtra) # For arranging plots
library(doParallel) # For parallel optimization
library(foreach)    # For parallel optimization

# --- 1. Parameters ---
# Data & Timeframe
N_TOP_COINS <- 15         # Number of top coins to consider (incl. BTC)
HISTORY_YEARS <- 6        # <<< Increased total history to fetch (4 yrs train, 2 yrs test)
BACKTEST_YEARS <- 2       # Years for backtesting (out-of-sample)
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS) # Define backtest start
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)     # Define training end

# Filtering & API
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # Common stablecoins to exclude

# Analysis Thresholds
VIF_THRESHOLD <- 5        # Threshold for Variance Inflation Factor in regression
ADF_P_VALUE_THRESHOLD <- 0.05 # Threshold for ADF test significance (on training data)
HURST_THRESHOLD <- 0.5    # Threshold for Hurst exponent (on training data, H < 0.5 suggests mean reversion)

# Trading Strategy Parameters (Initial - Will be Optimized)
INITIAL_ROLLING_WINDOW <- 30
INITIAL_ZSCORE_ENTRY <- 1.5
INITIAL_ZSCORE_EXIT <- 0.5
SD_OFFSET <- 1e-9           # Small value to prevent division by zero in Z-score

# Position Sizing Parameters (Initial - Could also be optimized)
INITIAL_MAX_POSITION_SCALAR <- 2.0
INITIAL_SCALING_FACTOR <- 0.5

# Optimization Parameters
OPTIMIZE_PARAMETERS <- TRUE  # Set to TRUE to run optimization, FALSE to use INITIAL values
OPTIMIZATION_METRIC <- "Omega" # Metric to maximize during optimization ("Omega", "Sharpe", "Calmar")

# Parallel Processing Setup for Optimization
N_CORES <- detectCores() - 1 # Use N-1 cores
if (N_CORES < 1) N_CORES <- 1


# --- 2. Data Acquisition (Using httr/jsonlite - Tidyverse) ---
cat("--- Step 2: Acquiring Data ---\n")
# Get list of coins and their current market caps (CoinGecko API)
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"
    cg_params <- list(vs_currency = "usd", order = "market_cap_desc",
                      per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- httr::GET(cg_url, query = cg_params)
    Sys.sleep(2)
    if (httr::status_code(cg_response) == 200) {
        cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed to fetch data from CoinGecko. Status: ", httr::status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })

# Filter for Prime Candidates
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS - 1)
prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")

# Fetching historical data using httr/jsonlite
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list()
not_found_symbols <- c()
base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE)
if(api_limit > 2000) {
    cat("Warning: Date range may exceed API limit (~2000 days). Fetching max possible up to END_DATE.\n")
    api_limit <- 2000
}
Sys.setenv(TZ='UTC')

for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "...\n")
    tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params)
        Sys.sleep(1.5)
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8")
            json_data <- jsonlite::fromJSON(content)
            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>%
                        mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>%
                        select(date, symbol, price = close, volume = volumeto) %>%
                        filter(date >= START_DATE & date <= END_DATE) %>%
                        arrange(date)
                    if(nrow(df_processed) > 0) {
                        all_data_list_tibbles[[sym]] <- df_processed
                        cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else { cat(" No data within specified date range for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
                } else { cat(" API Success, but missing columns for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
            } else { cat(" API returned error for", sym, ":", json_data$Message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
        } else { cat(" HTTP Error:", httr::status_code(response), "for", sym, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
    }, error = function(e) { cat(" Error fetching/processing", sym, ":", e$message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym); Sys.sleep(1.5) })
}

# Check fetched data and update candidate list
if (length(all_data_list_tibbles) == 0) stop("Failed to fetch ANY data. Exiting.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed to fetch BTC data. Exiting.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Fetched BTC, but failed for all candidates. Exiting.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")

# Combine into one long tibble
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>% arrange(symbol, date)


# --- 3. Data Preparation (Tidyverse - Returns) ---
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>%
    group_by(symbol) %>% arrange(date) %>%
    mutate(
        return = log(price) - log(lag(price)),
        log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))
    ) %>% ungroup() %>% filter(!is.na(return))


# --- 4. Training Period Analysis (Identify Potential Pairs) ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)
pairs_trading_analysis_train <- list()
potentially_mean_reverting_pairs <- c()
training_pair_data_list <- list() # Store joined training data for optimization

for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training Period)\n")
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)

    if(nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) { # Need ample data for training/optimization
         cat(" Insufficient overlapping data points for BTC-", candidate_symbol, " in training period. Skipping pair.\n")
         next
    }
    # Store joined data for optimization step
    training_pair_data_list[[candidate_symbol]] <- pair_data_train

    # Pairs Trading Spread Analysis (ADF/Hurst) on Training Data
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL)
        adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display=FALSE), error = function(e) NULL)
        hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) {
            cat("  >>> Pair BTC-", candidate_symbol, "identified as potentially mean-reverting. <<<\n")
            potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol)
        }
    } else { cat("  Insufficient spread data for ADF/Hurst tests on", candidate_symbol, "(Training).\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Pair Identification Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")

if (length(potentially_mean_reverting_pairs) == 0) {
    stop("No potentially mean-reverting pairs identified. Cannot proceed.")
}


# --- 4.5 Parameter Optimization (on Training Data) ---
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")

# Define parameter grid for optimization
param_grid <- expand.grid(
    roll_window = seq(15, 45, by = 5),    # Rolling window size
    entry_z = seq(1.0, 2.5, by = 0.25), # Z-score entry threshold
    exit_z = seq(0.0, 0.75, by = 0.25), # Z-score exit threshold
    # Keep sizing fixed for this optimization loop, optimize separately if needed
    max_scalar = INITIAL_MAX_POSITION_SCALAR,
    scale_factor = INITIAL_SCALING_FACTOR,
    stringsAsFactors = FALSE
)
# Ensure exit threshold is less than entry threshold
param_grid <- param_grid %>% filter(exit_z < entry_z)
cat("Total parameter combinations to test:", nrow(param_grid), "\n")

# Function to evaluate one parameter set for one pair on training data
evaluate_params_on_train <- function(pair_symbol, pair_training_data, params) {
    cat(".") # Progress indicator
    tryCatch({
        # Extract parameters
        roll_win <- params$roll_window
        entry_z <- params$entry_z
        exit_z <- params$exit_z
        max_s <- params$max_scalar
        scale_f <- params$scale_factor

        # Calculate Spread, Z-Score, Signals, Positions, Returns on training data
        pair_data_signals <- pair_training_data %>%
            filter(btc_price > 0, candidate_price > 0) %>% # Needed again if function is standalone
            mutate(
                spread = log(candidate_price) - log(btc_price),
                rolling_mean = slider::slide_dbl(spread, mean, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE),
                rolling_sd = slider::slide_dbl(spread, sd, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE),
                rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
                zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
            ) %>%
            filter(!is.na(zscore))

        if(nrow(pair_data_signals) < 2) return(NA) # Not enough data

        pair_data_trades <- pair_data_signals %>%
            mutate(
                current_signal = case_when(zscore > entry_z ~ -1, zscore < -entry_z ~ 1, abs(zscore) < exit_z ~ 0, TRUE ~ NA_real_),
                current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast = FALSE), # Ensure forward fill only
                current_signal = ifelse(is.na(current_signal), 0, current_signal),
                prev_signal = lag(current_signal, default = 0),
                prev_zscore = lag(zscore, default = 0),
                prev_prev_signal = lag(prev_signal, default=0),
                size_scalar = case_when(
                    prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (prev_zscore - entry_z), max_s),
                    prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (abs(prev_zscore) - entry_z), max_s),
                    prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, # simplified hold
                    TRUE ~ 1.0
                ),
                size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)),
                position = prev_signal * size_scalar
            ) %>% mutate(position = as.numeric(position))

        pair_strategy_returns <- pair_data_trades %>%
            mutate(
                strategy_return = position * (candidate_return - btc_return),
                strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0)
            ) %>% select(date, strategy_return)

        if(nrow(pair_strategy_returns) < 2) return(NA)

        # Calculate chosen metric (e.g., Omega)
        returns_xts <- xts(pair_strategy_returns$strategy_return, order.by = pair_strategy_returns$date)
        metric_value <- NA
        if (OPTIMIZATION_METRIC == "Omega") {
            # Omega needs MAR (Minimum Acceptable Return), use 0
            metric_value <- PerformanceAnalytics::Omega(returns_xts, MAR = 0)
        } else if (OPTIMIZATION_METRIC == "Sharpe") {
            metric_value <- PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = 0, scale = 252, geometric = FALSE)[1,]
        } else if (OPTIMIZATION_METRIC == "Calmar") {
             metric_value <- PerformanceAnalytics::CalmarRatio(returns_xts, scale = 252)
        } else {
             stop("Unsupported OPTIMIZATION_METRIC specified.")
        }
        # Ensure single numeric value is returned
        metric_value <- as.numeric(metric_value)
        return(ifelse(is.finite(metric_value), metric_value, NA))

    }, error = function(e) {
        cat("E") # Error indicator
        # message("Error evaluating params for ", pair_symbol, ": ", e$message) # Optional detailed error
        return(NA) # Return NA on error
    })
}


if (OPTIMIZE_PARAMETERS) {
    # Setup parallel backend
    cl <- makeCluster(N_CORES)
    registerDoParallel(cl)
    cat("\nStarting parallel optimization across", N_CORES, "cores...\n")

    # Use foreach for parallel execution over parameter grid rows
    optimization_results_list <- foreach(
        i = 1:nrow(param_grid),
        .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts", "pracma"), # Need packages inside foreach
        .combine = 'rbind' # Combine results by row
    ) %dopar% {
        current_params <- param_grid[i, ]
        # Evaluate this parameter set for all potentially mean-reverting pairs
        pair_metrics <- sapply(potentially_mean_reverting_pairs, function(sym) {
            if (sym %in% names(training_pair_data_list)) {
                 evaluate_params_on_train(sym, training_pair_data_list[[sym]], current_params)
            } else {
                 NA # Should not happen if list populated correctly
            }
        })

        # Aggregate metric (e.g., mean, median) across pairs - handle NAs
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]
        aggregate_metric <- if(length(valid_metrics) > 0) mean(valid_metrics) else NA

        # Return a row combining parameters and the aggregate metric
        cbind(current_params, aggregate_metric = aggregate_metric)
    }

    # Stop the cluster
    stopCluster(cl)
    cat("\nOptimization loop finished.\n")

    # Find the best parameter set based on the aggregate metric
    optimization_results_df <- as_tibble(optimization_results_list) %>% filter(!is.na(aggregate_metric))

    if(nrow(optimization_results_df) > 0) {
        best_result_row <- optimization_results_df %>% arrange(desc(aggregate_metric)) %>% head(1)
        # Extract best parameters
        best_params <- list(
            roll_window = best_result_row$roll_window,
            entry_z = best_result_row$entry_z,
            exit_z = best_result_row$exit_z,
            max_scalar = best_result_row$max_scalar,
            scale_factor = best_result_row$scale_factor
        )
        cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n")
        print(bind_rows(best_params)) # Print neatly
        cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")

        # --- Use optimized parameters for the backtest ---
        ROLLING_WINDOW <- best_params$roll_window
        ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z
        ZSCORE_EXIT_THRESHOLD <- best_params$exit_z
        MAX_POSITION_SCALAR <- best_params$max_scalar
        SCALING_FACTOR <- best_params$scale_factor

    } else {
        cat("WARNING: Optimization failed to find valid results. Using INITIAL parameters for backtest.\n")
        ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW
        ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY
        ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT
        MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR
        SCALING_FACTOR <- INITIAL_SCALING_FACTOR
    }

} else {
    cat("Skipping optimization. Using INITIAL parameters for backtest.\n")
    ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW
    ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY
    ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT
    MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR
    SCALING_FACTOR <- INITIAL_SCALING_FACTOR
}


# --- 5. Backtesting Period (Using selected parameters) ---
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW, " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD, "\n")

# Filter data for backtest period
backtest_data_long <- all_data_calculated %>%
    filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>%
    filter(date >= BACKTEST_START_DATE - days(ROLLING_WINDOW)) # Need bit extra history for initial rolling calc

backtest_details <- list() # Store results

for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    btc_data_backtest <- backtest_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_backtest <- backtest_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) {
         cat(" Insufficient overlapping data for BTC-", candidate_symbol, " in backtest period scope. Skipping pair.\n")
         next
    }

    # Calculate Spread, Z-Score, Signals, Positions (using selected parameters)
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        # IMPORTANT: Filter for the actual backtest start date *after* rolling calculations use prior data
        filter(date >= BACKTEST_START_DATE, !is.na(zscore))

     if(nrow(pair_data_signals) < 2) {
         cat(" Insufficient data after Z-score calculation starting", format(BACKTEST_START_DATE), "for BTC-", candidate_symbol, ". Skipping pair.\n")
         next
     }

     pair_data_trades <- pair_data_signals %>%
         mutate(
            current_signal = case_when(zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, TRUE ~ NA_real_),
            current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast = FALSE),
            current_signal = ifelse(is.na(current_signal), 0, current_signal),
            prev_signal = lag(current_signal, default = 0),
            prev_zscore = lag(zscore, default = 0),
            prev_prev_signal = lag(prev_signal, default=0),
            size_scalar = case_when(
                prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, # Simplified hold
                TRUE ~ 1.0
            ),
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)),
            position = prev_signal * size_scalar
        ) %>% mutate(position = as.numeric(position))

    # Calculate Strategy Returns
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate(
            candidate_return = as.numeric(candidate_return), btc_return = as.numeric(btc_return),
            strategy_return = position * (candidate_return - btc_return),
            strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0),
            symbol = candidate_symbol
        ) %>% select(date, symbol, strategy_return)

    # Store results
    backtest_details[[candidate_symbol]] <- list(trade_data = pair_data_trades, returns_data = pair_strategy_returns_tibble)
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
} # End backtesting loop


# --- 6. Aggregate & Individual Performance Metrics (Refined with Zero SD Check) ---
cat("\n--- Step 6: Calculating Performance Metrics ---\n")

if (length(backtest_details) == 0) {
    stop("Strategy returns could not be calculated for any pair during backtest.")
}

# Helper function to calculate metrics safely, checking for sd=0
calculate_safe_metrics <- function(returns_xts, scale = 252, MAR = 0, Rf = 0) {
    metrics <- list(
        AnnReturn = NA, AnnStdDev = NA, Omega = NA, Sharpe = NA, MaxDD = NA, Calmar = NA
    )
    if (is.null(returns_xts) || nrow(returns_xts) < 2) {
        cat(" Warning: Not enough returns data to calculate metrics.\n")
        return(metrics)
    }

    # Calculate standard deviation
    sd_val <- sd(coredata(returns_xts), na.rm = TRUE)
    cat("  Std Dev Check:", sd_val, "\n") # Debugging SD

    # Calculate common metrics first (less likely to fail)
    tryCatch({ metrics$MaxDD <- PerformanceAnalytics::maxDrawdown(returns_xts) }, error = function(e) {cat(" Error MaxDD:", e$message, "\n")})
    tryCatch({
         tbl <- PerformanceAnalytics::table.AnnualizedReturns(returns_xts, scale = scale, geometric = FALSE)
         metrics$AnnReturn <- tbl[1, 1]
         metrics$AnnStdDev <- tbl[2, 1]
    }, error = function(e) {cat(" Error Ann Ret/Stdev:", e$message, "\n")})

    # Handle metrics sensitive to zero standard deviation
    if (!is.na(sd_val) && sd_val > SD_OFFSET) { # Use SD_OFFSET threshold
        tryCatch({
            metrics$Omega <- PerformanceAnalytics::Omega(returns_xts, MAR = MAR)
        }, error = function(e) {cat(" Error Omega:", e$message, "\n")})
        tryCatch({
            # Sharpe might be Inf if mean != 0 and sd -> 0, handle that later if needed
            shp <- PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = Rf, scale = scale, geometric = FALSE)
            metrics$Sharpe <- shp[1,] # Get the numeric value
        }, error = function(e) {cat(" Error Sharpe:", e$message, "\n")})
        tryCatch({
            metrics$Calmar <- PerformanceAnalytics::CalmarRatio(returns_xts, scale = scale)
        }, error = function(e) {cat(" Error Calmar:", e$message, "\n")})

    } else {
        cat(" Warning: Standard deviation is near zero. Omega, Sharpe, Calmar cannot be reliably calculated.\n")
        # Sharpe could be Inf or 0/NaN depending on mean return, set to NA
        metrics$Sharpe <- NA
        # Omega is ill-defined, set to NA (or 1 if mean > MAR, 0 if mean < MAR, etc.)
        metrics$Omega <- NA
        # Calmar depends on MaxDD and AnnReturn, might be Inf or NA
        if(!is.na(metrics$MaxDD) && metrics$MaxDD != 0 && !is.na(metrics$AnnReturn)) {
             metrics$Calmar <- metrics$AnnReturn / abs(metrics$MaxDD)
        } else {
             metrics$Calmar <- NA
        }
    }
    return(metrics)
}


# --- 6a. Aggregate Portfolio Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>%
    group_by(date) %>% summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>%
    arrange(date) %>%
    mutate(portfolio_return = ifelse(is.na(portfolio_return), 0, portfolio_return)) # Replace NA with 0

if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts <- xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts) <- "Aggregate Strategy (Optimized/Sized)"

    cat("\n--- AGGREGATE Backtest Performance Metrics (Equal Weighted, Optimized/Sized) ---\n")
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts)

    # Print Aggregate Metrics
    cat("Annualized Return:", round(agg_metrics$AnnReturn, 4), "\n")
    cat("Annualized Std Dev:", round(agg_metrics$AnnStdDev, 4), "\n")
    cat("Omega (MAR=0%):", round(agg_metrics$Omega, 3), "\n")
    cat("Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n")
    cat("Maximum Drawdown:", round(agg_metrics$MaxDD, 4), "\n")
    cat("Calmar Ratio:", round(agg_metrics$Calmar, 3), "\n")

} else {
    cat("\nError: portfolio_daily_returns has no rows. Cannot calculate aggregate metrics.\n")
    stop("Cannot calculate aggregate metrics due to empty returns data.")
}


# --- 6b. Individual Pair Performance ---
cat("\n--- INDIVIDUAL Pair Backtest Performance Metrics (Optimized/Sized) ---\n")
individual_metrics_list <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data

    if (nrow(pair_returns_tibble) > 1) {
        # Replace potential NAs with 0 before creating xts for individual pair
        pair_returns_tibble <- pair_returns_tibble %>%
            mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return))

        pair_returns_xts <- xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol

        # Use the safe calculation function
        pair_metrics <- calculate_safe_metrics(pair_returns_xts)

        # Print Pair Metrics
        cat(" Annualized Return:", round(pair_metrics$AnnReturn, 4), "\n")
        cat(" Annualized Std Dev:", round(pair_metrics$AnnStdDev, 4), "\n")
        cat(" Omega (MAR=0%):", round(pair_metrics$Omega, 3), "\n")
        cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n")
        cat(" Maximum Drawdown:", round(pair_metrics$MaxDD, 4), "\n")
        cat(" Calmar Ratio:", round(pair_metrics$Calmar, 3), "\n\n")

        individual_metrics_list[[candidate_symbol]] <- pair_metrics # Store the list of metrics
    } else {
        cat(" Insufficient return data to calculate metrics for", candidate_symbol, "\n\n")
    }
}



# --- 7. Plotting (Refined for Clarity) ---
cat("--- Step 7: Plotting Results ---\n")
# Create a directory for plots
plot_dir <- "strategy_plots"
if (!dir.exists(plot_dir)) dir.create(plot_dir)
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")

# Plot Aggregate Performance
agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png")
png(agg_plot_file, width=800, height=1000)
PerformanceAnalytics::charts.PerformanceSummary(portfolio_returns_xts, main = "Aggregate Strategy Performance (Optimized/Sized)", geometric = FALSE)
dev.off()

# Plot Individual Pairs
for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data
    pair_plot_data <- pair_plot_data %>% mutate(position_plot = ifelse(abs(position) < SD_OFFSET, NA, position))
    trade_entries <- pair_plot_data %>% filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>% mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread"))

    # Define plots
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) + geom_line(color="black") + labs(title = paste("BTC vs", candidate_symbol, ": Log Spread"), x = NULL, y = "Log Spread") + theme_minimal()
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) + geom_line(color="black") +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") +
        geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") +
        geom_hline(yintercept = 0, color = "grey", linetype = "solid") +
        geom_point(data = trade_entries, aes(color = entry_type), size = 2.5, alpha=0.8) + scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"), x = NULL, y = "Z-Score", color = "Entry Signal") + theme_minimal() + theme(legend.position = "bottom")
    plot_position <- ggplot(pair_plot_data, aes(x = date, y = position_plot)) + geom_step(na.rm = TRUE) + geom_hline(yintercept = 0, color = "grey") + labs(title = "Position Size (Scaled, 0=Flat)", x = "Date", y = "Position") + theme_minimal()

    # Arrange and Save Spread/Z/Position plots
    pair_plot_file <- file.path(plot_dir, paste0("01_spread_z_pos_", candidate_symbol, ".png"))
    tryCatch({
        ggsave(pair_plot_file, gridExtra::grid.arrange(plot_spread, plot_zscore, plot_position, ncol = 1, heights = c(2, 2, 1.5)), width=10, height=8)
        cat(" Spread/Z/Pos plot saved:", pair_plot_file, "\n")
    }, error = function(e){cat("Could not save spread/z/pos plot for", candidate_symbol, ":", e$message, "\n")})

    # Prepare & Save Performance plots (CumReturn + Drawdown)
    pair_returns_xts <- xts(backtest_details[[candidate_symbol]]$returns_data$strategy_return, order.by = backtest_details[[candidate_symbol]]$returns_data$date)
    colnames(pair_returns_xts) <- candidate_symbol
    pair_perf_plot_file <- file.path(plot_dir, paste0("02_performance_", candidate_symbol, ".png"))
    png(pair_perf_plot_file, width=800, height=600)
    PerformanceAnalytics::charts.PerformanceSummary(pair_returns_xts, main = paste("Performance: BTC vs", candidate_symbol), geometric=FALSE)
    dev.off()
    cat(" Performance plot saved:", pair_perf_plot_file, "\n")
}


# --- 8. Notes on Bitcoin Fundamental Value Strategy ---
cat("\n--- Step 8: Notes on Bitcoin Fundamental Value Strategy --- \n")
cat("The analysis and backtest performed above focus on a **relative value pairs trading strategy**.\n")
cat("This strategy relies on the spread between two assets (BTC and a candidate) being mean-reverting, using the Z-score as a signal.\n")
cat("Synthesizing Bitcoin's 'fundamental value' for **directional trading** of Bitcoin itself is a different approach.\n")
cat("Potential fundamental/on-chain metrics for such a model might include:\n")
cat("  - Network Activity: Hash Rate, Active Addresses, Transaction Count/Volume.\n")
cat("  - Valuation Ratios: NVT (Network Value to Transactions), MVRV (Market Value to Realized Value).\n")
cat("  - Supply Dynamics: Stock-to-Flow (S2F) model variations, HODL waves, Exchange Balances.\n")
cat("  - Market Sentiment: Social media analysis, Google trends (as used in some factor models).\n")
cat("  - Macroeconomic Factors: Correlation with traditional assets (e.g., Nasdaq), inflation rates, monetary policy.\n")
cat("Implementing such a strategy would require:\n")
cat("  1. Access to specialized on-chain/fundamental data APIs (e.g., Glassnode, CryptoQuant, potentially paid).\n")
cat("  2. Different modeling techniques (e.g., time series regression, machine learning models like Random Forest or LSTM) to relate these factors to future BTC price movements.\n")
cat("  3. Different signal generation logic (e.g., predict price direction, generate buy/sell signals based on model output).\n")
cat("  4. Different backtesting framework focused on single-asset directional trades (long/short/flat BTC).\n")
cat("This current script does *not* incorporate these fundamental metrics or attempt to model Bitcoin's absolute value.\n")


# --- 9. Notes on Dynamic Portfolio Rebalancing ---
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n")
cat("Dynamic rebalancing involves adjusting positions periodically (e.g., daily, weekly) to maintain desired target weights or risk exposure.\n")
cat("Why it matters for Pairs Trading:\n")
cat(" 1. Maintaining Dollar Neutrality: If prices diverge significantly, a dollar-neutral pair (e.g., long $1000 BTC, short $1000 ETH) can become unbalanced.\n")
cat(" 2. Maintaining Target Ratio: If the pair is based on a specific price ratio, rebalancing might bring it back towards that ratio.\n")
cat(" 3. Risk Management: Prevents excessive exposure to one leg of the pair.\n")
cat("Implementation Considerations:\n")
cat(" - Frequency: How often to rebalance (daily, weekly, threshold-based)?\n")
cat(" - Thresholds: Rebalance only if the imbalance exceeds a certain percentage?\n")
cat(" - Transaction Costs: Frequent rebalancing incurs higher costs.\n")
cat(" - Mechanism: Calculate current value of long/short legs, determine necessary trades to restore balance.\n")
cat("Where it fits: The rebalancing logic would typically run *daily* within the backtest loop, *after* calculating the day's P&L based on the existing position, and *before* determining the *next* day's position based on the Z-score signal. It might adjust the sizes of the positions held.\n")
cat("NOTE: The current backtest does *not* implement dynamic rebalancing; it uses scaled positions determined at entry and holds that *relative scaling* until exit signal.\n")


cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")
```

# **Test6:**

```{r}
# --- 0. Setup: Install and Load Libraries ---
# Ensure required packages are installed. Run this once if needed:
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra", "doParallel", "foreach", "scales"))

# Load Libraries
library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics
library(gridExtra) # For arranging plots
library(doParallel) # For parallel optimization
library(foreach)    # For parallel optimization
library(scales)     # For formatting percentages

# --- 1. Parameters ---
# Data & Timeframe
N_TOP_COINS <- 15
HISTORY_YEARS <- 6
BACKTEST_YEARS <- 2
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS)
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)

# Filtering & API
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT")

# Analysis Thresholds
VIF_THRESHOLD <- 5
ADF_P_VALUE_THRESHOLD <- 0.05
HURST_THRESHOLD <- 0.5

# Trading Strategy Parameters (Initial - Will be Optimized)
INITIAL_ROLLING_WINDOW <- 30
INITIAL_ZSCORE_ENTRY <- 1.5
INITIAL_ZSCORE_EXIT <- 0.5
SD_OFFSET <- 1e-9

# Position Sizing Parameters (Initial - Could also be optimized)
INITIAL_MAX_POSITION_SCALAR <- 2.0
INITIAL_SCALING_FACTOR <- 0.5

# Optimization Parameters
OPTIMIZE_PARAMETERS <- TRUE
OPTIMIZATION_METRIC <- "Sharpe" # <<< Changed default to Sharpe ("Sharpe", "Calmar")
STARTING_BALANCE <- 500000 # For simulated equity curve display

# Parallel Processing Setup for Optimization
N_CORES <- detectCores() - 1
if (N_CORES < 1) N_CORES <- 1


# --- 2. Data Acquisition ---
# (Code is unchanged from previous version - Gets CoinGecko list, fetches history via API)
cat("--- Step 2: Acquiring Data ---\n")
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"; cg_params <- list(vs_currency = "usd", order = "market_cap_desc", per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- httr::GET(cg_url, query = cg_params); Sys.sleep(2)
    if (httr::status_code(cg_response) == 200) {
        cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed CoinGecko fetch. Status: ", httr::status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS - 1)
prime_candidate_symbols <- prime_candidates_info$symbol; btc_symbol <- "BTC"; all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list(); not_found_symbols <- c(); base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE); if(api_limit > 2000) { cat("Warning: Date range may exceed API limit (~2000 days).\n"); api_limit <- 2000 }
Sys.setenv(TZ='UTC')
for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "..."); tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params); Sys.sleep(1.5)
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8"); json_data <- jsonlite::fromJSON(content)
            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>% mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>%
                        select(date, symbol, price = close, volume = volumeto) %>% filter(date >= START_DATE & date <= END_DATE) %>% arrange(date)
                    if(nrow(df_processed) > 0) { all_data_list_tibbles[[sym]] <- df_processed; cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else { cat(" No data in range. Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
                } else { cat(" Missing columns. Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
            } else { cat(" API Error:", json_data$Message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
        } else { cat(" HTTP Error:", httr::status_code(response), ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
    }, error = function(e) { cat(" Error:", e$message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym); Sys.sleep(1.5) })
}
if (length(all_data_list_tibbles) == 0) stop("Failed fetch ANY data.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed fetch BTC data.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Failed fetch all candidates.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>% arrange(symbol, date)


# --- 3. Data Preparation ---
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>% group_by(symbol) %>% arrange(date) %>%
    mutate( return = log(price) - log(lag(price)), log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))) %>%
    ungroup() %>% filter(!is.na(return))


# --- 4. Training Period Analysis (Identify Potential Pairs) ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)
pairs_trading_analysis_train <- list(); potentially_mean_reverting_pairs <- c(); training_pair_data_list <- list()
for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training Period)\n")
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)
    if(nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) { cat(" Insufficient overlap. Skipping pair.\n"); next }
    training_pair_data_list[[candidate_symbol]] <- pair_data_train # Store for optimization
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL); adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display=FALSE), error = function(e) NULL); hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) { cat("  >>> Pair BTC-", candidate_symbol, "identified potentially mean-reverting. <<<\n"); potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol) }
    } else { cat("  Insufficient spread data for tests.\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Pair Identification Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")
if (length(potentially_mean_reverting_pairs) == 0) stop("No potentially mean-reverting pairs identified.")


# --- 4.5 Parameter Optimization (on Training Data - Revised) ---
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")
param_grid <- expand.grid( roll_window = seq(15, 45, by = 10), entry_z = seq(1.25, 2.25, by = 0.25), exit_z = seq(0.25, 0.75, by = 0.25),
                           max_scalar = INITIAL_MAX_POSITION_SCALAR, scale_factor = INITIAL_SCALING_FACTOR, stringsAsFactors = FALSE) %>% filter(exit_z < entry_z)
cat("Total parameter combinations to test:", nrow(param_grid), "\n")

# Revised evaluation function - simplified metric calculation, added checks
evaluate_params_on_train <- function(pair_symbol, pair_training_data, params) {
    # Removed cat(".") for cleaner output unless debugging
    tryCatch({
        roll_win <- params$roll_window; entry_z <- params$entry_z; exit_z <- params$exit_z; max_s <- params$max_scalar; scale_f <- params$scale_factor
        pair_data_signals <- pair_training_data %>% filter(btc_price > 0, candidate_price > 0) %>%
            mutate( spread = log(candidate_price) - log(btc_price), rolling_mean = slider::slide_dbl(spread, mean, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE),
                    rolling_sd = slider::slide_dbl(spread, sd, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE), rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
                    zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)) %>% filter(!is.na(zscore))
        if(nrow(pair_data_signals) < 2) return(NA) # Check 1: Need rows after Z calc
        pair_data_trades <- pair_data_signals %>% mutate(
                current_signal = case_when(zscore > entry_z ~ -1, zscore < -entry_z ~ 1, abs(zscore) < exit_z ~ 0, TRUE ~ NA_real_),
                current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast=FALSE), current_signal = ifelse(is.na(current_signal), 0, current_signal),
                prev_signal = lag(current_signal, default = 0), prev_zscore = lag(zscore, default = 0), prev_prev_signal = lag(prev_signal, default=0),
                size_scalar = case_when( prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (prev_zscore - entry_z), max_s),
                                         prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (abs(prev_zscore) - entry_z), max_s),
                                         prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, TRUE ~ 1.0),
                size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), position = prev_signal * size_scalar) %>% mutate(position = as.numeric(position))
        pair_strategy_returns <- pair_data_trades %>% mutate( strategy_return = position * (candidate_return - btc_return), strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0)) %>% select(date, strategy_return)

        if(nrow(pair_strategy_returns) < 20) return(NA) # Check 2: Need enough returns for metric

        returns_xts <- xts::xts(pair_strategy_returns$strategy_return, order.by = pair_strategy_returns$date)
        sd_val_train <- sd(coredata(returns_xts), na.rm=TRUE)
        if (is.na(sd_val_train) || sd_val_train < SD_OFFSET) return(NA) # Check 3: Need non-zero SD

        metric_value <- NA
        if (OPTIMIZATION_METRIC == "Sharpe") {
            metric_value <- tryCatch(PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = 0, scale = 252, geometric = FALSE)[1,], error=function(e) NA)
        } else if (OPTIMIZATION_METRIC == "Calmar") {
            metric_value <- tryCatch(PerformanceAnalytics::CalmarRatio(returns_xts, scale = 252), error=function(e) NA)
        } # Removed Omega
        metric_value <- as.numeric(metric_value) # Ensure single numeric value
        return(ifelse(is.finite(metric_value), metric_value, NA)) # Return NA if metric calculation failed

    }, error = function(e) {
        # Optionally log the specific error e$message if needed during deep debug
        return(NA) # Return NA on any unexpected error
    })
}

if (OPTIMIZE_PARAMETERS) {
    cl <- makeCluster(N_CORES); registerDoParallel(cl); cat("\nStarting parallel optimization across", N_CORES, "cores...\n")
    optimization_results_list <- foreach(i = 1:nrow(param_grid), .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts"), .combine = 'rbind', .errorhandling = 'pass') %dopar% { # Added errorhandling
        current_params <- param_grid[i, ]
        pair_metrics <- sapply(potentially_mean_reverting_pairs, function(sym) {
            if (sym %in% names(training_pair_data_list)) { evaluate_params_on_train(sym, training_pair_data_list[[sym]], current_params) } else { NA }
        })
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]
        aggregate_metric <- if(length(valid_metrics) > 0) mean(valid_metrics) else NA
        # Return a named list or vector for easier conversion later
        list(params=current_params, aggregate_metric=aggregate_metric, num_valid_pairs=length(valid_metrics))
    }
    stopCluster(cl); cat("\nOptimization loop finished.\n")

    # Process results (handle potential errors/NULLs from foreach)
    valid_results <- Filter(function(x) !inherits(x, "error") && !is.null(x) && !is.na(x$aggregate_metric), optimization_results_list)

    if(length(valid_results) > 0) {
        # Combine results into a data frame more robustly
        results_df_list <- lapply(valid_results, function(res) {
            cbind(as.data.frame(res$params), aggregate_metric = res$aggregate_metric, num_valid_pairs = res$num_valid_pairs)
        })
        optimization_results_df <- bind_rows(results_df_list)

        best_result_row <- optimization_results_df %>% arrange(desc(aggregate_metric)) %>% head(1)
        # Check if best_result_row is valid
        if(nrow(best_result_row) > 0 && is.finite(best_result_row$aggregate_metric)) {
            best_params <- list( roll_window = best_result_row$roll_window, entry_z = best_result_row$entry_z, exit_z = best_result_row$exit_z, max_scalar = best_result_row$max_scalar, scale_factor = best_result_row$scale_factor)
            cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n"); print(bind_rows(best_params)); cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")
            ROLLING_WINDOW <- best_params$roll_window; ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z; ZSCORE_EXIT_THRESHOLD <- best_params$exit_z; MAX_POSITION_SCALAR <- best_params$max_scalar; SCALING_FACTOR <- best_params$scale_factor
        } else {
            cat("WARNING: Optimization resulted in non-finite best metric. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR
        }
    } else {
        cat("WARNING: Optimization failed to yield any valid results. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR
    }
} else {
    cat("Skipping optimization. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR
}


# --- 5. Backtesting Period ---
# (Code unchanged - runs backtest with selected parameters)
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW, " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD, "\n")
backtest_data_long <- all_data_calculated %>% filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>% filter(date >= BACKTEST_START_DATE - days(ROLLING_WINDOW))
backtest_details <- list()
for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    btc_data_backtest <- backtest_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_backtest <- backtest_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)
    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) { cat(" Insufficient overlap. Skipping.\n"); next }
    pair_data_signals <- pair_data_backtest %>% mutate(
            spread = log(candidate_price) - log(btc_price), rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE), rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd) ) %>% filter(date >= BACKTEST_START_DATE, !is.na(zscore))
     if(nrow(pair_data_signals) < 2) { cat(" Insufficient data post Z-score. Skipping.\n"); next }
     pair_data_trades <- pair_data_signals %>% mutate(
            current_signal = case_when(zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, TRUE ~ NA_real_),
            current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast=FALSE), current_signal = ifelse(is.na(current_signal), 0, current_signal),
            prev_signal = lag(current_signal, default = 0), prev_zscore = lag(zscore, default = 0), prev_prev_signal = lag(prev_signal, default=0),
            size_scalar = case_when( prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                     prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                     prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, TRUE ~ 1.0),
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), position = prev_signal * size_scalar) %>% mutate(position = as.numeric(position))
    pair_strategy_returns_tibble <- pair_data_trades %>% mutate( candidate_return = as.numeric(candidate_return), btc_return = as.numeric(btc_return),
            strategy_return = position * (candidate_return - btc_return), strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0), symbol = candidate_symbol) %>% select(date, symbol, strategy_return)
    backtest_details[[candidate_symbol]] <- list(trade_data = pair_data_trades, returns_data = pair_strategy_returns_tibble)
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
}


# --- 6. Performance Metrics & Drawdown Calculation (Revised - No Omega) ---
cat("\n--- Step 6: Calculating Performance Metrics ---\n")
if (length(backtest_details) == 0) stop("No pairs successfully backtested.")

# Helper function (Revised: No Omega, Added CumReturn, Win%, PctInMarket)
calculate_safe_metrics <- function(returns_xts, trade_data, scale = 252, Rf = 0) {
    metrics <- list( AnnReturn = NA, AnnStdDev = NA, Sharpe = NA, MaxDD = NA, Calmar = NA, WinPerc = NA, PctInMarket= NA, CumReturn = NA )
    if (is.null(returns_xts) || nrow(returns_xts) < 20) { # Need reasonable length for metrics
        cat(" Warning: Not enough returns data (<20) to calculate metrics reliably.\n")
        # Try to calculate CumReturn even for short series if possible
        tryCatch({ metrics$CumReturn <- PerformanceAnalytics::Return.cumulative(returns_xts, geometric=FALSE) }, error = function(e) {})
        return(metrics)
    }
    sd_val <- sd(coredata(returns_xts), na.rm = TRUE); cat("  Std Dev Check:", sd_val, "\n")
    tryCatch({ metrics$MaxDD <- PerformanceAnalytics::maxDrawdown(returns_xts) }, error = function(e) {cat(" Error MaxDD:", e$message, "\n")})
    tryCatch({ tbl <- PerformanceAnalytics::table.AnnualizedReturns(returns_xts, scale = scale, geometric = FALSE); metrics$AnnReturn <- tbl[1, 1]; metrics$AnnStdDev <- tbl[2, 1] }, error = function(e) {cat(" Error Ann Ret/Stdev:", e$message, "\n")})
    tryCatch({ metrics$CumReturn <- PerformanceAnalytics::Return.cumulative(returns_xts, geometric=FALSE) }, error = function(e) {cat(" Error CumReturn:", e$message, "\n")})

    # Calculate Win % and % In Market from trade_data
    if (!is.null(trade_data) && "position" %in% names(trade_data) && "strategy_return" %in% names(trade_data)){
         positions <- trade_data$position[!is.na(trade_data$position)]
         returns_in_market <- coredata(returns_xts)[positions != 0]
         if(length(returns_in_market) > 0) metrics$WinPerc <- mean(returns_in_market > 0, na.rm=TRUE) else metrics$WinPerc <- NA
         metrics$PctInMarket <- mean(positions != 0, na.rm=TRUE)
    }

    # Calculate Sharpe and Calmar only if SD is sufficient
    if (!is.na(sd_val) && sd_val > SD_OFFSET) {
        tryCatch({ shp <- PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = Rf, scale = scale, geometric = FALSE); metrics$Sharpe <- shp[1,] }, error = function(e) {cat(" Error Sharpe:", e$message, "\n")})
        tryCatch({ metrics$Calmar <- PerformanceAnalytics::CalmarRatio(returns_xts, scale = scale) }, error = function(e) {cat(" Error Calmar:", e$message, "\n")})
    } else {
        cat(" Warning: Zero SD. Sharpe, Calmar set to NA.\n"); metrics$Sharpe <- NA
        if(!is.na(metrics$MaxDD) && metrics$MaxDD != 0 && !is.na(metrics$AnnReturn)) { metrics$Calmar <- metrics$AnnReturn / abs(metrics$MaxDD) } else { metrics$Calmar <- NA }
    }
    # Ensure all are single numeric values
    metrics <- lapply(metrics, function(x) if(length(x) > 1 && is.numeric(x)) x[[1]] else x) # Avoid error on non-numeric/single value
    return(metrics)
}


# --- 6a. Aggregate Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>% group_by(date) %>% summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>% arrange(date) %>% mutate(portfolio_return = ifelse(is.na(portfolio_return), 0, portfolio_return))
if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts_agg <- xts::xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts_agg) <- "Aggregate Strategy"
    cat("\n--- AGGREGATE Backtest Performance Metrics ---\n")
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts_agg, trade_data = NULL) # Pass NULL for trade_data
    cat("Annualized Return:", scales::percent(agg_metrics$AnnReturn, accuracy=0.01), "\n")
    cat("Annualized Std Dev:", scales::percent(agg_metrics$AnnStdDev, accuracy=0.01), "\n")
    cat("Cumulative Return:", scales::percent(agg_metrics$CumReturn, accuracy=0.01), "\n")
    # Removed Omega
    cat("Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n")
    cat("Maximum Drawdown:", scales::percent(agg_metrics$MaxDD, accuracy=0.01), "\n")
    cat("Calmar Ratio:", round(agg_metrics$Calmar, 3), "\n")
} else { cat("\nError: Aggregate returns empty.\n"); stop("Cannot calculate aggregate metrics.") }


# --- 6b. Individual Pair Performance & Drawdowns ---
cat("\n--- INDIVIDUAL Pair Backtest Performance & Drawdowns ---\n")
individual_metrics_list <- list(); individual_drawdown_tables <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data
    pair_trade_data <- backtest_details[[candidate_symbol]]$trade_data
    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_tibble <- pair_returns_tibble %>% mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return))
        pair_returns_xts <- xts::xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol
        pair_metrics <- calculate_safe_metrics(pair_returns_xts, pair_trade_data)
        individual_metrics_list[[candidate_symbol]] <- pair_metrics
        cat(" Annualized Return:", scales::percent(pair_metrics$AnnReturn, accuracy=0.01), "\n")
        cat(" Annualized Std Dev:", scales::percent(pair_metrics$AnnStdDev, accuracy=0.01), "\n")
        cat(" Cumulative Return:", scales::percent(pair_metrics$CumReturn, accuracy=0.01), "\n")
        cat(" Win % (of active days):", scales::percent(pair_metrics$WinPerc, accuracy=0.1), "\n") # Clarified Win % calculation
        cat(" % Time In Market:", scales::percent(pair_metrics$PctInMarket, accuracy=0.1), "\n")
        # Removed Omega
        cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n")
        cat(" Maximum Drawdown:", scales::percent(pair_metrics$MaxDD, accuracy=0.01), "\n")
        cat(" Calmar Ratio:", round(pair_metrics$Calmar, 3), "\n")
        dd_table <- tryCatch(PerformanceAnalytics::table.Drawdowns(pair_returns_xts, top=5), error=function(e) NULL)
        if(!is.null(dd_table)) { cat(" Top 5 Drawdowns:\n"); print(dd_table); individual_drawdown_tables[[candidate_symbol]] <- dd_table } else { cat(" Could not calculate drawdown table.\n") }
        cat("\n")
    } else { cat(" Insufficient return data for", candidate_symbol, "\n\n") }
}

# --- 6c. Create Summary Table ---
summary_df <- tryCatch({
    bind_rows(lapply(individual_metrics_list, as.data.frame), .id = "Candidate") %>%
    mutate(Pair = paste("BTC", Candidate, sep="-")) %>%
    select(Pair, AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn) %>% # Removed Omega
    arrange(desc(Sharpe)) # Example sort
}, error = function(e) {
    cat("Error creating summary table:", e$message, "\n"); NULL
})

if (!is.null(summary_df)){
    cat("\n--- Performance Summary Table ---\n")
    print(summary_df, digits=3)
}

# --- 7. Plotting (Enhanced Strategy Plot) ---
# (Code unchanged from previous version - plots based on data in backtest_details)
cat("\n--- Step 7: Plotting Results ---\n")
plot_dir <- "strategy_plots"; if (!dir.exists(plot_dir)) dir.create(plot_dir)
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")
agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png"); png(agg_plot_file, width=800, height=1000); tryCatch({ PerformanceAnalytics::charts.PerformanceSummary(portfolio_returns_xts_agg, main = "Aggregate Strategy Performance", geometric = FALSE)}, error=function(e) plot.new()); dev.off()
btc_prices_backtest <- all_data_calculated %>% filter(symbol == btc_symbol, date >= BACKTEST_START_DATE) %>% select(date, btc_price = price)
for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data
    pair_returns_data <- backtest_details[[candidate_symbol]]$returns_data
    # Plot 1: Spread + Z-Score + Entries
    trade_entries <- pair_plot_data %>% filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>% mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread"))
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) + geom_line(color="black") + labs(title = paste("BTC vs", candidate_symbol, ": Log Spread"), x = NULL, y = "Log Spread") + theme_minimal() + theme(plot.title = element_text(size=10))
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) + geom_line(color="black") +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") + geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") + geom_hline(yintercept = 0, color = "grey") +
        geom_point(data = trade_entries, aes(color = entry_type), size = 2) + scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"), x = "Date", y = "Z-Score", color = NULL) + theme_minimal() + theme(legend.position = "bottom", plot.title = element_text(size=10))
    pair_plot_file_spread <- file.path(plot_dir, paste0("01_spread_z_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_spread, gridExtra::grid.arrange(plot_spread, plot_zscore, ncol = 1, heights = c(1, 2)), width=10, height=6); cat(" Spread/Z plot saved:", pair_plot_file_spread, "\n") }, error = function(e){cat(" Could not save spread/z plot:", e$message, "\n")})
    # Plot 2: Strategy Results Plot
    plot_data_strategy <- pair_plot_data %>% select(date, position) %>% inner_join(pair_returns_data, by="date") %>% inner_join(btc_prices_backtest, by="date") %>%
        mutate(cum_ret = cumprod(1 + strategy_return), CumEQ = STARTING_BALANCE * cum_ret,
               trade_signal = case_when( position != 0 & lag(position, default=0) == 0 ~ 1, position == 0 & lag(position, default=0) != 0 ~ -1, TRUE ~ 0 ),
               pos_start = date, pos_end = lead(date, default = END_DATE + days(1)), position_plot = ifelse(abs(position) < SD_OFFSET, NA, position))
    p_price <- ggplot(plot_data_strategy, aes(x=date, y=btc_price)) + geom_line() + labs(title=paste("Strategy Results: BTC vs", candidate_symbol), y="BTC Price", x=NULL) + theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
    p_trades <- ggplot(plot_data_strategy %>% filter(trade_signal != 0), aes(x=date, y=trade_signal)) + geom_col(aes(fill=factor(trade_signal)), width=1.5) + scale_fill_manual(values=c("1"="darkgreen", "-1"="darkred"), guide="none") + labs(y="Trade", x=NULL) + ylim(-2, 2) + theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
    p_pos <- ggplot(plot_data_strategy %>% filter(!is.na(position_plot))) + geom_rect(aes(xmin = pos_start, xmax = pos_end, ymin = -abs(position_plot), ymax = abs(position_plot), fill = factor(sign(position_plot))), alpha=0.8) + geom_hline(yintercept=0) + scale_fill_manual(values=c("1"="blue", "-1"="blue"), guide="none") + labs(y="Position", x=NULL) + theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + ylim(-MAX_POSITION_SCALAR*1.1, MAX_POSITION_SCALAR*1.1)
    p_cumeq <- ggplot(plot_data_strategy, aes(x=date, y=CumEQ)) + geom_line(color="blue") + labs(y="Equity", x="Date") + theme_minimal() + scale_y_continuous(labels = scales::comma)
    pair_plot_file_strategy <- file.path(plot_dir, paste0("02_strategy_results_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_strategy, gridExtra::grid.arrange(p_price, p_trades, p_pos, p_cumeq, ncol=1, heights=c(3,1,1,2)), width=10, height=8); cat(" Strategy results plot saved:", pair_plot_file_strategy, "\n") }, error=function(e){cat(" Could not save strategy plot:", e$message, "\n")})
}

# --- 8. Notes on Dynamic Portfolio Rebalancing ---
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n")
cat("Dynamic rebalancing involves adjusting positions periodically (e.g., daily, weekly) to maintain desired target weights or risk exposure.\n")
cat("Why it matters for Pairs Trading:\n")
cat(" 1. Maintaining Dollar Neutrality: If prices diverge significantly, a dollar-neutral pair (e.g., long $1000 BTC, short $1000 ETH) can become unbalanced.\n")
cat(" 2. Maintaining Target Ratio: If the pair is based on a specific price ratio, rebalancing might bring it back towards that ratio.\n")
cat(" 3. Risk Management: Prevents excessive exposure to one leg of the pair.\n")
cat("Implementation Considerations:\n")
cat(" - Frequency: How often to rebalance (daily, weekly, threshold-based)?\n")
cat(" - Thresholds: Rebalance only if the imbalance exceeds a certain percentage?\n")
cat(" - Transaction Costs: Frequent rebalancing incurs higher costs.\n")
cat(" - Mechanism: Calculate current value of long/short legs, determine necessary trades to restore balance.\n")
cat("Where it fits: The rebalancing logic would typically run *daily* within the backtest loop, *after* calculating the day's P&L based on the existing position, and *before* determining the *next* day's position based on the Z-score signal. It might adjust the sizes of the positions held.\n")
cat("NOTE: The current backtest does *not* implement dynamic rebalancing; it uses scaled positions determined at entry and holds that *relative scaling* until exit signal.\n")


cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")
```

# **Test7:**

```{r}
# --- 0. Setup: Install and Load Libraries ---
# Ensure required packages are installed. Run this once if needed:
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra", "doParallel", "foreach", "scales"))

# Load Libraries
library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics
library(gridExtra) # For arranging plots
library(doParallel) # For parallel optimization
library(foreach)    # For parallel optimization
library(scales)     # For formatting percentages

# --- 1. Parameters ---
# Data & Timeframe
N_TOP_COINS <- 15         # Number of top coins to consider (incl. BTC)
HISTORY_YEARS <- 6        # Increased total history to fetch (e.g., 4 yrs train, 2 yrs test)
BACKTEST_YEARS <- 2       # Years for backtesting (out-of-sample)
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS) # Define backtest start
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)     # Define training end

# Filtering & API
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # Common stablecoins to exclude

# Analysis Thresholds
VIF_THRESHOLD <- 5        # Threshold for Variance Inflation Factor in regression
ADF_P_VALUE_THRESHOLD <- 0.05 # Threshold for ADF test significance (on training data)
HURST_THRESHOLD <- 0.5    # Threshold for Hurst exponent (on training data, H < 0.5 suggests mean reversion)

# Trading Strategy Parameters (Initial - Will be Optimized)
INITIAL_ROLLING_WINDOW <- 30
INITIAL_ZSCORE_ENTRY <- 1.5
INITIAL_ZSCORE_EXIT <- 0.5
SD_OFFSET <- 1e-9           # Small value to prevent division by zero in Z-score

# Position Sizing Parameters (Initial - Could also be optimized)
INITIAL_MAX_POSITION_SCALAR <- 2.0
INITIAL_SCALING_FACTOR <- 0.5

# Optimization Parameters
OPTIMIZE_PARAMETERS <- TRUE  # Set to TRUE to run optimization, FALSE to use INITIAL values
OPTIMIZATION_METRIC <- "Sharpe" # Metric to maximize ("Sharpe", "Calmar") - Removed Omega
STARTING_BALANCE <- 500000 # For simulated equity curve display

# Parallel Processing Setup for Optimization
N_CORES <- detectCores() - 1 # Use N-1 cores
if (N_CORES < 1) N_CORES <- 1


# --- 2. Data Acquisition ---
# (Code is unchanged - Gets CoinGecko list, fetches history via API)
cat("--- Step 2: Acquiring Data ---\n")
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"; cg_params <- list(vs_currency = "usd", order = "market_cap_desc", per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- httr::GET(cg_url, query = cg_params); Sys.sleep(2)
    if (httr::status_code(cg_response) == 200) {
        cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed CoinGecko fetch. Status: ", httr::status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS - 1)
prime_candidate_symbols <- prime_candidates_info$symbol; btc_symbol <- "BTC"; all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list(); not_found_symbols <- c(); base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE); if(api_limit > 2000) { cat("Warning: Date range may exceed API limit (~2000 days).\n"); api_limit <- 2000 }
Sys.setenv(TZ='UTC')
for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "..."); tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params); Sys.sleep(1.5)
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8"); json_data <- jsonlite::fromJSON(content)
            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>% mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>%
                        select(date, symbol, price = close, volume = volumeto) %>% filter(date >= START_DATE & date <= END_DATE) %>% arrange(date)
                    if(nrow(df_processed) > 0) { all_data_list_tibbles[[sym]] <- df_processed; cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else { cat(" No data in range. Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
                } else { cat(" Missing columns. Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
            } else { cat(" API Error:", json_data$Message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
        } else { cat(" HTTP Error:", httr::status_code(response), ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
    }, error = function(e) { cat(" Error:", e$message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym); Sys.sleep(1.5) })
}
if (length(all_data_list_tibbles) == 0) stop("Failed fetch ANY data.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed fetch BTC data.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Failed fetch all candidates.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>% arrange(symbol, date)


# --- 3. Data Preparation ---
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>% group_by(symbol) %>% arrange(date) %>%
    mutate( return = log(price) - log(lag(price)), log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))) %>%
    ungroup() %>% filter(!is.na(return))


# --- 4. Training Period Analysis (Identify Potential Pairs) ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)
pairs_trading_analysis_train <- list(); potentially_mean_reverting_pairs <- c(); training_pair_data_list <- list()
for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training Period)\n")
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)
    if(nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) { cat(" Insufficient overlap. Skipping pair.\n"); next }
    training_pair_data_list[[candidate_symbol]] <- pair_data_train # Store for optimization
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL); adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display=FALSE), error = function(e) NULL); hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) { cat("  >>> Pair BTC-", candidate_symbol, "identified potentially mean-reverting. <<<\n"); potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol) }
    } else { cat("  Insufficient spread data for tests.\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Pair Identification Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")
if (length(potentially_mean_reverting_pairs) == 0) stop("No potentially mean-reverting pairs identified.")


# --- 4.5 Parameter Optimization (on Training Data - Revised for Robustness) ---
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")
param_grid <- expand.grid( roll_window = seq(15, 45, by = 10),    # Reduced range for quicker test
                           entry_z = seq(1.25, 2.25, by = 0.25),
                           exit_z = seq(0.25, 0.75, by = 0.25),
                           max_scalar = INITIAL_MAX_POSITION_SCALAR,
                           scale_factor = INITIAL_SCALING_FACTOR,
                           stringsAsFactors = FALSE) %>%
              filter(exit_z < entry_z) # Ensure exit is closer to 0 than entry
cat("Total parameter combinations to test:", nrow(param_grid), "\n")

# Revised evaluation function - focused on Sharpe/Calmar, more robust checks
evaluate_params_on_train <- function(pair_symbol, pair_training_data, params) {
    tryCatch({
        # Extract params
        roll_win <- params$roll_window; entry_z <- params$entry_z; exit_z <- params$exit_z; max_s <- params$max_scalar; scale_f <- params$scale_factor

        # Calculate Spread, Z-Score, etc.
        pair_data_signals <- pair_training_data %>% filter(btc_price > 0, candidate_price > 0) %>%
            mutate( spread = log(candidate_price) - log(btc_price), rolling_mean = slider::slide_dbl(spread, mean, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE),
                    rolling_sd = slider::slide_dbl(spread, sd, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE), rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
                    zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)) %>% filter(!is.na(zscore))
        if(nrow(pair_data_signals) < 2) return(NA) # Need rows after Z calc

        pair_data_trades <- pair_data_signals %>% mutate(
                current_signal = case_when(zscore > entry_z ~ -1, zscore < -entry_z ~ 1, abs(zscore) < exit_z ~ 0, TRUE ~ NA_real_),
                current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast=FALSE), current_signal = ifelse(is.na(current_signal), 0, current_signal),
                prev_signal = lag(current_signal, default = 0), prev_zscore = lag(zscore, default = 0), prev_prev_signal = lag(prev_signal, default=0),
                size_scalar = case_when( prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (prev_zscore - entry_z), max_s),
                                         prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (abs(prev_zscore) - entry_z), max_s),
                                         prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, TRUE ~ 1.0),
                size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), position = prev_signal * size_scalar) %>% mutate(position = as.numeric(position))

        pair_strategy_returns <- pair_data_trades %>% mutate( strategy_return = position * (candidate_return - btc_return), strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0)) %>% select(date, strategy_return)

        if(nrow(pair_strategy_returns) < 20) return(NA) # Need enough returns for metric calculation

        returns_xts <- xts::xts(pair_strategy_returns$strategy_return, order.by = pair_strategy_returns$date)
        sd_val_train <- sd(coredata(returns_xts), na.rm=TRUE)
        if (is.na(sd_val_train) || sd_val_train < SD_OFFSET) return(NA) # Need non-zero SD

        metric_value <- NA
        if (OPTIMIZATION_METRIC == "Sharpe") {
            metric_value <- tryCatch(PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = 0, scale = 252, geometric = FALSE)[1,], error=function(e) NA)
        } else if (OPTIMIZATION_METRIC == "Calmar") {
             metric_value <- tryCatch(PerformanceAnalytics::CalmarRatio(returns_xts, scale = 252), error=function(e) NA)
        } else { stop("Invalid OPTIMIZATION_METRIC") }

        metric_value <- as.numeric(metric_value)
        return(ifelse(is.finite(metric_value), metric_value, NA)) # Return NA if metric calculation failed or result non-finite

    }, error = function(e) {
        # Return NA on any unexpected error during evaluation
        return(NA)
    })
}

if (OPTIMIZE_PARAMETERS) {
    cl <- makeCluster(N_CORES); registerDoParallel(cl); cat("\nStarting parallel optimization across", N_CORES, "cores...\n")
    # Use .combine = 'list' and .errorhandling = 'pass' for robust aggregation
    optimization_results_list <- foreach(i = 1:nrow(param_grid), .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts"), .combine = 'list', .errorhandling = 'pass') %dopar% {
        current_params <- param_grid[i, ]
        pair_metrics <- sapply(potentially_mean_reverting_pairs, function(sym) {
            if (sym %in% names(training_pair_data_list)) { evaluate_params_on_train(sym, training_pair_data_list[[sym]], current_params) } else { NA }
        })
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]
        aggregate_metric <- if(length(valid_metrics) > 0) mean(valid_metrics) else NA
        # Return a named list containing params and results
        list(params = current_params, aggregate_metric = aggregate_metric, num_valid_pairs = length(valid_metrics))
    }
    stopCluster(cl); cat("\nOptimization loop finished.\n")

    # Robustly process the list of lists returned by foreach
    valid_results <- Filter(function(x) !inherits(x, "error") && is.list(x) && !is.null(x$aggregate_metric) && is.finite(x$aggregate_metric), optimization_results_list)

    if(length(valid_results) > 0) {
        # Convert valid results to a data frame
        optimization_results_df <- purrr::map_dfr(valid_results, function(res) {
             bind_cols(as_tibble(res$params), tibble(aggregate_metric = res$aggregate_metric, num_valid_pairs = res$num_valid_pairs))
         })

        # Find the best parameter set
        best_result_row <- optimization_results_df %>% arrange(desc(aggregate_metric)) %>% head(1)

        if(nrow(best_result_row) > 0 && is.finite(best_result_row$aggregate_metric)) {
            best_params <- list( roll_window = best_result_row$roll_window, entry_z = best_result_row$entry_z, exit_z = best_result_row$exit_z, max_scalar = best_result_row$max_scalar, scale_factor = best_result_row$scale_factor)
            cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n"); print(bind_rows(best_params)); cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")
            ROLLING_WINDOW <- best_params$roll_window; ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z; ZSCORE_EXIT_THRESHOLD <- best_params$exit_z; MAX_POSITION_SCALAR <- best_params$max_scalar; SCALING_FACTOR <- best_params$scale_factor
        } else {
             cat("WARNING: Optimization yielded no finite best metric after filtering. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR
        }
    } else {
        cat("WARNING: Optimization failed to yield any valid results. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR
    }
} else {
    cat("Skipping optimization. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR
}


# --- 5. Backtesting Period ---
# (Code unchanged - runs backtest with selected parameters)
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW, " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD, "\n")
backtest_data_long <- all_data_calculated %>% filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>% filter(date >= BACKTEST_START_DATE - days(ROLLING_WINDOW)) # Need buffer for rolling calc
backtest_details <- list()
for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    btc_data_backtest <- backtest_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_backtest <- backtest_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)
    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) { cat(" Insufficient overlap. Skipping.\n"); next }
    pair_data_signals <- pair_data_backtest %>% mutate(
            spread = log(candidate_price) - log(btc_price), rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE), rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd) ) %>% filter(date >= BACKTEST_START_DATE, !is.na(zscore)) # Filter for backtest dates AFTER rolling calc
     if(nrow(pair_data_signals) < 2) { cat(" Insufficient data post Z-score. Skipping.\n"); next }
     pair_data_trades <- pair_data_signals %>% mutate(
            current_signal = case_when(zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, TRUE ~ NA_real_),
            current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast=FALSE), current_signal = ifelse(is.na(current_signal), 0, current_signal),
            prev_signal = lag(current_signal, default = 0), prev_zscore = lag(zscore, default = 0), prev_prev_signal = lag(prev_signal, default=0),
            size_scalar = case_when( prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                     prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                     prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, TRUE ~ 1.0),
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), position = prev_signal * size_scalar) %>% mutate(position = as.numeric(position))
    pair_strategy_returns_tibble <- pair_data_trades %>% mutate( candidate_return = as.numeric(candidate_return), btc_return = as.numeric(btc_return),
            strategy_return = position * (candidate_return - btc_return), strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0), symbol = candidate_symbol) %>% select(date, symbol, strategy_return)
    backtest_details[[candidate_symbol]] <- list(trade_data = pair_data_trades, returns_data = pair_strategy_returns_tibble)
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
}


# --- 6. Performance Metrics & Drawdown Calculation (Revised - No Omega) ---
cat("\n--- Step 6: Calculating Performance Metrics ---\n")
if (length(backtest_details) == 0) stop("No pairs successfully backtested.")

# Helper function (Revised: No Omega, Added CumReturn, Win%, PctInMarket)
calculate_safe_metrics <- function(returns_xts, trade_data, scale = 252, Rf = 0) {
    metrics <- list( AnnReturn = NA_real_, AnnStdDev = NA_real_, Sharpe = NA_real_, MaxDD = NA_real_, Calmar = NA_real_, WinPerc = NA_real_, PctInMarket= NA_real_, CumReturn = NA_real_ )
    if (is.null(returns_xts) || nrow(returns_xts) < 20) {
        cat(" Warning: Not enough returns data (<20) to calculate metrics reliably.\n")
        tryCatch({ metrics$CumReturn <- as.numeric(PerformanceAnalytics::Return.cumulative(returns_xts, geometric=FALSE)) }, error = function(e) {})
        return(metrics)
    }
    sd_val <- sd(coredata(returns_xts), na.rm = TRUE); cat("  Std Dev Check:", sd_val, "\n")
    tryCatch({ metrics$MaxDD <- as.numeric(PerformanceAnalytics::maxDrawdown(returns_xts)) }, error = function(e) {cat(" Error MaxDD:", e$message, "\n")})
    tryCatch({ tbl <- PerformanceAnalytics::table.AnnualizedReturns(returns_xts, scale = scale, geometric = FALSE); metrics$AnnReturn <- as.numeric(tbl[1, 1]); metrics$AnnStdDev <- as.numeric(tbl[2, 1]) }, error = function(e) {cat(" Error Ann Ret/Stdev:", e$message, "\n")})
    tryCatch({ metrics$CumReturn <- as.numeric(PerformanceAnalytics::Return.cumulative(returns_xts, geometric=FALSE)) }, error = function(e) {cat(" Error CumReturn:", e$message, "\n")})

    if (!is.null(trade_data) && "position" %in% names(trade_data) && nrow(trade_data) == nrow(returns_xts)){ # Ensure trade_data aligns
         positions <- trade_data$position[!is.na(trade_data$position)]
         returns_in_market <- coredata(returns_xts)[positions != 0]
         if(length(returns_in_market) > 0) metrics$WinPerc <- mean(returns_in_market > 0, na.rm=TRUE) else metrics$WinPerc <- NA_real_
         metrics$PctInMarket <- mean(positions != 0, na.rm=TRUE)
    }

    if (!is.na(sd_val) && sd_val > SD_OFFSET) {
        tryCatch({ shp <- PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = Rf, scale = scale, geometric = FALSE); metrics$Sharpe <- as.numeric(shp[1,]) }, error = function(e) {cat(" Error Sharpe:", e$message, "\n")})
        tryCatch({ metrics$Calmar <- as.numeric(PerformanceAnalytics::CalmarRatio(returns_xts, scale = scale)) }, error = function(e) {cat(" Error Calmar:", e$message, "\n")})
    } else {
        cat(" Warning: Zero SD. Sharpe, Calmar set to NA.\n"); metrics$Sharpe <- NA_real_
        if(!is.na(metrics$MaxDD) && metrics$MaxDD != 0 && !is.na(metrics$AnnReturn)) { metrics$Calmar <- metrics$AnnReturn / abs(metrics$MaxDD) } else { metrics$Calmar <- NA_real_ }
    }
    return(metrics)
}

# --- 6a. Aggregate Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>% group_by(date) %>% summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>% arrange(date) %>% mutate(portfolio_return = ifelse(is.na(portfolio_return), 0, portfolio_return))
if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts_agg <- xts::xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts_agg) <- "Aggregate Strategy"
    cat("\n--- AGGREGATE Backtest Performance Metrics ---\n")
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts_agg, trade_data = NULL) # Pass NULL for trade_data for aggregate Win%/PctInMarket
    cat("Annualized Return:", scales::percent(agg_metrics$AnnReturn, accuracy=0.01), "\n")
    cat("Annualized Std Dev:", scales::percent(agg_metrics$AnnStdDev, accuracy=0.01), "\n")
    cat("Cumulative Return:", scales::percent(agg_metrics$CumReturn, accuracy=0.01), "\n")
    # Removed Omega
    cat("Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n")
    cat("Maximum Drawdown:", scales::percent(agg_metrics$MaxDD, accuracy=0.01), "\n")
    cat("Calmar Ratio:", round(agg_metrics$Calmar, 3), "\n")
} else { cat("\nError: Aggregate returns empty.\n"); stop("Cannot calculate aggregate metrics.") }

# --- 6b. Individual Pair Performance & Drawdowns ---
cat("\n--- INDIVIDUAL Pair Backtest Performance & Drawdowns ---\n")
individual_metrics_list <- list(); individual_drawdown_tables <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data
    pair_trade_data <- backtest_details[[candidate_symbol]]$trade_data # Get trade data for metrics
    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_tibble <- pair_returns_tibble %>% mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return))
        pair_returns_xts <- xts::xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol
        # Calculate metrics using the safe function, passing trade_data
        pair_metrics <- calculate_safe_metrics(pair_returns_xts, pair_trade_data) # Pass trade data here
        individual_metrics_list[[candidate_symbol]] <- pair_metrics # Store metrics
        # Print Pair Metrics
        cat(" Annualized Return:", scales::percent(pair_metrics$AnnReturn, accuracy=0.01), "\n")
        cat(" Annualized Std Dev:", scales::percent(pair_metrics$AnnStdDev, accuracy=0.01), "\n")
        cat(" Cumulative Return:", scales::percent(pair_metrics$CumReturn, accuracy=0.01), "\n")
        cat(" Win % (of active days):", scales::percent(pair_metrics$WinPerc, accuracy=0.1), "\n")
        cat(" % Time In Market:", scales::percent(pair_metrics$PctInMarket, accuracy=0.1), "\n")
        # Removed Omega
        cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n")
        cat(" Maximum Drawdown:", scales::percent(pair_metrics$MaxDD, accuracy=0.01), "\n")
        cat(" Calmar Ratio:", round(pair_metrics$Calmar, 3), "\n")
        dd_table <- tryCatch(PerformanceAnalytics::table.Drawdowns(pair_returns_xts, top=5), error=function(e) NULL)
        if(!is.null(dd_table)) { cat(" Top 5 Drawdowns:\n"); print(dd_table); individual_drawdown_tables[[candidate_symbol]] <- dd_table } else { cat(" Could not calculate drawdown table.\n") }
        cat("\n")
    } else { cat(" Insufficient return data for", candidate_symbol, "\n\n") }
}

# --- 6c. Create Summary Table ---
summary_df <- tryCatch({
    bind_rows(lapply(individual_metrics_list, function(l) as.data.frame(l)), .id = "Candidate") %>% # Ensure conversion works
    mutate(Pair = paste("BTC", Candidate, sep="-")) %>%
    select(Pair, AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn) %>% # Removed Omega
    arrange(desc(Sharpe))
}, error = function(e) { cat("Error creating summary table:", e$message, "\n"); NULL })
if (!is.null(summary_df)){ cat("\n--- Performance Summary Table ---\n"); print(summary_df, digits=3) }


# --- 7. Plotting (Enhanced Strategy Plot) ---
cat("\n--- Step 7: Plotting Results ---\n")
plot_dir <- "strategy_plots"; if (!dir.exists(plot_dir)) dir.create(plot_dir)
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")
agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png"); png(agg_plot_file, width=800, height=1000); tryCatch({ PerformanceAnalytics::charts.PerformanceSummary(portfolio_returns_xts_agg, main = "Aggregate Strategy Performance", geometric = FALSE)}, error=function(e) plot.new()); dev.off()
btc_prices_backtest <- all_data_calculated %>% filter(symbol == btc_symbol, date >= BACKTEST_START_DATE) %>% select(date, btc_price = price)
for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data
    pair_returns_data <- backtest_details[[candidate_symbol]]$returns_data
    # Plot 1: Spread + Z-Score + Entries
    trade_entries <- pair_plot_data %>% filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>% mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread"))
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) + geom_line(color="black") + labs(title = paste("BTC vs", candidate_symbol, ": Log Spread"), x = NULL, y = "Log Spread") + theme_minimal() + theme(plot.title = element_text(size=10))
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) + geom_line(color="black") +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") + geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") + geom_hline(yintercept = 0, color = "grey") +
        geom_point(data = trade_entries, aes(color = entry_type), size = 2) + scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"), x = "Date", y = "Z-Score", color = NULL) + theme_minimal() + theme(legend.position = "bottom", plot.title = element_text(size=10))
    pair_plot_file_spread <- file.path(plot_dir, paste0("01_spread_z_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_spread, gridExtra::grid.arrange(plot_spread, plot_zscore, ncol = 1, heights = c(1, 2)), width=10, height=6); cat(" Spread/Z plot saved:", pair_plot_file_spread, "\n") }, error = function(e){cat(" Could not save spread/z plot:", e$message, "\n")})
    # Plot 2: Strategy Results Plot (Example Style)
    plot_data_strategy <- pair_plot_data %>% select(date, position) %>% inner_join(pair_returns_data, by="date") %>% inner_join(btc_prices_backtest, by="date") %>%
        mutate(cum_ret = cumprod(1 + strategy_return), CumEQ = STARTING_BALANCE * cum_ret,
               trade_signal = case_when( position != 0 & lag(position, default=0) == 0 ~ 1, position == 0 & lag(position, default=0) != 0 ~ -1, TRUE ~ 0 ),
               pos_start = date, pos_end = lead(date, default = END_DATE + days(1)), position_plot = ifelse(abs(position) < SD_OFFSET, NA, position))
    # Plot: Underlying Price (BTC Price)
    p_price <- ggplot(plot_data_strategy, aes(x=date, y=btc_price)) + geom_line() + labs(y="BTC Price", x=NULL) + theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), plot.title = element_text(size=10)) + ggtitle(paste("Strategy Results: BTC vs", candidate_symbol))
    # Plot: Trades (Entries/Exits Markers)
    p_trades <- ggplot(plot_data_strategy %>% filter(trade_signal != 0), aes(x=date, y=0)) + # Plotting on y=0 line
                  geom_point(aes(shape=factor(trade_signal), color=factor(trade_signal)), size=2.5) +
                  scale_shape_manual(values=c("1"=17, "-1"=17)) + # Use triangles (or other shapes)
                  scale_color_manual(values=c("1"="darkgreen", "-1"="darkred"), guide="none") +
                  labs(y="Trades", x=NULL) + ylim(-1, 1) + # Limit y-axis for clarity
                  theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position="none")
    # Plot: Positions (using geom_rect for block style)
    p_pos <- ggplot(plot_data_strategy %>% filter(!is.na(position_plot))) +
             geom_rect(aes(xmin = pos_start, xmax = pos_end, ymin = 0, ymax = position_plot, fill = factor(sign(position_plot))), alpha=0.8) + # Rect from 0 to position
             geom_hline(yintercept=0) +
             scale_fill_manual(values=c("1"="blue", "-1"="lightblue"), guide="none") + # Diff colors for long/short pos
             labs(y="Position", x=NULL) + theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
             ylim(-MAX_POSITION_SCALAR*1.1, MAX_POSITION_SCALAR*1.1)
    # Plot: Cumulative Equity (Simulated)
    p_cumeq <- ggplot(plot_data_strategy, aes(x=date, y=CumEQ)) + geom_line(color="blue") + labs(y="Equity", x="Date") + theme_minimal() + scale_y_continuous(labels = scales::comma)

    # Arrange and Save Strategy Plot
    pair_plot_file_strategy <- file.path(plot_dir, paste0("02_strategy_results_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_strategy, gridExtra::grid.arrange(p_price, p_trades, p_pos, p_cumeq, ncol=1, heights=c(3, 0.5, 1, 2)), width=10, height=8) # Adjusted heights
               cat(" Strategy results plot saved:", pair_plot_file_strategy, "\n") }, error=function(e){cat(" Could not save strategy plot:", e$message, "\n")})
}


# --- 8. Notes on Bitcoin Fundamental Value Strategy ---
# (Code unchanged)
cat("\n--- Step 8: Notes on Bitcoin Fundamental Value Strategy --- \n")
cat("The analysis and backtest performed above focus on a **relative value pairs trading strategy**.\n")
cat("This strategy relies on the spread between two assets (BTC and a candidate) being mean-reverting, using the Z-score as a signal.\n")
cat("Synthesizing Bitcoin's 'fundamental value' for **directional trading** of Bitcoin itself is a different approach.\n")
cat("Potential fundamental/on-chain metrics for such a model might include:\n")
cat("  - Network Activity: Hash Rate, Active Addresses, Transaction Count/Volume.\n")
cat("  - Valuation Ratios: NVT (Network Value to Transactions), MVRV (Market Value to Realized Value).\n")
cat("  - Supply Dynamics: Stock-to-Flow (S2F) model variations, HODL waves, Exchange Balances.\n")
cat("  - Market Sentiment: Social media analysis, Google trends (as used in some factor models).\n")
cat("  - Macroeconomic Factors: Correlation with traditional assets (e.g., Nasdaq), inflation rates, monetary policy.\n")
cat("Implementing such a strategy would require:\n")
cat("  1. Access to specialized on-chain/fundamental data APIs (e.g., Glassnode, CryptoQuant, potentially paid).\n")
cat("  2. Different modeling techniques (e.g., time series regression, machine learning models like Random Forest or LSTM) to relate these factors to future BTC price movements.\n")
cat("  3. Different signal generation logic (e.g., predict price direction, generate buy/sell signals based on model output).\n")
cat("  4. Different backtesting framework focused on single-asset directional trades (long/short/flat BTC).\n")
cat("This current script does *not* incorporate these fundamental metrics or attempt to model Bitcoin's absolute value.\n")


# --- 9. Notes on Dynamic Portfolio Rebalancing ---
# (Code unchanged)
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n")
cat("Dynamic rebalancing involves adjusting positions periodically (e.g., daily, weekly) to maintain desired target weights or risk exposure.\n")
cat("Why it matters for Pairs Trading:\n")
cat(" 1. Maintaining Dollar Neutrality: If prices diverge significantly, a dollar-neutral pair (e.g., long $1000 BTC, short $1000 ETH) can become unbalanced.\n")
cat(" 2. Maintaining Target Ratio: If the pair is based on a specific price ratio, rebalancing might bring it back towards that ratio.\n")
cat(" 3. Risk Management: Prevents excessive exposure to one leg of the pair.\n")
cat("Implementation Considerations:\n")
cat(" - Frequency: How often to rebalance (daily, weekly, threshold-based)?\n")
cat(" - Thresholds: Rebalance only if the imbalance exceeds a certain percentage?\n")
cat(" - Transaction Costs: Frequent rebalancing incurs higher costs.\n")
cat(" - Mechanism: Calculate current value of long/short legs, determine necessary trades to restore balance.\n")
cat("Where it fits: The rebalancing logic would typically run *daily* within the backtest loop, *after* calculating the day's P&L based on the existing position, and *before* determining the *next* day's position based on the Z-score signal. It might adjust the sizes of the positions held.\n")
cat("NOTE: The current backtest does *not* implement dynamic rebalancing; it uses scaled positions determined at entry and holds that *relative scaling* until exit signal.\n")


cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")
```

# Test8:

```{r}
# --- 0. Setup: Install and Load Libraries ---
# Ensure required packages are installed. Run this once if needed:
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra", "doParallel", "foreach", "scales"))

# Load Libraries
library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
library(httr)
library(jsonlite)
library(slider) # For rolling window calculations
library(PerformanceAnalytics) # For risk metrics
library(gridExtra) # For arranging plots
library(doParallel) # For parallel optimization
library(foreach)    # For parallel optimization
library(scales)     # For formatting percentages

# --- 1. Parameters ---
# Data & Timeframe
N_TOP_COINS <- 15
HISTORY_YEARS <- 6
BACKTEST_YEARS <- 2
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS)
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)

# Filtering & API
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT")

# Analysis Thresholds
VIF_THRESHOLD <- 5
ADF_P_VALUE_THRESHOLD <- 0.05
HURST_THRESHOLD <- 0.5

# Trading Strategy Parameters (Initial - Will be Optimized)
INITIAL_ROLLING_WINDOW <- 30
INITIAL_ZSCORE_ENTRY <- 1.5
INITIAL_ZSCORE_EXIT <- 0.5
SD_OFFSET <- 1e-9

# Position Sizing Parameters (Initial - Could also be optimized)
INITIAL_MAX_POSITION_SCALAR <- 2.0
INITIAL_SCALING_FACTOR <- 0.5

# --- Regime Filter Parameters ---
REGIME_FILTER_ENABLED <- TRUE      # <<< Set to TRUE to enable the volatility regime filter
REGIME_VOL_WINDOW <- 60           # <<< Window for calculating rolling BTC volatility
REGIME_VOL_PERCENTILE <- 0.75     # <<< Percentile threshold (e.g., 0.75 = high vol if > 75th percentile)

# Optimization Parameters
OPTIMIZE_PARAMETERS <- TRUE
OPTIMIZATION_METRIC <- "Sharpe" # ("Sharpe", "Calmar")
STARTING_BALANCE <- 500000

# Parallel Processing Setup for Optimization
N_CORES <- detectCores() - 1
if (N_CORES < 1) N_CORES <- 1


# --- 2. Data Acquisition ---
# (Code is unchanged - Gets CoinGecko list, fetches history via API)
cat("--- Step 2: Acquiring Data ---\n")
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"; cg_params <- list(vs_currency = "usd", order = "market_cap_desc", per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- httr::GET(cg_url, query = cg_params); Sys.sleep(2)
    if (httr::status_code(cg_response) == 200) {
        cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed CoinGecko fetch. Status: ", httr::status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })
cat("Filtering for prime candidates...\n")
prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS - 1)
prime_candidate_symbols <- prime_candidates_info$symbol; btc_symbol <- "BTC"; all_symbols_of_interest <- c(btc_symbol, prime_candidate_symbols)
cat("Prime candidates identified:", paste(prime_candidate_symbols, collapse=", "), "\n")
cat("Fetching", HISTORY_YEARS, "years historical data...\n")
all_data_list_tibbles <- list(); not_found_symbols <- c(); base_url <- "https://min-api.cryptocompare.com/data/v2/histoday"
api_limit <- as.numeric(END_DATE - START_DATE); if(api_limit > 2000) { cat("Warning: Date range may exceed API limit (~2000 days).\n"); api_limit <- 2000 }
Sys.setenv(TZ='UTC')
for (sym in all_symbols_of_interest) {
    cat("Fetching data for:", sym, "..."); tryCatch({
        query_params <- list(fsym = sym, tsym = "USD", limit = api_limit, toTs = as.numeric(as.POSIXct(END_DATE)))
        response <- httr::GET(base_url, query = query_params); Sys.sleep(1.5)
        if (httr::status_code(response) == 200) {
            content <- httr::content(response, "text", encoding = "UTF-8"); json_data <- jsonlite::fromJSON(content)
            if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                df <- json_data$Data$Data %>% as_tibble()
                if (all(c("time", "close", "volumeto") %in% names(df))) {
                    df_processed <- df %>% mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>%
                        select(date, symbol, price = close, volume = volumeto) %>% filter(date >= START_DATE & date <= END_DATE) %>% arrange(date)
                    if(nrow(df_processed) > 0) { all_data_list_tibbles[[sym]] <- df_processed; cat(" Success. Rows:", nrow(df_processed), "Range:", format(min(df_processed$date)), "to", format(max(df_processed$date)), "\n")
                    } else { cat(" No data in range. Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
                } else { cat(" Missing columns. Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
            } else { cat(" API Error:", json_data$Message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
        } else { cat(" HTTP Error:", httr::status_code(response), ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym) }
    }, error = function(e) { cat(" Error:", e$message, ". Skipping.\n"); not_found_symbols <- c(not_found_symbols, sym); Sys.sleep(1.5) })
}
if (length(all_data_list_tibbles) == 0) stop("Failed fetch ANY data.")
if (!("BTC" %in% names(all_data_list_tibbles))) stop("Failed fetch BTC data.")
prime_candidate_symbols <- intersect(prime_candidate_symbols, names(all_data_list_tibbles))
if (length(prime_candidate_symbols) == 0) stop("Failed fetch all candidates.")
cat("Successfully fetched data for:", paste(names(all_data_list_tibbles), collapse=", "), "\n")
all_data_long <- dplyr::bind_rows(all_data_list_tibbles) %>% arrange(symbol, date)


# --- 3. Data Preparation ---
cat("--- Step 3: Calculating Returns ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    mutate(price = as.numeric(price), volume = as.numeric(volume)) %>% group_by(symbol) %>% arrange(date) %>%
    mutate( return = log(price) - log(lag(price)), log_vol_change = log(volume + volume_offset) - log(lag(volume + volume_offset))) %>%
    ungroup() %>% filter(!is.na(return))


# --- 4. Training Period Analysis (Identify Pairs & Calc Regime Threshold) ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)
pairs_trading_analysis_train <- list(); potentially_mean_reverting_pairs <- c(); training_pair_data_list <- list()

# Identify potentially mean-reverting pairs
for (candidate_symbol in prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training - Pair ID)\n")
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)
    if(nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) { cat(" Insufficient overlap. Skipping pair.\n"); next }
    training_pair_data_list[[candidate_symbol]] <- pair_data_train # Store for optimization
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL); adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display=FALSE), error = function(e) NULL); hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) { cat("  >>> Pair BTC-", candidate_symbol, "identified potentially mean-reverting. <<<\n"); potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol) }
    } else { cat("  Insufficient spread data for tests.\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Pair Identification Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")
if (length(potentially_mean_reverting_pairs) == 0) stop("No potentially mean-reverting pairs identified.")

# --- Calculate Regime Threshold on Training Data ---
regime_volatility_threshold <- NA
if (REGIME_FILTER_ENABLED) {
    cat("Calculating volatility regime threshold on training data...\n")
    btc_returns_train <- training_data_long %>% filter(symbol == btc_symbol) %>% arrange(date)
    if(nrow(btc_returns_train) > REGIME_VOL_WINDOW) {
         btc_rolling_vol_train <- slider::slide_dbl(btc_returns_train$return, ~sd(.x, na.rm = TRUE), .before = REGIME_VOL_WINDOW - 1, .complete = TRUE)
         # Calculate threshold based on percentile of non-NA rolling volatility values
         regime_volatility_threshold <- quantile(btc_rolling_vol_train, probs = REGIME_VOL_PERCENTILE, na.rm = TRUE, type = 8) # type 8 is recommended
         cat(" Regime Filter: BTC Rolling Volatility (", REGIME_VOL_WINDOW,"d) Threshold (", scales::percent(REGIME_VOL_PERCENTILE)," percentile) on Training Data:", round(regime_volatility_threshold, 6), "\n")
         if(is.na(regime_volatility_threshold)){
              cat("WARNING: Could not calculate valid regime threshold. Disabling filter.\n")
              REGIME_FILTER_ENABLED <- FALSE
         }
    } else {
         cat("WARNING: Not enough BTC data in training period to calculate regime threshold. Disabling filter.\n")
         REGIME_FILTER_ENABLED <- FALSE
    }
} else {
     cat("Regime filter is disabled.\n")
}


# --- 4.5 Parameter Optimization (on Training Data - Revised for Robustness) ---
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")
# (Reduced grid for faster example execution, expand ranges for thorough search)
param_grid <- expand.grid( roll_window = seq(20, 40, by = 10),
                           entry_z = seq(1.5, 2.0, by = 0.25),
                           exit_z = seq(0.25, 0.75, by = 0.25),
                           max_scalar = INITIAL_MAX_POSITION_SCALAR,
                           scale_factor = INITIAL_SCALING_FACTOR,
                           stringsAsFactors = FALSE) %>%
              filter(exit_z < entry_z)
cat("Total parameter combinations to test:", nrow(param_grid), "\n")

evaluate_params_on_train <- function(pair_symbol, pair_training_data, params) {
    # This function remains largely the same as the previous version
    # It calculates Sharpe or Calmar for a given pair and parameter set on training data
    # Returns NA on errors or insufficient data/SD
    tryCatch({
        roll_win <- params$roll_window; entry_z <- params$entry_z; exit_z <- params$exit_z; max_s <- params$max_scalar; scale_f <- params$scale_factor
        pair_data_signals <- pair_training_data %>% filter(btc_price > 0, candidate_price > 0) %>%
            mutate( spread = log(candidate_price) - log(btc_price), rolling_mean = slider::slide_dbl(spread, mean, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE),
                    rolling_sd = slider::slide_dbl(spread, sd, .before = roll_win - 1, .complete = FALSE, na.rm = TRUE), rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
                    zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)) %>% filter(!is.na(zscore))
        if(nrow(pair_data_signals) < 2) return(NA)
        pair_data_trades <- pair_data_signals %>% mutate(
                current_signal = case_when(zscore > entry_z ~ -1, zscore < -entry_z ~ 1, abs(zscore) < exit_z ~ 0, TRUE ~ NA_real_),
                current_signal = zoo::na.locf(current_signal, na.rm = FALSE, fromLast=FALSE), current_signal = ifelse(is.na(current_signal), 0, current_signal),
                prev_signal = lag(current_signal, default = 0), prev_zscore = lag(zscore, default = 0), prev_prev_signal = lag(prev_signal, default=0),
                size_scalar = case_when( prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (prev_zscore - entry_z), max_s),
                                         prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + scale_f * (abs(prev_zscore) - entry_z), max_s),
                                         prev_signal != 0 & prev_signal == prev_prev_signal ~ 1.0, TRUE ~ 1.0),
                size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), position = prev_signal * size_scalar) %>% mutate(position = as.numeric(position))
        pair_strategy_returns <- pair_data_trades %>% mutate( strategy_return = position * (candidate_return - btc_return), strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0)) %>% select(date, strategy_return)
        if(nrow(pair_strategy_returns) < 20) return(NA)
        returns_xts <- xts::xts(pair_strategy_returns$strategy_return, order.by = pair_strategy_returns$date)
        sd_val_train <- sd(coredata(returns_xts), na.rm=TRUE)
        if (is.na(sd_val_train) || sd_val_train < SD_OFFSET) return(NA)
        metric_value <- NA
        if (OPTIMIZATION_METRIC == "Sharpe") { metric_value <- tryCatch(PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = 0, scale = 252, geometric = FALSE)[1,], error=function(e) NA)
        } else if (OPTIMIZATION_METRIC == "Calmar") { metric_value <- tryCatch(PerformanceAnalytics::CalmarRatio(returns_xts, scale = 252), error=function(e) NA)
        } else { stop("Invalid OPTIMIZATION_METRIC") }
        metric_value <- as.numeric(metric_value)
        return(ifelse(is.finite(metric_value), metric_value, NA))
    }, error = function(e) { return(NA) })
}

if (OPTIMIZE_PARAMETERS) {
    cl <- makeCluster(N_CORES); registerDoParallel(cl); cat("\nStarting parallel optimization across", N_CORES, "cores...\n")
    optimization_results_list <- foreach(i = 1:nrow(param_grid), .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts"), .combine = 'list', .errorhandling = 'pass') %dopar% {
        current_params <- param_grid[i, ]
        pair_metrics <- sapply(potentially_mean_reverting_pairs, function(sym) { if (sym %in% names(training_pair_data_list)) { evaluate_params_on_train(sym, training_pair_data_list[[sym]], current_params) } else { NA } })
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]
        aggregate_metric <- if(length(valid_metrics) > 0) mean(valid_metrics) else NA
        list(params = current_params, aggregate_metric = aggregate_metric, num_valid_pairs = length(valid_metrics))
    }
    stopCluster(cl); cat("\nOptimization loop finished.\n")
    valid_results <- Filter(function(x) !inherits(x, "error") && is.list(x) && !is.null(x$aggregate_metric) && is.finite(x$aggregate_metric), optimization_results_list)
    if(length(valid_results) > 0) {
        optimization_results_df <- purrr::map_dfr(valid_results, ~bind_cols(as_tibble(.x$params), tibble(aggregate_metric = .x$aggregate_metric, num_valid_pairs = .x$num_valid_pairs)))
        best_result_row <- optimization_results_df %>% arrange(desc(aggregate_metric)) %>% head(1)
        if(nrow(best_result_row) > 0 && is.finite(best_result_row$aggregate_metric)) {
            best_params <- list( roll_window = best_result_row$roll_window, entry_z = best_result_row$entry_z, exit_z = best_result_row$exit_z, max_scalar = best_result_row$max_scalar, scale_factor = best_result_row$scale_factor)
            cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n"); print(bind_rows(best_params)); cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")
            ROLLING_WINDOW <- best_params$roll_window; ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z; ZSCORE_EXIT_THRESHOLD <- best_params$exit_z; MAX_POSITION_SCALAR <- best_params$max_scalar; SCALING_FACTOR <- best_params$scale_factor
        } else { cat("WARNING: Optimization yielded no finite best metric. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }
    } else { cat("WARNING: Optimization failed. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }
} else { cat("Skipping optimization. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }


# --- 5. Backtesting Period (with Regime Filter) ---
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW, " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD, "\n")
if(REGIME_FILTER_ENABLED) cat("Regime Filter: Enabled (BTC Vol > ", round(regime_volatility_threshold, 6), ")\n") else cat("Regime Filter: Disabled\n")

# Get data including buffer for rolling calcs
backtest_data_full_period <- all_data_calculated %>%
    filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>%
    filter(date >= BACKTEST_START_DATE - days(max(ROLLING_WINDOW, REGIME_VOL_WINDOW)))

# Pre-calculate regime state for the full backtest period + buffer
if(REGIME_FILTER_ENABLED) {
    btc_rolling_vol_backtest <- backtest_data_full_period %>%
        filter(symbol == btc_symbol) %>% arrange(date) %>%
        mutate(
             rolling_vol = slider::slide_dbl(return, ~sd(.x, na.rm = TRUE), .before = REGIME_VOL_WINDOW - 1, .complete = TRUE),
             market_regime = ifelse(!is.na(rolling_vol) & rolling_vol > regime_volatility_threshold, "High Vol", "Normal")
        ) %>% select(date, market_regime)
    # Join regime state back to the main data
    backtest_data_full_period <- left_join(backtest_data_full_period, btc_rolling_vol_backtest, by = "date") %>%
                                 # Fill initial NA regimes with Normal (conservative assumption)
                                 mutate(market_regime = ifelse(is.na(market_regime), "Normal", market_regime))
} else {
     # If filter disabled, add a dummy column
     backtest_data_full_period <- backtest_data_full_period %>% mutate(market_regime = "Normal")
}


backtest_details <- list() # Store results
for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    # Filter data for the pair for the full period needed for rolling calcs
    btc_data_backtest <- backtest_data_full_period %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, market_regime)
    candidate_data_backtest <- backtest_data_full_period %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) { cat(" Insufficient overlap. Skipping.\n"); next }

    # Calculate Spread, Z-Score, Signals, Positions (using selected parameters & regime filter)
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        filter(date >= BACKTEST_START_DATE, !is.na(zscore)) # Filter for backtest dates AFTER rolling calc

     if(nrow(pair_data_signals) < 2) { cat(" Insufficient data post Z-score. Skipping.\n"); next }

     pair_data_trades <- pair_data_signals %>%
         mutate(
            # Raw signal based on Z-score
            raw_signal = case_when(zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, TRUE ~ NA_real_),
            raw_signal = zoo::na.locf(raw_signal, na.rm = FALSE, fromLast=FALSE), # Carry forward signal if between bands
            raw_signal = ifelse(is.na(raw_signal), 0, raw_signal),

            # Lagged regime state (regime on the day signal was generated)
            prev_regime = lag(market_regime, default = "Normal"),

            # Apply Regime Filter: Prevent *new* entries during High Vol
            # If previous position was 0 and regime was High Vol, force signal to 0
            current_signal = ifelse(lag(raw_signal, default=0) == 0 & prev_regime == "High Vol", 0, raw_signal),

            # Position based on previous day's (potentially filtered) signal
            prev_signal = lag(current_signal, default = 0),
            prev_zscore = lag(zscore, default = 0),
            prev_prev_signal = lag(prev_signal, default=0), # Need signal from t-2 to detect entry

            # Size scalar based on Z-score *at entry* (when prev_signal != 0 and prev_prev_signal == 0)
            size_scalar = case_when(
                 prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                 prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                 prev_signal != 0 & prev_signal == prev_prev_signal ~ lag(pmax(1.0, # Carry forward entry scalar if holding
                       case_when( lag(prev_signal, default=0) == -1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (lag(prev_zscore,default=0) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                  lag(prev_signal, default=0) == 1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (abs(lag(prev_zscore,default=0)) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                  TRUE ~ 1.0 )), default=1.0), # Default to 1 if issues lagging scalar
                 TRUE ~ 1.0 # Default if flat or exiting
            ),
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), # Ensure scalar is 0 if flat, >=1 if holding

            position = prev_signal * size_scalar
        ) %>% mutate(position = as.numeric(position))

    # Calculate Strategy Returns
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate( candidate_return = as.numeric(candidate_return), btc_return = as.numeric(btc_return),
                strategy_return = position * (candidate_return - btc_return),
                strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0),
                symbol = candidate_symbol) %>%
        select(date, symbol, strategy_return)

    # Store results
    backtest_details[[candidate_symbol]] <- list(trade_data = pair_data_trades, returns_data = pair_strategy_returns_tibble)
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
}


# --- 6. Performance Metrics & Drawdown Calculation (No Omega) ---
cat("\n--- Step 6: Calculating Performance Metrics ---\n")
if (length(backtest_details) == 0) stop("No pairs successfully backtested.")

# Helper function (No Omega)
calculate_safe_metrics <- function(returns_xts, trade_data, scale = 252, Rf = 0) {
    # Initialize metrics list
    metrics <- list(AnnReturn=NA_real_, AnnStdDev=NA_real_, Sharpe=NA_real_, MaxDD=NA_real_, Calmar=NA_real_, WinPerc=NA_real_, PctInMarket=NA_real_, CumReturn=NA_real_)
    if (is.null(returns_xts) || nrow(returns_xts) < 20) { cat(" Warning: Not enough returns data (<20).\n"); tryCatch({ metrics$CumReturn <- as.numeric(Return.cumulative(returns_xts, geometric=FALSE)) }, error=function(e){}); return(metrics) }
    sd_val <- sd(coredata(returns_xts), na.rm = TRUE); cat("  Std Dev Check:", sd_val, "\n")
    # Calculate standard metrics
    tryCatch({ metrics$MaxDD <- as.numeric(maxDrawdown(returns_xts)) }, error=function(e) {cat(" Error MaxDD:", e$message, "\n")})
    tryCatch({ tbl <- table.AnnualizedReturns(returns_xts, scale=scale, geometric=FALSE); metrics$AnnReturn <- as.numeric(tbl[1, 1]); metrics$AnnStdDev <- as.numeric(tbl[2, 1]) }, error=function(e) {cat(" Error Ann Ret/Stdev:", e$message, "\n")})
    tryCatch({ metrics$CumReturn <- as.numeric(Return.cumulative(returns_xts, geometric=FALSE)) }, error=function(e) {cat(" Error CumReturn:", e$message, "\n")})
    # Calculate trade-based metrics if trade_data is valid and aligns
    if (!is.null(trade_data) && "position" %in% names(trade_data) && nrow(trade_data) == nrow(returns_xts)){
         positions <- trade_data$position[!is.na(trade_data$position)]
         returns_in_market <- coredata(returns_xts)[positions != 0]
         if(length(returns_in_market) > 0) metrics$WinPerc <- mean(returns_in_market > 0, na.rm=TRUE) else metrics$WinPerc <- NA_real_
         metrics$PctInMarket <- mean(positions != 0, na.rm=TRUE)
    } else if (!is.null(trade_data)) { cat(" Warning: trade_data misaligned or missing position column for Win%/PctInMarket.\n") }
    # Calculate metrics sensitive to standard deviation
    if (!is.na(sd_val) && sd_val > SD_OFFSET) {
        tryCatch({ shp <- SharpeRatio.annualized(returns_xts, Rf=Rf, scale=scale, geometric=FALSE); metrics$Sharpe <- as.numeric(shp[1,]) }, error=function(e) {cat(" Error Sharpe:", e$message, "\n")})
        tryCatch({ metrics$Calmar <- as.numeric(CalmarRatio(returns_xts, scale=scale)) }, error=function(e) {cat(" Error Calmar:", e$message, "\n")})
    } else {
        cat(" Warning: Zero SD. Sharpe, Calmar set to NA.\n"); metrics$Sharpe <- NA_real_
        if(!is.na(metrics$MaxDD) && metrics$MaxDD != 0 && !is.na(metrics$AnnReturn)) { metrics$Calmar <- metrics$AnnReturn / abs(metrics$MaxDD) } else { metrics$Calmar <- NA_real_ }
    }
    return(metrics)
}

# --- 6a. Aggregate Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>% group_by(date) %>% summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>% arrange(date) %>% mutate(portfolio_return = ifelse(is.na(portfolio_return), 0, portfolio_return))
if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts_agg <- xts::xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts_agg) <- "Aggregate Strategy"
    cat("\n--- AGGREGATE Backtest Performance Metrics ---\n")
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts_agg, trade_data = NULL)
    cat("Annualized Return:", scales::percent(agg_metrics$AnnReturn, accuracy=0.01), "\n")
    cat("Annualized Std Dev:", scales::percent(agg_metrics$AnnStdDev, accuracy=0.01), "\n")
    cat("Cumulative Return:", scales::percent(agg_metrics$CumReturn, accuracy=0.01), "\n")
    cat("Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n")
    cat("Maximum Drawdown:", scales::percent(agg_metrics$MaxDD, accuracy=0.01), "\n")
    cat("Calmar Ratio:", round(agg_metrics$Calmar, 3), "\n")
} else { cat("\nError: Aggregate returns empty.\n"); stop("Cannot calculate aggregate metrics.") }

# --- 6b. Individual Pair Performance & Drawdowns ---
cat("\n--- INDIVIDUAL Pair Backtest Performance & Drawdowns ---\n")
individual_metrics_list <- list(); individual_drawdown_tables <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data
    pair_trade_data <- backtest_details[[candidate_symbol]]$trade_data
    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_tibble <- pair_returns_tibble %>% mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return))
        pair_returns_xts <- xts::xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol
        pair_metrics <- calculate_safe_metrics(pair_returns_xts, pair_trade_data)
        individual_metrics_list[[candidate_symbol]] <- pair_metrics
        cat(" Annualized Return:", scales::percent(pair_metrics$AnnReturn, accuracy=0.01), "\n")
        cat(" Annualized Std Dev:", scales::percent(pair_metrics$AnnStdDev, accuracy=0.01), "\n")
        cat(" Cumulative Return:", scales::percent(pair_metrics$CumReturn, accuracy=0.01), "\n")
        cat(" Win % (of active days):", scales::percent(pair_metrics$WinPerc, accuracy=0.1), "\n")
        cat(" % Time In Market:", scales::percent(pair_metrics$PctInMarket, accuracy=0.1), "\n")
        cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n")
        cat(" Maximum Drawdown:", scales::percent(pair_metrics$MaxDD, accuracy=0.01), "\n")
        cat(" Calmar Ratio:", round(pair_metrics$Calmar, 3), "\n")
        dd_table <- tryCatch(PerformanceAnalytics::table.Drawdowns(pair_returns_xts, top=5), error=function(e) NULL)
        if(!is.null(dd_table)) { cat(" Top 5 Drawdowns:\n"); print(dd_table); individual_drawdown_tables[[candidate_symbol]] <- dd_table } else { cat(" Could not calculate drawdown table.\n") }
        cat("\n")
    } else { cat(" Insufficient return data for", candidate_symbol, "\n\n") }
}

# --- 6c. Create Summary Table ---
summary_df <- tryCatch({
    bind_rows(lapply(individual_metrics_list, function(l) as.data.frame(l)), .id = "Candidate") %>%
    mutate(Pair = paste("BTC", Candidate, sep="-")) %>%
    select(Pair, AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn) %>%
    arrange(desc(Sharpe))
}, error = function(e) { cat("Error creating summary table:", e$message, "\n"); NULL })
if (!is.null(summary_df)){ cat("\n--- Performance Summary Table ---\n"); print(summary_df, digits=3) }


# --- 7. Plotting (Revised Strategy Plot) ---
cat("\n--- Step 7: Plotting Results ---\n")
plot_dir <- "strategy_plots"; if (!dir.exists(plot_dir)) dir.create(plot_dir)
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")
agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png"); png(agg_plot_file, width=800, height=1000); tryCatch({ PerformanceAnalytics::charts.PerformanceSummary(portfolio_returns_xts_agg, main = "Aggregate Strategy Performance", geometric = FALSE)}, error=function(e) plot.new()); dev.off()
btc_prices_backtest <- all_data_calculated %>% filter(symbol == btc_symbol, date >= BACKTEST_START_DATE) %>% select(date, btc_price = price)

for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data
    pair_returns_data <- backtest_details[[candidate_symbol]]$returns_data

    # Plot 1: Spread + Z-Score + Entries + Regime Background
    trade_entries <- pair_plot_data %>% filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>% mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread"))
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) + geom_line(color="black") + labs(title = paste("BTC vs", candidate_symbol, ": Log Spread"), x = NULL, y = "Log Spread") + theme_minimal() + theme(plot.title = element_text(size=10))
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) +
        # Add shaded background for High Vol regime
        geom_rect(data = . %>% filter(market_regime == "High Vol"), aes(xmin=date, xmax=lead(date, default=max(date)+days(1)), ymin=-Inf, ymax=Inf), fill="grey80", alpha=0.3, inherit.aes = FALSE) +
        geom_line(color="black") +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") + geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") + geom_hline(yintercept = 0, color = "grey") +
        geom_point(data = trade_entries, aes(color = entry_type), size = 2) + scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"), x = "Date", y = "Z-Score", color = NULL) + theme_minimal() + theme(legend.position = "bottom", plot.title = element_text(size=10))
    pair_plot_file_spread <- file.path(plot_dir, paste0("01_spread_z_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_spread, gridExtra::grid.arrange(plot_spread, plot_zscore, ncol = 1, heights = c(1, 2)), width=10, height=6); cat(" Spread/Z plot saved:", pair_plot_file_spread, "\n") }, error = function(e){cat(" Could not save spread/z plot:", e$message, "\n")})

    # Plot 2: Strategy Results Plot (Similar to user example)
    plot_data_strategy <- pair_plot_data %>% select(date, position, market_regime) %>% inner_join(pair_returns_data, by="date") %>% inner_join(btc_prices_backtest, by="date") %>%
        mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return), # Ensure no NAs
               CumEQ = STARTING_BALANCE * cumprod(1 + strategy_return), # Calculate equity curve
               # Trades: Use change in position sign
               trade_signal = case_when( sign(position) != 0 & lag(sign(position), default=0) == 0 ~ sign(position), # Entry
                                         sign(position) == 0 & lag(sign(position), default=0) != 0 ~ -lag(sign(position)), # Exit (opposite sign of prev pos)
                                         TRUE ~ 0 ),
               pos_start = date, pos_end = lead(date, default = END_DATE + days(1)),
               position_plot = ifelse(abs(position) < SD_OFFSET, NA, position)) # NA for plotting gaps

    # Plot: Underlying Price (BTC Price)
    p_price <- ggplot(plot_data_strategy, aes(x=date, y=btc_price)) + geom_line() + labs(y="BTC Price", x=NULL) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + ggtitle(paste("Strategy Results: BTC vs", candidate_symbol))
    # Plot: Trades (as points on y=0)
    p_trades <- ggplot(plot_data_strategy %>% filter(trade_signal != 0), aes(x=date, y=0)) +
                  geom_point(aes(shape=factor(sign(trade_signal)), color=factor(sign(trade_signal))), size=2.0) +
                  scale_shape_manual(values=c("1"=17, "-1"=17), guide="none") + # Triangle Up/Down
                  scale_color_manual(values=c("1"="darkgreen", "-1"="darkred"), guide="none") +
                  labs(y="Trades", x=NULL) + ylim(-1, 1) +
                  theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
    # Plot: Positions (using geom_rect)
    p_pos <- ggplot(plot_data_strategy %>% filter(!is.na(position_plot))) +
             geom_rect(aes(xmin = pos_start, xmax = pos_end, ymin = 0, ymax = position_plot, fill = factor(sign(position_plot))), alpha=0.8) +
             geom_hline(yintercept=0) +
             scale_fill_manual(values=c("1"="blue", "-1"="lightblue"), guide="none") +
             labs(y="Position", x=NULL) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
             ylim(-MAX_POSITION_SCALAR*1.1, MAX_POSITION_SCALAR*1.1)
    # Plot: Cumulative Equity (Simulated)
    p_cumeq <- ggplot(plot_data_strategy, aes(x=date, y=CumEQ)) + geom_line(color="blue") + labs(y="Equity", x="Date") + theme_minimal(base_size = 9) + scale_y_continuous(labels = scales::comma)

    # Arrange and Save Strategy Plot
    pair_plot_file_strategy <- file.path(plot_dir, paste0("02_strategy_results_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_strategy, gridExtra::grid.arrange(p_price, p_trades, p_pos, p_cumeq, ncol=1, heights=c(3, 0.5, 1.5, 2)), width=8, height=6) # Adjusted layout
               cat(" Strategy results plot saved:", pair_plot_file_strategy, "\n") }, error=function(e){cat(" Could not save strategy plot:", e$message, "\n")})
}


# --- 8. Notes on Bitcoin Fundamental Value Strategy ---
# (Code unchanged)
cat("\n--- Step 8: Notes on Bitcoin Fundamental Value Strategy --- \n")#...rest of notes

# --- 9. Notes on Dynamic Portfolio Rebalancing ---
# (Code unchanged)
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n") #...rest of notes

cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")
```

# **TEST9:**

```{r}

# --- 0. Setup: Install and Load Libraries ---
# Ensure required packages are installed. Run this once if needed:
# install.packages(c("tidyverse", "lmtest", "car", "tseries", "pracma", "lubridate", "httr", "jsonlite", "slider", "PerformanceAnalytics", "gridExtra", "doParallel", "foreach", "scales", "remotes"))
# remotes::install_github("coinmetrics/api-client-r") # Install CoinMetrics client

# Load Libraries
library(tidyverse)
library(lmtest)
library(car)
library(tseries)
library(pracma) # For Hurst exponent
library(lubridate)
# library(httr) # No longer needed for fetching
# library(jsonlite) # No longer needed for fetching
library(coinmetrics) # <<< Added CoinMetrics client
library(slider)
library(PerformanceAnalytics)
library(gridExtra)
library(doParallel)
library(foreach)
library(scales)

# --- 1. Parameters ---
# Data & Timeframe
N_TOP_COINS <- 15
HISTORY_YEARS <- 6
BACKTEST_YEARS <- 2
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS)
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)

# Filtering
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT") # BUSD might be less relevant now
unsupported_assets_cm <- c(
  "sol", "selt", "ton", "leo", "avax", 
  "sui", "wsteth", "usds", "shib", "hbar"
)

# CoinMetrics API Key - Set as environment variable CM_API_KEY
CM_API_KEY <- Sys.getenv("CM_API_KEY")
if (CM_API_KEY == "") {
  warning("CoinMetrics API Key not found in environment variable CM_API_KEY. Free tier limits will apply heavily.")
  # Optionally, stop execution if key is mandatory: stop("CM_API_KEY not set.")
}

# Analysis Thresholds
VIF_THRESHOLD <- 5
ADF_P_VALUE_THRESHOLD <- 0.05
HURST_THRESHOLD <- 0.5

# Trading Strategy Parameters (Initial - Will be Optimized)
INITIAL_ROLLING_WINDOW <- 30
INITIAL_ZSCORE_ENTRY <- 1.5
INITIAL_ZSCORE_EXIT <- 0.5
SD_OFFSET <- 1e-9

# Position Sizing Parameters (Initial)
INITIAL_MAX_POSITION_SCALAR <- 2.0
INITIAL_SCALING_FACTOR <- 0.5

# Regime Filter Parameters
REGIME_FILTER_ENABLED <- TRUE
REGIME_VOL_WINDOW <- 60           # Window for BTC volatility
REGIME_ADR_WINDOW <- 30           # Window for BTC active address momentum/smoothing
REGIME_VOL_PERCENTILE <- 0.75     # High vol threshold
REGIME_ADR_PERCENTILE <- 0.25     # Low active address momentum threshold

# Optimization Parameters
OPTIMIZE_PARAMETERS <- TRUE
OPTIMIZATION_METRIC <- "Sharpe" # ("Sharpe", "Calmar")
STARTING_BALANCE <- 500000

# Parallel Processing Setup
N_CORES <- detectCores() - 1
if (N_CORES < 1) N_CORES <- 1


# --- 2. Data Acquisition (Using coinmetrics Package - Filtering Unsupported Assets) ---
cat("--- Step 2: Acquiring Data using CoinMetrics ---\n")
# Get list of coins (use CoinGecko for ranking initially)
cat("Fetching coin list and market caps from CoinGecko...\n")
top_coins_df <- data.frame()
tryCatch({
    cg_url <- "https://api.coingecko.com/api/v3/coins/markets"; cg_params <- list(vs_currency = "usd", order = "market_cap_desc", per_page = N_TOP_COINS + length(STABLECOINS) + 10, page = 1, sparkline = "false")
    cg_response <- httr::GET(cg_url, query = cg_params); Sys.sleep(2)
    if (httr::status_code(cg_response) == 200) {
        cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8"))
        top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol))
        cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
    } else { stop("Failed CoinGecko fetch. Status: ", httr::status_code(cg_response)) }
}, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })

cat("Filtering for prime candidates (initial list)...\n")
prime_candidates_info <- top_coins_df %>%
    filter(!symbol %in% STABLECOINS, symbol != "BTC") %>%
    # Fetch slightly more initially to allow for filtering unsupported ones later
    slice_head(n = N_TOP_COINS + length(unsupported_assets_cm)) # Fetch more initially

# Initial list of candidates (UPPERCASE) from CoinGecko ranking
initial_prime_candidate_symbols <- prime_candidates_info$symbol
btc_symbol <- "BTC"
initial_all_symbols_of_interest <- c(btc_symbol, initial_prime_candidate_symbols)
cat("Initial candidates identified:", paste(initial_prime_candidate_symbols, collapse=", "), "\n")

# --- Filter out specifically unsupported assets BEFORE API call ---
# Ensure list uses correct lowercase CoinMetrics IDs
unsupported_assets_cm <- c("sol", "steth", "ton", "leo", "avax") # Corrected 'selt' to 'steth'

# Create the list of lowercase symbols for the API call
all_symbols_cm_unfiltered <- tolower(initial_all_symbols_of_interest)
# Remove the known unsupported assets
all_symbols_cm_filtered <- setdiff(all_symbols_cm_unfiltered, unsupported_assets_cm)
# Ensure we still have enough candidates + BTC
if (length(all_symbols_cm_filtered) <= 1) {
    stop("Not enough supported assets remain after filtering.")
}
# --- END Filtering ---

# Check for API Key but allow script to continue if blank
CM_API_KEY <- Sys.getenv("CM_API_KEY")
if (CM_API_KEY == "") {
  warning("CoinMetrics API Key (CM_API_KEY) is blank. Using free tier access - rate limits and metric availability will be restricted.", immediate. = TRUE)
} else {
  cat("Using provided CoinMetrics API Key.\n")
}

# Define Metrics (Using Vlm as likely available, change if needed)
metrics_to_fetch <- c("PriceUSD", "TxTfrValAdjUSD", "AdrActCnt", "TxCnt")

cat("Attempting to fetch data from CoinMetrics for:", paste(toupper(all_symbols_cm_filtered), collapse=", "), "\n") # <<< Print filtered list
cat("Metrics:", paste(metrics_to_fetch, collapse=", "), "\n")

# Fetch data using the *filtered* list
all_data_cm <- tryCatch({
    get_asset_metrics(
        assets = all_symbols_cm_filtered, # <<< Use the filtered list
        metrics = metrics_to_fetch,
        start_time = format(START_DATE),
        end_time = format(END_DATE),
        frequency = "1d",
        api_key = CM_API_KEY,
        paging_from = "start"
    )
}, error = function(e) {
    cat("Error fetching data from CoinMetrics API:", e$message, "\n")
    cat("Check internet connection, metric/asset names, and rate limits. If using free tier, limits might be strict.\n")
    NULL
})

if (is.null(all_data_cm) || nrow(all_data_cm) == 0) {
    stop("Failed to fetch any data from CoinMetrics. Exiting.")
}

cat("Successfully fetched data from CoinMetrics. Processing...\n")

# Process fetched data
all_data_long_raw <- all_data_cm %>%
    mutate(date = as.Date(time)) %>%
    select(date, asset, metric, value) %>%
    mutate(symbol = toupper(asset)) %>%
    select(-asset)

all_data_long <- all_data_long_raw %>%
    pivot_wider(names_from = metric, values_from = value) %>%
    rename(price = PriceUSD, volume = TxTfrValAdjUSD, adr_act_cnt = AdrActCnt, tx_cnt = TxCnt) %>%
    mutate(across(c(price, volume, adr_act_cnt, tx_cnt), as.numeric)) %>%
    select(date, symbol, price, volume, adr_act_cnt, tx_cnt) %>%
    filter(complete.cases(price)) %>%
    arrange(symbol, date)

# --- Final check and update of prime_candidate_symbols ---
# This uses the symbols for which data was ACTUALLY retrieved
fetched_symbols <- unique(all_data_long$symbol)
# Ensure BTC data was retrieved
if (!("BTC" %in% fetched_symbols)) stop("Failed fetch BTC data from CoinMetrics.")
# Update prime_candidate_symbols to only those successfully fetched (excluding BTC)
prime_candidate_symbols <- intersect(initial_prime_candidate_symbols, fetched_symbols)
# Ensure we still have candidates left
if (length(prime_candidate_symbols) == 0) stop("Failed fetch data for any prime candidates from CoinMetrics.")
cat("Data successfully processed for:", paste(c("BTC", prime_candidate_symbols), collapse=", "), "\n")
# --- End Update ---

# --- 3. Data Preparation ---
cat("--- Step 3: Calculating Returns & Indicators ---\n")
volume_offset <- 1e-6
all_data_calculated <- all_data_long %>%
    group_by(symbol) %>% arrange(date) %>%
    mutate(
        return = log(price) - log(lag(price)),
        # Calculate rolling volatility (e.g., for BTC)
        rolling_vol = slider::slide_dbl(return, ~sd(.x, na.rm = TRUE), .before = REGIME_VOL_WINDOW - 1, .complete = TRUE),
        # Calculate rolling average/momentum of active addresses (e.g., for BTC)
        adr_act_smooth = slider::slide_dbl(adr_act_cnt, ~mean(.x, na.rm=TRUE), .before = REGIME_ADR_WINDOW - 1, .complete = TRUE),
        adr_act_mom = adr_act_smooth / lag(adr_act_smooth, REGIME_ADR_WINDOW) - 1 # Example: Momentum over window
    ) %>%
    ungroup() %>%
    filter(!is.na(return)) # Keep only rows with valid returns


# --- 4. Training Period Analysis (Identify Pairs & Calc Regime Threshold) ---
cat("--- Step 4: Analyzing Training Period (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_long <- all_data_calculated %>% filter(date <= TRAINING_END_DATE)
pairs_trading_analysis_train <- list(); potentially_mean_reverting_pairs <- c(); training_pair_data_list <- list()

# Identify potentially mean-reverting pairs (same logic as before)
for (candidate_symbol in prime_candidate_symbols) {
    # ... (Keep the pair identification logic using ADF/Hurst - same as previous version) ...
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training - Pair ID)\n")
    btc_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return)
    candidate_data_train <- training_data_long %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)
    if(nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) { cat(" Insufficient overlap. Skipping pair.\n"); next }
    training_pair_data_list[[candidate_symbol]] <- pair_data_train # Store for optimization
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread)
        adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL); adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA
        hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display=FALSE), error = function(e) NULL); hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) { cat("  >>> Pair BTC-", candidate_symbol, "identified potentially mean-reverting. <<<\n"); potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol) }
    } else { cat("  Insufficient spread data for tests.\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Pair Identification Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")
if (length(potentially_mean_reverting_pairs) == 0) stop("No potentially mean-reverting pairs identified.")

# --- Calculate Regime Thresholds on Training Data ---
regime_vol_threshold_value <- NA
regime_adr_mom_threshold_value <- NA
if (REGIME_FILTER_ENABLED) {
    cat("Calculating volatility/adr regime thresholds on training data...\n")
    btc_regime_data_train <- training_data_long %>% filter(symbol == btc_symbol) %>% select(date, rolling_vol, adr_act_mom)
    if(nrow(btc_regime_data_train) > max(REGIME_VOL_WINDOW, REGIME_ADR_WINDOW)) {
         regime_vol_threshold_value <- quantile(btc_regime_data_train$rolling_vol, probs = REGIME_VOL_PERCENTILE, na.rm = TRUE, type = 8)
         regime_adr_mom_threshold_value <- quantile(btc_regime_data_train$adr_act_mom, probs = REGIME_ADR_PERCENTILE, na.rm = TRUE, type = 8)
         cat(" Regime Filter Thresholds (Training Data):\n")
         cat("   BTC Rolling Vol (", REGIME_VOL_WINDOW,"d) >", round(regime_vol_threshold_value, 6), "(", scales::percent(REGIME_VOL_PERCENTILE)," percentile)\n")
         cat("   BTC Active Address Momentum (", REGIME_ADR_WINDOW,"d) <", round(regime_adr_mom_threshold_value, 4), "(", scales::percent(REGIME_ADR_PERCENTILE)," percentile)\n")
         if(is.na(regime_vol_threshold_value) || is.na(regime_adr_mom_threshold_value)){
              cat("WARNING: Could not calculate valid regime thresholds. Disabling filter.\n")
              REGIME_FILTER_ENABLED <- FALSE
         }
    } else {
         cat("WARNING: Not enough BTC data in training period for regime thresholds. Disabling filter.\n")
         REGIME_FILTER_ENABLED <- FALSE
    }
} else { cat("Regime filter is disabled.\n") }


# --- 4.5 Parameter Optimization (on Training Data - No Omega) ---
# (Code largely unchanged, uses evaluate_params_on_train which no longer calls Omega)
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")
param_grid <- expand.grid( roll_window = seq(20, 40, by = 10), entry_z = seq(1.5, 2.0, by = 0.25), exit_z = seq(0.25, 0.75, by = 0.25),
                           max_scalar = INITIAL_MAX_POSITION_SCALAR, scale_factor = INITIAL_SCALING_FACTOR, stringsAsFactors = FALSE) %>% filter(exit_z < entry_z)
cat("Total parameter combinations to test:", nrow(param_grid), "\n")
# evaluate_params_on_train function (already defined and corrected in previous response)
# ...
if (OPTIMIZE_PARAMETERS) {
    cl <- makeCluster(N_CORES); registerDoParallel(cl); cat("\nStarting parallel optimization across", N_CORES, "cores...\n")
    optimization_results_list <- foreach(i = 1:nrow(param_grid), .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts"), .combine = 'list', .errorhandling = 'pass') %dopar% {
        current_params <- param_grid[i, ]
        pair_metrics <- sapply(potentially_mean_reverting_pairs, function(sym) { if (sym %in% names(training_pair_data_list)) { evaluate_params_on_train(sym, training_pair_data_list[[sym]], current_params) } else { NA } })
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]
        aggregate_metric <- if(length(valid_metrics) > 0) mean(valid_metrics) else NA
        list(params = current_params, aggregate_metric = aggregate_metric, num_valid_pairs = length(valid_metrics))
    }
    stopCluster(cl); cat("\nOptimization loop finished.\n")
    valid_results <- Filter(function(x) !inherits(x, "error") && is.list(x) && !is.null(x$aggregate_metric) && is.finite(x$aggregate_metric), optimization_results_list)
    if(length(valid_results) > 0) {
        optimization_results_df <- purrr::map_dfr(valid_results, ~bind_cols(as_tibble(.x$params), tibble(aggregate_metric = .x$aggregate_metric, num_valid_pairs = .x$num_valid_pairs)))
        best_result_row <- optimization_results_df %>% arrange(desc(aggregate_metric)) %>% head(1)
        if(nrow(best_result_row) > 0 && is.finite(best_result_row$aggregate_metric)) {
            best_params <- list( roll_window = best_result_row$roll_window, entry_z = best_result_row$entry_z, exit_z = best_result_row$exit_z, max_scalar = best_result_row$max_scalar, scale_factor = best_result_row$scale_factor)
            cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n"); print(bind_rows(best_params)); cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")
            ROLLING_WINDOW <- best_params$roll_window; ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z; ZSCORE_EXIT_THRESHOLD <- best_params$exit_z; MAX_POSITION_SCALAR <- best_params$max_scalar; SCALING_FACTOR <- best_params$scale_factor
        } else { cat("WARNING: Optimization yielded non-finite best metric. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }
    } else { cat("WARNING: Optimization failed. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }
} else { cat("Skipping optimization. Using INITIAL parameters.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }


# --- 5. Backtesting Period (with Regime Filter) ---
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW, " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD, "\n")
if(REGIME_FILTER_ENABLED) cat("Regime Filter: Enabled (BTC Vol > ~", round(regime_vol_threshold_value, 6), " AND AdrMom < ~", round(regime_adr_mom_threshold_value, 4) ,")\n") else cat("Regime Filter: Disabled\n")

# Get data including buffer for rolling calcs
backtest_data_full_period <- all_data_calculated %>%
    filter(symbol %in% c(btc_symbol, potentially_mean_reverting_pairs)) %>%
    filter(date >= BACKTEST_START_DATE - days(max(ROLLING_WINDOW, REGIME_VOL_WINDOW, REGIME_ADR_WINDOW*2))) # Ensure enough buffer

# Pre-calculate regime state for the full backtest period + buffer
if(REGIME_FILTER_ENABLED) {
    # Use previously calculated rolling_vol and adr_act_mom columns
    btc_regime_data_backtest <- backtest_data_full_period %>%
        filter(symbol == btc_symbol) %>%
        mutate(
             is_high_vol = rolling_vol > regime_vol_threshold_value,
             is_low_adr_mom = adr_act_mom < regime_adr_mom_threshold_value,
             market_regime = ifelse(!is.na(is_high_vol) & !is.na(is_low_adr_mom) & is_high_vol & is_low_adr_mom, "High Risk", "Normal")
        ) %>% select(date, market_regime) # Add rolling_vol, adr_act_mom here if needed for plotting
    # Join regime state back to the main data
    backtest_data_full_period <- left_join(backtest_data_full_period, btc_regime_data_backtest, by = "date") %>%
                                 mutate(market_regime = ifelse(is.na(market_regime), "Normal", market_regime)) # Fill NAs
} else {
     backtest_data_full_period <- backtest_data_full_period %>% mutate(market_regime = "Normal") # Dummy column
}


backtest_details <- list() # Store results
for (candidate_symbol in potentially_mean_reverting_pairs) {
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    # Filter data for the pair for the full period needed
    btc_data_backtest <- backtest_data_full_period %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, market_regime)
    candidate_data_backtest <- backtest_data_full_period %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) { cat(" Insufficient overlap. Skipping.\n"); next }

    # Calculate Spread, Z-Score, Signals, Positions (using selected parameters & regime filter)
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        filter(date >= BACKTEST_START_DATE, !is.na(zscore)) # Filter for backtest dates AFTER rolling calc

     if(nrow(pair_data_signals) < 2) { cat(" Insufficient data post Z-score. Skipping.\n"); next }

     pair_data_trades <- pair_data_signals %>%
         mutate(
            # Raw signal based on Z-score
            raw_signal = case_when(zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, TRUE ~ NA_real_),
            raw_signal = zoo::na.locf(raw_signal, na.rm = FALSE, fromLast=FALSE), # Carry forward signal if between bands
            raw_signal = ifelse(is.na(raw_signal), 0, raw_signal),
            prev_raw_signal = lag(raw_signal, default = 0), # Need previous raw signal to check for entry attempt

            # Apply Regime Filter: Prevent *new* entries during High Risk
            current_signal = ifelse(prev_raw_signal == 0 & raw_signal != 0 & market_regime == "High Risk", 0, raw_signal), # Override entry if high risk

            # Position based on previous day's *filtered* signal
            prev_signal = lag(current_signal, default = 0),
            prev_zscore = lag(zscore, default = 0),
            prev_prev_signal = lag(prev_signal, default=0), # Need signal from t-2 to detect *actual* entry

            # Size scalar based on Z-score *at entry* (when position actually taken)
            size_scalar = case_when(
                 prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                 prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                 prev_signal != 0 & prev_signal == prev_prev_signal ~ lag(pmax(1.0, # Carry forward entry scalar if holding
                       case_when( lag(prev_signal, default=0) == -1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (lag(prev_zscore,default=0) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                  lag(prev_signal, default=0) == 1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (abs(lag(prev_zscore,default=0)) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                  TRUE ~ 1.0 )), default=1.0), # Default scalar if needed
                 TRUE ~ 1.0 # Default if flat or exiting
            ),
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)), # Ensure scalar is 0 if flat, >=1 if holding
            position = prev_signal * size_scalar
        ) %>% mutate(position = as.numeric(position))

    # Calculate Strategy Returns
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate( candidate_return = as.numeric(candidate_return), btc_return = as.numeric(btc_return),
                strategy_return = position * (candidate_return - btc_return),
                strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0),
                symbol = candidate_symbol) %>%
        select(date, symbol, strategy_return)

    # Store results
    backtest_details[[candidate_symbol]] <- list(trade_data = pair_data_trades, returns_data = pair_strategy_returns_tibble)
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
} # End backtesting loop


# --- 6. Performance Metrics & Drawdown Calculation (No Omega) ---
# (Code unchanged - Uses the safe metric calculation function)
cat("\n--- Step 6: Calculating Performance Metrics ---\n")
if (length(backtest_details) == 0) stop("No pairs successfully backtested.")
# calculate_safe_metrics function (already defined and corrected)
# ...
# --- 6a. Aggregate Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>% group_by(date) %>% summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>% arrange(date) %>% mutate(portfolio_return = ifelse(is.na(portfolio_return), 0, portfolio_return))
if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts_agg <- xts::xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts_agg) <- "Aggregate Strategy"
    cat("\n--- AGGREGATE Backtest Performance Metrics ---\n")
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts_agg, trade_data = NULL)
    cat("Annualized Return:", scales::percent(agg_metrics$AnnReturn, accuracy=0.01), "\n")
    cat("Annualized Std Dev:", scales::percent(agg_metrics$AnnStdDev, accuracy=0.01), "\n")
    cat("Cumulative Return:", scales::percent(agg_metrics$CumReturn, accuracy=0.01), "\n")
    cat("Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n")
    cat("Maximum Drawdown:", scales::percent(agg_metrics$MaxDD, accuracy=0.01), "\n")
    cat("Calmar Ratio:", round(agg_metrics$Calmar, 3), "\n")
} else { cat("\nError: Aggregate returns empty.\n"); stop("Cannot calculate aggregate metrics.") }
# --- 6b. Individual Pair Performance & Drawdowns ---
cat("\n--- INDIVIDUAL Pair Backtest Performance & Drawdowns ---\n")
individual_metrics_list <- list(); individual_drawdown_tables <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n")
    pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data
    pair_trade_data <- backtest_details[[candidate_symbol]]$trade_data
    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_tibble <- pair_returns_tibble %>% mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return))
        pair_returns_xts <- xts::xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date)
        colnames(pair_returns_xts) <- candidate_symbol
        pair_metrics <- calculate_safe_metrics(pair_returns_xts, pair_trade_data)
        individual_metrics_list[[candidate_symbol]] <- pair_metrics
        cat(" Annualized Return:", scales::percent(pair_metrics$AnnReturn, accuracy=0.01), "\n")
        cat(" Annualized Std Dev:", scales::percent(pair_metrics$AnnStdDev, accuracy=0.01), "\n")
        cat(" Cumulative Return:", scales::percent(pair_metrics$CumReturn, accuracy=0.01), "\n")
        cat(" Win % (of active days):", scales::percent(pair_metrics$WinPerc, accuracy=0.1), "\n")
        cat(" % Time In Market:", scales::percent(pair_metrics$PctInMarket, accuracy=0.1), "\n")
        cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n")
        cat(" Maximum Drawdown:", scales::percent(pair_metrics$MaxDD, accuracy=0.01), "\n")
        cat(" Calmar Ratio:", round(pair_metrics$Calmar, 3), "\n")
        dd_table <- tryCatch(PerformanceAnalytics::table.Drawdowns(pair_returns_xts, top=5), error=function(e) NULL)
        if(!is.null(dd_table)) { cat(" Top 5 Drawdowns:\n"); print(dd_table); individual_drawdown_tables[[candidate_symbol]] <- dd_table } else { cat(" Could not calculate drawdown table.\n") }
        cat("\n")
    } else { cat(" Insufficient return data for", candidate_symbol, "\n\n") }
}
# --- 6c. Create Summary Table ---
summary_df <- tryCatch({ bind_rows(lapply(individual_metrics_list, function(l) as.data.frame(l)), .id = "Candidate") %>% mutate(Pair = paste("BTC", Candidate, sep="-")) %>% select(Pair, AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn) %>% arrange(desc(Sharpe)) }, error = function(e) { cat("Error creating summary table:", e$message, "\n"); NULL })
if (!is.null(summary_df)){ cat("\n--- Performance Summary Table ---\n"); print(summary_df, digits=3) }


# --- 7. Plotting (Revised Strategy Plot & Regime) ---
# (Code unchanged, plots reflect updated backtest including regime filter effect)
cat("\n--- Step 7: Plotting Results ---\n")
plot_dir <- "strategy_plots"; if (!dir.exists(plot_dir)) dir.create(plot_dir)
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")
agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png"); png(agg_plot_file, width=800, height=1000); tryCatch({ PerformanceAnalytics::charts.PerformanceSummary(portfolio_returns_xts_agg, main = "Aggregate Strategy Performance", geometric = FALSE)}, error=function(e) plot.new()); dev.off()
btc_prices_backtest <- all_data_calculated %>% filter(symbol == btc_symbol, date >= BACKTEST_START_DATE) %>% select(date, btc_price = price)
for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n")
    pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data
    pair_returns_data <- backtest_details[[candidate_symbol]]$returns_data
    # Plot 1: Spread + Z-Score + Entries + Regime Background
    trade_entries <- pair_plot_data %>% filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>% mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread"))
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) + geom_line(color="black") + labs(title = paste("BTC vs", candidate_symbol, ": Log Spread"), x = NULL, y = "Log Spread") + theme_minimal() + theme(plot.title = element_text(size=10))
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) +
        geom_rect(data = . %>% filter(market_regime == "High Risk"), aes(xmin=date, xmax=lead(date, default=max(date)+days(1)), ymin=-Inf, ymax=Inf), fill="grey80", alpha=0.3, inherit.aes = FALSE) +
        geom_line(color="black") +
        geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") + geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") + geom_hline(yintercept = 0, color = "grey") +
        geom_point(data = trade_entries, aes(color = entry_type), size = 2) + scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"), x = "Date", y = "Z-Score", color = NULL) + theme_minimal() + theme(legend.position = "bottom", plot.title = element_text(size=10))
    pair_plot_file_spread <- file.path(plot_dir, paste0("01_spread_z_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_spread, gridExtra::grid.arrange(plot_spread, plot_zscore, ncol = 1, heights = c(1, 2)), width=10, height=6); cat(" Spread/Z plot saved:", pair_plot_file_spread, "\n") }, error = function(e){cat(" Could not save spread/z plot:", e$message, "\n")})
    # Plot 2: Strategy Results Plot
    plot_data_strategy <- pair_plot_data %>% select(date, position, market_regime) %>% inner_join(pair_returns_data, by="date") %>% inner_join(btc_prices_backtest, by="date") %>%
        mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return), CumEQ = STARTING_BALANCE * cumprod(1 + strategy_return),
               trade_signal = case_when( sign(position) != 0 & lag(sign(position), default=0) == 0 ~ sign(position), sign(position) == 0 & lag(sign(position), default=0) != 0 ~ -lag(sign(position)), TRUE ~ 0 ),
               pos_start = date, pos_end = lead(date, default = END_DATE + days(1)), position_plot = ifelse(abs(position) < SD_OFFSET, NA, position))
    p_price <- ggplot(plot_data_strategy, aes(x=date, y=btc_price)) + geom_line() + labs(y="BTC Price", x=NULL) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + ggtitle(paste("Strategy Results: BTC vs", candidate_symbol))
    p_trades <- ggplot(plot_data_strategy %>% filter(trade_signal != 0), aes(x=date, y=0)) + geom_point(aes(shape=factor(sign(trade_signal)), color=factor(sign(trade_signal))), size=2.0) + scale_shape_manual(values=c("1"=17, "-1"=17), guide="none") + scale_color_manual(values=c("1"="darkgreen", "-1"="darkred"), guide="none") + labs(y="Trades", x=NULL) + ylim(-1, 1) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
    p_pos <- ggplot(plot_data_strategy %>% filter(!is.na(position_plot))) + geom_rect(aes(xmin = pos_start, xmax = pos_end, ymin = 0, ymax = position_plot, fill = factor(sign(position_plot))), alpha=0.8) + geom_hline(yintercept=0) + scale_fill_manual(values=c("1"="blue", "-1"="lightblue"), guide="none") + labs(y="Position", x=NULL) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + ylim(-MAX_POSITION_SCALAR*1.1, MAX_POSITION_SCALAR*1.1)
    p_cumeq <- ggplot(plot_data_strategy, aes(x=date, y=CumEQ)) + geom_line(color="blue") + labs(y="Equity", x="Date") + theme_minimal(base_size = 9) + scale_y_continuous(labels = scales::comma)
    pair_plot_file_strategy <- file.path(plot_dir, paste0("02_strategy_results_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_strategy, gridExtra::grid.arrange(p_price, p_trades, p_pos, p_cumeq, ncol=1, heights=c(3, 0.5, 1.5, 2)), width=8, height=6); cat(" Strategy results plot saved:", pair_plot_file_strategy, "\n") }, error=function(e){cat(" Could not save strategy plot:", e$message, "\n")})
}


# --- 8. Notes on Bitcoin Fundamental Value Strategy ---
# (Code unchanged)
cat("\n--- Step 8: Notes on Bitcoin Fundamental Value Strategy --- \n")#...rest of notes

# --- 9. Notes on Dynamic Portfolio Rebalancing ---
# (Code unchanged)
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n") #...rest of notes

cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")

```

# **Test10: (look at this one)**

```{r}
# --- 0. Setup: Install and Load Libraries ---
# ... (libraries remain the same) ...
library(tidyverse); library(lmtest); library(car); library(tseries); library(pracma)
library(lubridate); library(httr); library(jsonlite); library(coinmetrics); library(slider)
library(PerformanceAnalytics); library(gridExtra); library(doParallel); library(foreach); library(scales) ; library(crypto2)

# --- 1. Parameters ---
# Data & Timeframe
N_TOP_COINS <- 100
HISTORY_YEARS <- 6
BACKTEST_YEARS <- 2
START_DATE <- Sys.Date() - years(HISTORY_YEARS)
END_DATE <- Sys.Date()
BACKTEST_START_DATE <- END_DATE - years(BACKTEST_YEARS)
TRAINING_END_DATE <- BACKTEST_START_DATE - days(1)

# Filtering
STABLECOINS <- c("USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "GUSD", "PAXG", "XAUT")

# --- Explicit List for Assets NOT Supported by CoinMetrics API ---
# Add symbols (lowercase) known to cause issues with the CoinMetrics API fetch here
CM_UNSUPPORTED_ASSETS <- c("sol", "steth", "ton", "leo", "avax", "sui", "wsteth", "usds", "pi", "weth", "usde", "bsc-usd", "hype", "weeth", "wbt", "susds", "cbtc", "susde", "buidl", "lbtc", "kas", "pol", "s", "ftn", "arb", "ip", "solvbtc", "rseth", "weth", "move", "bnsol", "wld", "dexe", "bonk", "sei", "usdt0", "reth", "usd0", "bera", "usdc.e", "flr", "solvbtc.bbn", "pyusd", "wbnb",
                           "om", "inj") # <<< Added om, inj
# ---

# CoinMetrics API Key - Set as environment variable CM_API_KEY
# The R client primarily uses the environment variable.
CM_API_KEY <- Sys.getenv("CM_API_KEY")
if (CM_API_KEY == "") {
  warning("CoinMetrics API Key (CM_API_KEY) is blank. Using free tier access - rate limits and metric availability will be restricted.", immediate. = TRUE)
} else {
  cat("Attempting to use CoinMetrics API Key found in environment variable CM_API_KEY.\n")
}

# Analysis Thresholds
VIF_THRESHOLD <- 5
ADF_P_VALUE_THRESHOLD <- 0.05
HURST_THRESHOLD <- 0.5

# Trading Strategy Parameters (Initial - Will be Optimized)
INITIAL_ROLLING_WINDOW <- 30
INITIAL_ZSCORE_ENTRY <- 1.5
INITIAL_ZSCORE_EXIT <- 0.5
SD_OFFSET <- 1e-9

# Position Sizing Parameters (Initial)
INITIAL_MAX_POSITION_SCALAR <- 2.0
INITIAL_SCALING_FACTOR <- 0.5

# Regime Filter Parameters
REGIME_FILTER_ENABLED <- TRUE
REGIME_VOL_WINDOW <- 60
REGIME_ADR_WINDOW <- 30
REGIME_VOL_PERCENTILE <- 0.75
REGIME_ADR_PERCENTILE <- 0.25

# Optimization Parameters
OPTIMIZE_PARAMETERS <- TRUE
OPTIMIZATION_METRIC <- "Sharpe" # ("Sharpe", "Calmar")
STARTING_BALANCE <- 500000

# Parallel Processing Setup
N_CORES <- detectCores() - 1
if (N_CORES < 1) N_CORES <- 1

# Cache Parameters
CACHE_DIR <- "data_cache"
MAX_CACHE_AGE_DAYS <- 1
FORCE_REFRESH <- FALSE
if (!dir.exists(CACHE_DIR)) { dir.create(CACHE_DIR, showWarnings = FALSE, recursive = TRUE); cat("Cache directory created:", file.path(getwd(), CACHE_DIR), "\n") }

# --- Moved Function Definition: calculate_safe_metrics ---
calculate_safe_metrics <- function(returns_xts, trade_data, scale = 252, Rf = 0) {
    metrics <- list( AnnReturn = NA_real_, AnnStdDev = NA_real_, Sharpe = NA_real_, MaxDD = NA_real_, Calmar = NA_real_, WinPerc = NA_real_, PctInMarket= NA_real_, CumReturn = NA_real_ )
    if (is.null(returns_xts) || nrow(returns_xts) < 20) {
        # cat(" Warning: Not enough returns data (<20) to calculate metrics reliably.\n") # Reduced verbosity
        tryCatch({ metrics$CumReturn <- as.numeric(PerformanceAnalytics::Return.cumulative(returns_xts, geometric=FALSE)) }, error = function(e) {})
        return(metrics)
    }
    sd_val <- sd(coredata(returns_xts), na.rm = TRUE); # cat("  Std Dev Check:", sd_val, "\n") # Debug SD value
    tryCatch({ metrics$MaxDD <- as.numeric(PerformanceAnalytics::maxDrawdown(returns_xts)) }, error = function(e) {cat(" Error MaxDD:", e$message, "\n")})
    tryCatch({ tbl <- PerformanceAnalytics::table.AnnualizedReturns(returns_xts, scale = scale, geometric = FALSE); metrics$AnnReturn <- as.numeric(tbl[1, 1]); metrics$AnnStdDev <- as.numeric(tbl[2, 1]) }, error = function(e) {cat(" Error Ann Ret/Stdev:", e$message, "\n")})
    tryCatch({ metrics$CumReturn <- as.numeric(PerformanceAnalytics::Return.cumulative(returns_xts, geometric=FALSE)) }, error = function(e) {cat(" Error CumReturn:", e$message, "\n")})

    # Calculate trade-based metrics if trade_data is valid and aligns
    if (!is.null(trade_data) && "position" %in% names(trade_data) && nrow(trade_data) == nrow(returns_xts)){
         positions <- trade_data$position[!is.na(trade_data$position)]
         returns_coredata <- coredata(returns_xts)
         if(length(positions) == length(returns_coredata)) {
              returns_in_market <- returns_coredata[positions != 0]
              if(length(returns_in_market) > 0) metrics$WinPerc <- mean(returns_in_market > 0, na.rm=TRUE) else metrics$WinPerc <- NA_real_
              metrics$PctInMarket <- mean(positions != 0, na.rm=TRUE)
         } else {
              # cat(" Warning: Length mismatch returns/positions. Cannot calculate Win%/PctInMarket.\n") # Reduced verbosity
              metrics$WinPerc <- NA_real_; metrics$PctInMarket <- NA_real_
         }
    } else if (!is.null(trade_data)) { cat(" Warning: trade_data issue for Win%/PctInMarket.\n") }

    # Calculate metrics sensitive to standard deviation
    if (!is.na(sd_val) && sd_val > SD_OFFSET) {
        tryCatch({ shp <- PerformanceAnalytics::SharpeRatio.annualized(returns_xts, Rf = Rf, scale = scale, geometric = FALSE); metrics$Sharpe <- as.numeric(shp[1,]) }, error = function(e) {cat(" Error Sharpe:", e$message, "\n")})
        tryCatch({ metrics$Calmar <- as.numeric(PerformanceAnalytics::CalmarRatio(returns_xts, scale = scale)) }, error = function(e) {cat(" Error Calmar:", e$message, "\n")})
    } else {
        # cat(" Warning: Zero SD. Sharpe, Calmar set to NA.\n"); # Reduced verbosity
        metrics$Sharpe <- NA_real_
        if(!is.na(metrics$MaxDD) && !is.null(metrics$MaxDD) && metrics$MaxDD != 0 && !is.na(metrics$AnnReturn) && !is.null(metrics$AnnReturn)) {
           tryCatch({ metrics$Calmar <- metrics$AnnReturn / abs(metrics$MaxDD) }, error=function(e){ metrics$Calmar <- NA_real_})
        } else { metrics$Calmar <- NA_real_ }
    }
    return(metrics)
}
# --- END Function Definition ---


# --- 2. Data Acquisition Part 1: Basic Price Data (CryptoCompare via httr) ---
cat("--- Step 2: Acquiring Basic Price Data ---\n")
# --- Caching for CoinGecko Ranking ---
cg_cache_file <- file.path(CACHE_DIR, paste0("coingecko_ranking_cache_N", N_TOP_COINS, ".rds"))
top_coins_df <- NULL
if (!FORCE_REFRESH && file.exists(cg_cache_file)) { cat("Loading CoinGecko ranking from cache:", cg_cache_file, "\n"); top_coins_df <- tryCatch(readRDS(cg_cache_file), error = function(e) { cat(" Error loading CG cache:", e$message, "\n"); NULL }) }
if (is.null(top_coins_df)) {
    cat("Fetching coin list and market caps from CoinGecko...\n"); tryCatch({
        cg_per_page <- N_TOP_COINS + length(STABLECOINS) + 30; if (cg_per_page > 250) cg_per_page <- 250
        cg_url <- "https://api.coingecko.com/api/v3/coins/markets"; cg_params <- list(vs_currency = "usd", order = "market_cap_desc", per_page = cg_per_page, page = 1, sparkline = "false")
        cg_response <- httr::GET(cg_url, query = cg_params); Sys.sleep(2)
        if (httr::status_code(cg_response) == 200) {
            cg_data <- jsonlite::fromJSON(httr::content(cg_response, "text", encoding = "UTF-8")); top_coins_df <- cg_data %>% as_tibble() %>% select(symbol, id, market_cap) %>% mutate(symbol = toupper(symbol)); cat("Successfully fetched", nrow(top_coins_df), "coins from CoinGecko.\n")
            tryCatch(saveRDS(top_coins_df, file = cg_cache_file), error = function(e) { cat(" Error saving CG cache:", e$message, "\n")})
        } else { stop("Failed CoinGecko fetch. Status: ", httr::status_code(cg_response)) }
    }, error = function(e) { cat("Error CoinGecko:", e$message, "\n"); stop("Cannot proceed.") })
}
# --- Filter for initial candidates ---
cat("Filtering for initial prime candidates...\n"); prime_candidates_info <- top_coins_df %>% filter(!symbol %in% STABLECOINS, symbol != "BTC") %>% slice_head(n = N_TOP_COINS - 1)
initial_prime_candidate_symbols <- prime_candidates_info$symbol; btc_symbol <- "BTC"; initial_all_symbols_of_interest <- c(btc_symbol, initial_prime_candidate_symbols)
if (length(initial_prime_candidate_symbols) < (N_TOP_COINS - 10)) { warning("Fetched fewer candidates than expected: ", length(initial_prime_candidate_symbols)) }
cat("Initial", length(initial_prime_candidate_symbols), "candidates identified for basic fetch:", paste(initial_prime_candidate_symbols, collapse=", "), "\n")

# --- Caching for Basic Price Data ---
basic_data_cache_file <- file.path(CACHE_DIR, paste0("basic_price_data_N", N_TOP_COINS, "_", format(START_DATE),"_",format(END_DATE),".rds"))
all_data_list_basic <- list(); cache_valid_basic <- FALSE
if (!FORCE_REFRESH && file.exists(basic_data_cache_file)) {cache_age <- Sys.time() - file.info(basic_data_cache_file)$mtime; if (cache_age < duration(days = MAX_CACHE_AGE_DAYS)) { cat("Loading basic price data from cache:", basic_data_cache_file, "\n"); all_data_list_basic <- tryCatch(readRDS(basic_data_cache_file), error = function(e) { cat(" Error loading basic price cache:", e$message, "\n"); list() }); if(length(all_data_list_basic) > 0) cache_valid_basic <- TRUE else all_data_list_basic <- list() } else { cat("Basic price cache is outdated.\n") }}
if (!cache_valid_basic) {
    cat("Fetching", HISTORY_YEARS, "years historical PRICE data via httr/jsonlite for ~", length(initial_all_symbols_of_interest), "symbols...\n"); not_found_symbols_basic <- c(); base_url_cc <- "https://min-api.cryptocompare.com/data/v2/histoday"
    api_limit_cc <- as.numeric(END_DATE - START_DATE); if(api_limit_cc > 2000) api_limit_cc <- 2000
    Sys.setenv(TZ='UTC')
    for (sym in initial_all_symbols_of_interest) {
        cat("Fetching price data for:", sym, "..."); tryCatch({
            query_params <- list(fsym = sym, tsym = "USD", limit = api_limit_cc, toTs = as.numeric(as.POSIXct(END_DATE)))
            response <- httr::GET(base_url_cc, query = query_params); Sys.sleep(1.5)
            if (httr::status_code(response) == 200) {
                content <- httr::content(response, "text", encoding = "UTF-8"); json_data <- jsonlite::fromJSON(content)
                if (json_data$Response == "Success" && !is.null(json_data$Data$Data) && nrow(json_data$Data$Data) > 0) {
                    df <- json_data$Data$Data %>% as_tibble(); if ("time" %in% names(df) && "close" %in% names(df)) {
                        df_processed <- df %>% mutate(timestamp = lubridate::as_datetime(time, tz = "UTC"), date = as.Date(timestamp), symbol = sym) %>% select(date, symbol, price = close) %>% filter(date >= START_DATE & date <= END_DATE) %>% arrange(date)
                        if(nrow(df_processed) > 0) { all_data_list_basic[[sym]] <- df_processed; cat(" Success.\n") } else { cat(" No data in range. Skipping.\n"); not_found_symbols_basic <- c(not_found_symbols_basic, sym) }
                    } else { cat(" Missing cols. Skipping.\n"); not_found_symbols_basic <- c(not_found_symbols_basic, sym) }
                } else { cat(" API Error:", json_data$Message, ". Skipping.\n"); not_found_symbols_basic <- c(not_found_symbols_basic, sym) }
            } else { cat(" HTTP Error:", httr::status_code(response), ". Skipping.\n"); not_found_symbols_basic <- c(not_found_symbols_basic, sym) }
        }, error = function(e) { cat(" Error:", e$message, ". Skipping.\n"); not_found_symbols_basic <- c(not_found_symbols_basic, sym); Sys.sleep(1.5) })
    }
    if (length(all_data_list_basic) > 0 && ("BTC" %in% names(all_data_list_basic))) { tryCatch(saveRDS(all_data_list_basic, file = basic_data_cache_file), error = function(e) { cat(" Error saving basic cache:", e$message, "\n")}); cat("Saved basic data to cache:", basic_data_cache_file, "\n") } else { cat("WARNING: Failed fetch sufficient basic data. Cache not saved.\n"); if(!("BTC" %in% names(all_data_list_basic))) stop("Failed fetch BTC basic data.") }
}
# --- Update lists based on fetch/load ---
successfully_fetched_basic <- names(all_data_list_basic); initial_prime_candidate_symbols <- intersect(initial_prime_candidate_symbols, successfully_fetched_basic)
if (!("BTC" %in% successfully_fetched_basic)) stop("BTC basic data missing."); if (length(initial_prime_candidate_symbols) == 0) stop("No candidate basic data.")
cat("Using basic price data for BTC and", length(initial_prime_candidate_symbols), "candidates.\n"); all_data_long_basic <- dplyr::bind_rows(all_data_list_basic) %>% arrange(symbol, date)


# --- 3. Basic Data Preparation ---
cat("--- Step 3: Calculating Returns from Basic Data ---\n")
all_data_calculated_basic <- all_data_long_basic %>%
    mutate(price = as.numeric(price)) %>% group_by(symbol) %>% arrange(date) %>%
    mutate(return = log(price) - log(lag(price))) %>%
    ungroup() %>% filter(!is.na(return))


# --- 4. Training Period Analysis (Identify Potential Pairs using Basic Data) ---
cat("--- Step 4: Analyzing Training Period for Pair Identification (", format(START_DATE), "to", format(TRAINING_END_DATE), ") ---\n")
training_data_basic <- all_data_calculated_basic %>% filter(date <= TRAINING_END_DATE)
pairs_trading_analysis_train <- list(); potentially_mean_reverting_pairs <- c(); training_pair_data_list_basic <- list()
for (candidate_symbol in initial_prime_candidate_symbols) {
    cat("Analyzing pair: BTC vs", candidate_symbol, " (Training - Pair ID)\n"); btc_data_train <- training_data_basic %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return); candidate_data_train <- training_data_basic %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return); pair_data_train <- inner_join(btc_data_train, candidate_data_train, by = "date") %>% arrange(date)
    if(nrow(pair_data_train) < INITIAL_ROLLING_WINDOW + 60) { cat(" Insufficient overlap. Skipping pair.\n"); next }
    training_pair_data_list_basic[[candidate_symbol]] <- pair_data_train
    pair_data_train_spread <- pair_data_train %>% filter(candidate_price > 0, btc_price > 0) %>% mutate(spread = log(candidate_price) - log(btc_price))
    if(nrow(pair_data_train_spread) > INITIAL_ROLLING_WINDOW) {
        spread_vector_train <- na.omit(pair_data_train_spread$spread); adf_result_train <- tryCatch(tseries::adf.test(spread_vector_train), error = function(e) NULL); adf_p_value_train <- if (!is.null(adf_result_train)) adf_result_train$p.value else NA; hurst_result_train <- tryCatch(pracma::hurstexp(spread_vector_train, d = INITIAL_ROLLING_WINDOW, display=FALSE), error = function(e) NULL); hurst_value_train <- if (!is.null(hurst_result_train)) hurst_result_train$He else NA
        is_mean_reverting_train <- (!is.na(adf_p_value_train) && adf_p_value_train < ADF_P_VALUE_THRESHOLD) || (!is.na(hurst_value_train) && hurst_value_train < HURST_THRESHOLD)
        pairs_trading_analysis_train[[candidate_symbol]] <- list(adf_p_value = adf_p_value_train, hurst_exponent = hurst_value_train, is_potentially_mean_reverting = is_mean_reverting_train)
        cat("  Spread Analysis (Train): ADF p-val:", round(adf_p_value_train, 4), "| Hurst:", round(hurst_value_train, 3), "\n")
        if(is_mean_reverting_train) { cat("  >>> Pair BTC-", candidate_symbol, "identified potentially mean-reverting. <<<\n"); potentially_mean_reverting_pairs <- c(potentially_mean_reverting_pairs, candidate_symbol) }
    } else { cat("  Insufficient spread data for tests.\n") }
    cat(" --- End analysis for", candidate_symbol, " ---\n")
}
cat("--- Training Period Pair Identification Complete ---\n")
cat("Potentially mean-reverting pairs identified:", paste(potentially_mean_reverting_pairs, collapse=", "), "\n")
if (length(potentially_mean_reverting_pairs) == 0) stop("No potentially mean-reverting pairs identified.")


# --- 4.1 Data Acquisition Part 2: Indicator Data (CoinMetrics) ---
cat("\n--- Step 4.1: Acquiring Indicator Data for Potential Pairs from CoinMetrics ---\n")
# --- Filter potential pairs based on CM_UNSUPPORTED_ASSETS list ---
symbols_to_fetch_indicators_cm <- setdiff(tolower(c(btc_symbol, potentially_mean_reverting_pairs)), CM_UNSUPPORTED_ASSETS) # Lowercase for fetch
if(length(symbols_to_fetch_indicators_cm) <= 1 && "btc" %in% symbols_to_fetch_indicators_cm) { cat("WARNING: All potential pairs unsupported by CM. Cannot fetch indicators.\n"); symbols_for_indicators <- character(0)
} else if (!("btc" %in% symbols_to_fetch_indicators_cm)) { cat("WARNING: BTC unsupported by CM? Cannot fetch indicators.\n"); symbols_for_indicators <- character(0)
} else { symbols_for_indicators <- toupper(symbols_to_fetch_indicators_cm) } # UPPERCASE list for internal use
# --- End Filter ---

metrics_indicators_fetch <- c("AdrActCnt", "TxCnt")
indicator_data_long <- NULL

# --- Caching for Indicator Data ---
indicator_cache_file <- file.path(CACHE_DIR, paste0("indicator_data_N", N_TOP_COINS, "_", format(START_DATE),"_",format(END_DATE),".rds"))
cache_valid_indicator <- FALSE
if (!FORCE_REFRESH && file.exists(indicator_cache_file)) {cache_age <- Sys.time() - file.info(indicator_cache_file)$mtime; if (cache_age < duration(days = MAX_CACHE_AGE_DAYS)) { cat("Loading indicator data from cache:", indicator_cache_file, "\n"); indicator_data_long <- tryCatch(readRDS(indicator_cache_file), error = function(e) { cat(" Error loading indicator cache:", e$message, "\n"); NULL }); if(!is.null(indicator_data_long)) cache_valid_indicator <- TRUE else indicator_data_long <- NULL } else { cat("Indicator cache is outdated.\n") }}

if (!cache_valid_indicator) {
    if (length(symbols_to_fetch_indicators_cm) > 0) { # Only fetch if symbols remain
        cat("Fetching indicators for:", paste(symbols_for_indicators, collapse=", "), "\n") # Print UPPERCASE
        cat("Metrics:", paste(metrics_indicators_fetch, collapse=", "), "\n")
        indicator_data_cm <- tryCatch({
            # --- CORRECTED: Removed api_key argument ---
            coinmetrics::get_asset_metrics(
                 assets = symbols_to_fetch_indicators_cm, # Pass lowercase list
                 metrics = metrics_indicators_fetch,
                 start_time = format(START_DATE), end_time = format(END_DATE), frequency = "1d",
                 paging_from = "start"
            )
            # --- END CORRECTION ---
        }, error = function(e) { cat("Error fetching indicator data:", e$message, "\n"); NULL })

        if (!is.null(indicator_data_cm) && nrow(indicator_data_cm) > 0) {
            cat("Processing indicator data...\n")
            indicator_data_long <- indicator_data_cm %>%
  mutate(date = as.Date(time)) %>%
  select(-time) %>%
  mutate(symbol = toupper(asset)) %>%
  select(date, symbol, everything(), -asset) %>%
  rename(
    adr_act_cnt = AdrActCnt,
    tx_cnt = TxCnt
  ) %>%
  mutate(across(-c(date, symbol), ~ suppressWarnings(as.numeric(.)))) %>%
  arrange(symbol, date)
            tryCatch(saveRDS(indicator_data_long, file = indicator_cache_file), error = function(e) { cat(" Error saving indicator cache:", e$message, "\n")}); cat("Saved indicator data to cache:", indicator_cache_file, "\n")
        } else { cat("WARNING: No indicator data fetched from CoinMetrics.\n") }
    } else {
         cat("Skipping CoinMetrics indicator fetch as no supported pairs were identified.\n")
    }
}
# --- Verify fetched indicators & disable regime if needed ---
fetched_indicators <- if(!is.null(indicator_data_long)) setdiff(names(indicator_data_long), c("date", "symbol")) else character(0)
if (length(fetched_indicators) > 0) cat("Indicators available:", paste(fetched_indicators, collapse=", "), "\n") else cat("No indicators available.\n")
if (REGIME_FILTER_ENABLED && (!("adr_act_cnt" %in% fetched_indicators) || !("BTC" %in% symbols_for_indicators) ) ) { # Check if BTC indicators were fetched
   cat("WARNING: Required indicators/BTC data missing for regime filter. Disabling filter.\n"); REGIME_FILTER_ENABLED <- FALSE
}


# --- 4.2 Combine Basic Data with Indicator Data ---
cat("\n--- Step 4.2: Combining Basic and Indicator Data ---\n")
if (!is.null(indicator_data_long)) { all_data_combined <- left_join(all_data_calculated_basic, indicator_data_long, by = c("date", "symbol"))
} else { all_data_combined <- all_data_calculated_basic %>% mutate(adr_act_cnt = NA_real_, tx_cnt=NA_real_); if (REGIME_FILTER_ENABLED) { cat("Disabling Regime filter.\n"); REGIME_FILTER_ENABLED <- FALSE }}


# --- 4.3 Calculate Rolling Indicators & Refine Pairs ---
cat("\n--- Step 4.3: Calculating Rolling Indicators & Refining Pairs ---\n")
all_data_combined <- all_data_combined %>% group_by(symbol) %>% arrange(date) %>% mutate( rolling_vol = slider::slide_dbl(return, ~sd(.x, na.rm = TRUE), .before = REGIME_VOL_WINDOW - 1, .complete = TRUE), adr_act_smooth = slider::slide_dbl(adr_act_cnt, ~mean(.x, na.rm=TRUE), .before = REGIME_ADR_WINDOW - 1, .complete = TRUE), adr_act_mom = adr_act_smooth / lag(adr_act_smooth, REGIME_ADR_WINDOW) - 1 ) %>% ungroup()
pairs_for_backtesting <- c(); training_pair_data_list_combined <- list(); regime_vol_threshold_value <- NA; regime_adr_mom_threshold_value <- NA
if (REGIME_FILTER_ENABLED) {
     training_data_combined <- all_data_combined %>% filter(date <= TRAINING_END_DATE)
     for (sym in potentially_mean_reverting_pairs) {
          # Check if pair has indicator data AND BTC has indicator data
          btc_check <- training_data_combined %>% filter(symbol=="BTC") %>% summarise(vol_ok = sum(!is.na(rolling_vol)) > REGIME_VOL_WINDOW, adr_ok = sum(!is.na(adr_act_mom)) > REGIME_ADR_WINDOW)
          pair_check <- training_data_combined %>% filter(symbol==sym) %>% summarise(adr_present = "adr_act_cnt" %in% names(.)) # Check if adr_act_cnt column exists for this pair
          if(nrow(pair_check) > 0 && pair_check$adr_present && btc_check$vol_ok && btc_check$adr_ok) {
              btc_data_train_comb <- training_data_combined %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, rolling_vol, adr_act_mom)
              candidate_data_train_comb <- training_data_combined %>% filter(symbol == sym) %>% select(date, candidate_price = price, candidate_return = return)
              joined_data <- inner_join(btc_data_train_comb, candidate_data_train_comb, by="date") %>% arrange(date)
              if (nrow(joined_data) > INITIAL_ROLLING_WINDOW + 60) { training_pair_data_list_combined[[sym]] <- joined_data; pairs_for_backtesting <- c(pairs_for_backtesting, sym) # Add to list if data sufficient
              } else {cat(" Skipping pair", sym, "- insufficient overlap after joining combined data.\n")}
          } else { cat(" Skipping pair", sym, "- missing required indicator data in training.\n") }
     }
     if (length(pairs_for_backtesting)==0) { cat("WARNING: No pairs remain after indicator check. Disabling Regime Filter.\n"); REGIME_FILTER_ENABLED <- FALSE; pairs_for_backtesting <- potentially_mean_reverting_pairs # Fallback: Use original pairs list
          # Re-populate training data list if falling back
          for (sym in pairs_for_backtesting) { if(sym %in% names(training_pair_data_list_basic)){ training_pair_data_list_combined[[sym]] <- training_pair_data_list_basic[[sym]] } }
     } else { # Proceed with calculating thresholds
          cat("Pairs with indicator data for regime filter:", paste(pairs_for_backtesting, collapse=", "), "\n"); cat("Calculating regime thresholds...\n")
          btc_regime_data_train <- training_data_combined %>% filter(symbol == btc_symbol)
          if(nrow(btc_regime_data_train) > max(REGIME_VOL_WINDOW, REGIME_ADR_WINDOW*2)) {
               regime_vol_threshold_value <- quantile(btc_regime_data_train$rolling_vol, probs = REGIME_VOL_PERCENTILE, na.rm = TRUE, type = 8)
               regime_adr_mom_threshold_value <- quantile(btc_regime_data_train$adr_act_mom, probs = REGIME_ADR_PERCENTILE, na.rm = TRUE, type = 8)
               cat(" Regime Thresholds (Training): Vol >", round(regime_vol_threshold_value, 6), " & AdrMom <", round(regime_adr_mom_threshold_value, 4), "\n")
               if(is.na(regime_vol_threshold_value) || is.na(regime_adr_mom_threshold_value)){ cat("WARNING: Threshold calc failed. Disabling filter.\n"); REGIME_FILTER_ENABLED <- FALSE }
          } else { cat("WARNING: Not enough data for thresholds. Disabling filter.\n"); REGIME_FILTER_ENABLED <- FALSE }
     }
} else {
     pairs_for_backtesting <- potentially_mean_reverting_pairs
     for (sym in pairs_for_backtesting) { if(sym %in% names(training_pair_data_list_basic)){ training_pair_data_list_combined[[sym]] <- training_pair_data_list_basic[[sym]] }}
     cat("Regime filter disabled. Using all potential pairs:", paste(pairs_for_backtesting, collapse=", "), "\n")
}
if (length(pairs_for_backtesting) == 0) stop("No pairs available for optimization/backtesting.")


# --- 4.5 Parameter Optimization ---
# (Code unchanged - runs using evaluate_params_on_train & training_pair_data_list_combined)
cat("\n--- Step 4.5: Optimizing Parameters on Training Data ---\n")
# ... (optimization loop) ...
param_grid <- expand.grid( roll_window = seq(20, 40, by = 10), entry_z = seq(1.5, 2.0, by = 0.25), exit_z = seq(0.25, 0.75, by = 0.25), max_scalar = INITIAL_MAX_POSITION_SCALAR, scale_factor = INITIAL_SCALING_FACTOR, stringsAsFactors = FALSE) %>% filter(exit_z < entry_z)
cat("Total parameter combinations to test:", nrow(param_grid), "\n")
if (OPTIMIZE_PARAMETERS) {
    cl <- makeCluster(N_CORES); registerDoParallel(cl); cat("\nStarting parallel optimization across", N_CORES, "cores...\n")
    optimization_results_list <- foreach(i = 1:nrow(param_grid), .packages = c("tidyverse", "slider", "PerformanceAnalytics", "xts"), .combine = 'list', .errorhandling = 'pass') %dopar% {
        current_params <- param_grid[i, ]; pair_metrics <- sapply(pairs_for_backtesting, function(sym) { if (sym %in% names(training_pair_data_list_combined)) { evaluate_params_on_train(sym, training_pair_data_list_combined[[sym]], current_params) } else { NA } })
        valid_metrics <- pair_metrics[!is.na(pair_metrics)]; aggregate_metric <- if(length(valid_metrics) > 0) mean(valid_metrics) else NA; list(params = current_params, aggregate_metric = aggregate_metric, num_valid_pairs = length(valid_metrics))
    }
    stopCluster(cl); cat("\nOptimization loop finished.\n"); valid_results <- Filter(function(x) !inherits(x, "error") && is.list(x) && !is.null(x$aggregate_metric) && is.finite(x$aggregate_metric), optimization_results_list)
    if(length(valid_results) > 0) {
        optimization_results_df <- purrr::map_dfr(valid_results, ~bind_cols(as_tibble(.x$params), tibble(aggregate_metric = .x$aggregate_metric, num_valid_pairs = .x$num_valid_pairs)))
        best_result_row <- optimization_results_df %>% arrange(desc(aggregate_metric)) %>% head(1)
        if(nrow(best_result_row) > 0 && is.finite(best_result_row$aggregate_metric)) {
            best_params <- list( roll_window = best_result_row$roll_window, entry_z = best_result_row$entry_z, exit_z = best_result_row$exit_z, max_scalar = best_result_row$max_scalar, scale_factor = best_result_row$scale_factor)
            cat("Best parameters found based on average training", OPTIMIZATION_METRIC, ":\n"); print(bind_rows(best_params)); cat("Best average metric value:", round(best_result_row$aggregate_metric, 4), "\n")
            ROLLING_WINDOW <- best_params$roll_window; ZSCORE_ENTRY_THRESHOLD <- best_params$entry_z; ZSCORE_EXIT_THRESHOLD <- best_params$exit_z; MAX_POSITION_SCALAR <- best_params$max_scalar; SCALING_FACTOR <- best_params$scale_factor
        } else { cat("WARNING: Opti non-finite best metric. Using INITIAL.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }
    } else { cat("WARNING: Optimization failed. Using INITIAL.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }
} else { cat("Skipping optimization. Using INITIAL.\n"); ROLLING_WINDOW <- INITIAL_ROLLING_WINDOW; ZSCORE_ENTRY_THRESHOLD <- INITIAL_ZSCORE_ENTRY; ZSCORE_EXIT_THRESHOLD <- INITIAL_ZSCORE_EXIT; MAX_POSITION_SCALAR <- INITIAL_MAX_POSITION_SCALAR; SCALING_FACTOR <- INITIAL_SCALING_FACTOR }


# --- 5. Backtesting Period (using Combined Data & Regime Filter) ---
cat("\n--- Step 5: Backtesting Period (", format(BACKTEST_START_DATE), "to", format(END_DATE), ") ---\n")
cat("Using Parameters: Window=", ROLLING_WINDOW, " EntryZ=", ZSCORE_ENTRY_THRESHOLD, " ExitZ=", ZSCORE_EXIT_THRESHOLD, "\n")
if(REGIME_FILTER_ENABLED) cat("Regime Filter: Enabled (Using calculated thresholds)\n") else cat("Regime Filter: Disabled\n")

# Get combined data including buffer for rolling calcs
backtest_data_full_period <- all_data_combined %>%
    filter(symbol %in% c(btc_symbol, pairs_for_backtesting)) %>%
    filter(date >= BACKTEST_START_DATE - days(max(ROLLING_WINDOW, REGIME_VOL_WINDOW, REGIME_ADR_WINDOW*2)))

# Add regime state for backtest period using TRAINING thresholds
if(REGIME_FILTER_ENABLED) {
    backtest_data_full_period <- backtest_data_full_period %>%
        mutate(
             is_high_vol = rolling_vol > regime_vol_threshold_value,
             is_low_adr_mom = adr_act_mom < regime_adr_mom_threshold_value,
             market_regime = ifelse(!is.na(is_high_vol) & !is.na(is_low_adr_mom) & is_high_vol & is_low_adr_mom, "High Risk", "Normal"),
             market_regime = ifelse(is.na(market_regime), "Normal", market_regime) # Fill NAs
        )
} else {
     backtest_data_full_period <- backtest_data_full_period %>% mutate(market_regime = "Normal") # Dummy column
}

backtest_details <- list() # Store results
for (candidate_symbol in pairs_for_backtesting) { # Use the final list
    cat("Backtesting pair: BTC vs", candidate_symbol, "\n")
    # Filter data for the pair for the full period needed
    btc_data_backtest <- backtest_data_full_period %>% filter(symbol == btc_symbol) %>% select(date, btc_price = price, btc_return = return, market_regime) # Keep regime
    candidate_data_backtest <- backtest_data_full_period %>% filter(symbol == candidate_symbol) %>% select(date, candidate_price = price, candidate_return = return)
    pair_data_backtest <- inner_join(btc_data_backtest, candidate_data_backtest, by = "date") %>% filter(btc_price > 0, candidate_price > 0) %>% arrange(date)

    if(nrow(pair_data_backtest) < ROLLING_WINDOW + 5) { cat(" Insufficient overlap. Skipping.\n"); next }

    # Calculate Spread, Z-Score, Signals, Positions (using regime filter)
    pair_data_signals <- pair_data_backtest %>%
        mutate(
            spread = log(candidate_price) - log(btc_price),
            rolling_mean = slider::slide_dbl(spread, mean, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = slider::slide_dbl(spread, sd, .before = ROLLING_WINDOW - 1, .complete = FALSE, na.rm = TRUE),
            rolling_sd = ifelse(is.na(rolling_sd) | rolling_sd < SD_OFFSET, NA, rolling_sd),
            zscore = ifelse(is.na(rolling_sd), NA, (spread - rolling_mean) / rolling_sd)
        ) %>%
        filter(date >= BACKTEST_START_DATE, !is.na(zscore))

     if(nrow(pair_data_signals) < 2) { cat(" Insufficient data post Z-score. Skipping.\n"); next }

     # Apply regime filter logic here
     pair_data_trades <- pair_data_signals %>%
         mutate(
            raw_signal = case_when(zscore > ZSCORE_ENTRY_THRESHOLD ~ -1, zscore < -ZSCORE_ENTRY_THRESHOLD ~ 1, abs(zscore) < ZSCORE_EXIT_THRESHOLD ~ 0, TRUE ~ NA_real_),
            raw_signal = zoo::na.locf(raw_signal, na.rm = FALSE, fromLast=FALSE), raw_signal = ifelse(is.na(raw_signal), 0, raw_signal),
            prev_raw_signal = lag(raw_signal, default = 0),
            # Apply Regime Filter: Prevent *new* entries during High Risk
            current_signal = ifelse(prev_raw_signal == 0 & raw_signal != 0 & market_regime == "High Risk", 0, raw_signal),
            # Position based on previous day's *filtered* signal
            prev_signal = lag(current_signal, default = 0),
            prev_zscore = lag(zscore, default = 0), prev_prev_signal = lag(prev_signal, default=0),
            # Position Sizing (same logic as before)
            size_scalar = case_when( prev_signal == -1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (prev_zscore - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                     prev_signal == 1 & prev_prev_signal == 0 ~ pmin(1 + SCALING_FACTOR * (abs(prev_zscore) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR),
                                     prev_signal != 0 & prev_signal == prev_prev_signal ~ lag(pmax(1.0, case_when( lag(prev_signal, default=0) == -1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (lag(prev_zscore,default=0) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR), lag(prev_signal, default=0) == 1 & lag(prev_prev_signal, default=0) == 0 ~ pmin(1 + SCALING_FACTOR * (abs(lag(prev_zscore,default=0)) - ZSCORE_ENTRY_THRESHOLD), MAX_POSITION_SCALAR), TRUE ~ 1.0 )), default=1.0),
                                     TRUE ~ 1.0),
            size_scalar = ifelse(prev_signal == 0, 0, pmax(1.0, size_scalar)),
            position = prev_signal * size_scalar
        ) %>% mutate(position = as.numeric(position))

    # Calculate Strategy Returns
    pair_strategy_returns_tibble <- pair_data_trades %>%
        mutate( candidate_return = as.numeric(candidate_return), btc_return = as.numeric(btc_return),
                strategy_return = position * (candidate_return - btc_return),
                strategy_return = ifelse(is.finite(strategy_return), strategy_return, 0),
                symbol = candidate_symbol) %>%
        select(date, symbol, strategy_return)

    backtest_details[[candidate_symbol]] <- list(trade_data = pair_data_trades, returns_data = pair_strategy_returns_tibble)
    cat(" Backtesting calculations complete for", candidate_symbol, "\n")
}


# --- 6. Performance Metrics & Drawdown Calculation (No Omega) ---
# (Code unchanged - Uses the safe metric calculation function)
cat("\n--- Step 6: Calculating Performance Metrics ---\n")
if (length(backtest_details) == 0) stop("No pairs successfully backtested.")
# calculate_safe_metrics function defined earlier
# ... (Aggregate and Individual calculations using calculate_safe_metrics) ...
# --- 6a. Aggregate Performance ---
all_strategy_returns_long <- bind_rows(lapply(backtest_details, `[[`, "returns_data"))
portfolio_daily_returns <- all_strategy_returns_long %>% group_by(date) %>% summarise(portfolio_return = mean(strategy_return, na.rm = TRUE), .groups = 'drop') %>% arrange(date) %>% mutate(portfolio_return = ifelse(is.na(portfolio_return), 0, portfolio_return))
if (nrow(portfolio_daily_returns) > 0) {
    portfolio_returns_xts_agg <- xts::xts(portfolio_daily_returns$portfolio_return, order.by = portfolio_daily_returns$date)
    colnames(portfolio_returns_xts_agg) <- "Aggregate Strategy"
    cat("\n--- AGGREGATE Backtest Performance Metrics ---\n")
    agg_metrics <- calculate_safe_metrics(portfolio_returns_xts_agg, trade_data = NULL)
    cat("Annualized Return:", scales::percent(agg_metrics$AnnReturn, accuracy=0.01), "\n"); cat("Annualized Std Dev:", scales::percent(agg_metrics$AnnStdDev, accuracy=0.01), "\n"); cat("Cumulative Return:", scales::percent(agg_metrics$CumReturn, accuracy=0.01), "\n"); cat("Annualized Sharpe (Rf=0%):", round(agg_metrics$Sharpe, 3), "\n"); cat("Maximum Drawdown:", scales::percent(agg_metrics$MaxDD, accuracy=0.01), "\n"); cat("Calmar Ratio:", round(agg_metrics$Calmar, 3), "\n")
} else { cat("\nError: Aggregate returns empty.\n"); stop("Cannot calculate aggregate metrics.") }
# --- 6b. Individual Pair Performance & Drawdowns ---
cat("\n--- INDIVIDUAL Pair Backtest Performance & Drawdowns ---\n")
individual_metrics_list <- list(); individual_drawdown_tables <- list()
for (candidate_symbol in names(backtest_details)) {
    cat("--- Pair: BTC vs", candidate_symbol, "---\n"); pair_returns_tibble <- backtest_details[[candidate_symbol]]$returns_data; pair_trade_data <- backtest_details[[candidate_symbol]]$trade_data
    if (nrow(pair_returns_tibble) > 1) {
        pair_returns_tibble <- pair_returns_tibble %>% mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return)); pair_returns_xts <- xts::xts(pair_returns_tibble$strategy_return, order.by = pair_returns_tibble$date); colnames(pair_returns_xts) <- candidate_symbol
        pair_metrics <- calculate_safe_metrics(pair_returns_xts, pair_trade_data); individual_metrics_list[[candidate_symbol]] <- pair_metrics
        cat(" Annualized Return:", scales::percent(pair_metrics$AnnReturn, accuracy=0.01), "\n"); cat(" Annualized Std Dev:", scales::percent(pair_metrics$AnnStdDev, accuracy=0.01), "\n"); cat(" Cumulative Return:", scales::percent(pair_metrics$CumReturn, accuracy=0.01), "\n"); cat(" Win % (of active days):", scales::percent(pair_metrics$WinPerc, accuracy=0.1), "\n"); cat(" % Time In Market:", scales::percent(pair_metrics$PctInMarket, accuracy=0.1), "\n"); cat(" Annualized Sharpe (Rf=0%):", round(pair_metrics$Sharpe, 3), "\n"); cat(" Maximum Drawdown:", scales::percent(pair_metrics$MaxDD, accuracy=0.01), "\n"); cat(" Calmar Ratio:", round(pair_metrics$Calmar, 3), "\n")
        dd_table <- tryCatch(PerformanceAnalytics::table.Drawdowns(pair_returns_xts, top=5), error=function(e) NULL)
        if(!is.null(dd_table)) { cat(" Top 5 Drawdowns:\n"); print(dd_table); individual_drawdown_tables[[candidate_symbol]] <- dd_table } else { cat(" Could not calculate drawdown table.\n") }
        cat("\n")
    } else { cat(" Insufficient return data for", candidate_symbol, "\n\n") }
}
# --- 6c. Create Summary Table ---
summary_df <- tryCatch({bind_rows(lapply(individual_metrics_list, function(l) as.data.frame(l)), .id = "Candidate") %>% mutate(Pair = paste("BTC", Candidate, sep="-")) %>% select(Pair, AnnReturn, AnnStdDev, Sharpe, MaxDD, Calmar, WinPerc, PctInMarket, CumReturn) %>% arrange(desc(Sharpe))}, error = function(e) { cat("Error creating summary table:", e$message, "\n"); NULL })
if (!is.null(summary_df)){ cat("\n--- Performance Summary Table ---\n"); print(summary_df, digits=3) }


# --- 7. Plotting (Revised Strategy Plot) ---
# (Code unchanged - plots reflect updated backtest including regime filter effect)
cat("\n--- Step 7: Plotting Results ---\n")
plot_dir <- "strategy_plots"; if (!dir.exists(plot_dir)) dir.create(plot_dir)
cat("Plots will be saved to:", file.path(getwd(), plot_dir), "\n")
agg_plot_file <- file.path(plot_dir, "00_aggregate_performance.png"); png(agg_plot_file, width=800, height=1000); tryCatch({ PerformanceAnalytics::charts.PerformanceSummary(portfolio_returns_xts_agg, main = "Aggregate Strategy Performance", geometric = FALSE)}, error=function(e) plot.new()); dev.off()
btc_prices_backtest <- all_data_calculated_basic %>% filter(symbol == btc_symbol, date >= BACKTEST_START_DATE) %>% select(date, btc_price = price) # Use basic data for price plot
for (candidate_symbol in names(backtest_details)) {
    cat("Generating plots for BTC vs", candidate_symbol, "\n"); pair_plot_data <- backtest_details[[candidate_symbol]]$trade_data; pair_returns_data <- backtest_details[[candidate_symbol]]$returns_data
    # Plot 1: Spread + Z-Score + Entries + Regime Background
    trade_entries <- pair_plot_data %>% filter(abs(position) > SD_OFFSET & abs(lag(position, default = 0)) < SD_OFFSET) %>% mutate(entry_type = ifelse(position > 0, "Long Spread", "Short Spread"))
    plot_spread <- ggplot(pair_plot_data, aes(x = date, y = spread)) + geom_line(color="black") + labs(title = paste("BTC vs", candidate_symbol, ": Log Spread"), x = NULL, y = "Log Spread") + theme_minimal() + theme(plot.title = element_text(size=10))
    plot_zscore <- ggplot(pair_plot_data, aes(x = date, y = zscore)) +
        geom_rect(data = . %>% filter(market_regime == "High Risk"), aes(xmin=date, xmax=lead(date, default=max(date)+days(1)), ymin=-Inf, ymax=Inf), fill="grey80", alpha=0.3, inherit.aes = FALSE) +
        geom_line(color="black") + geom_hline(yintercept = c(ZSCORE_ENTRY_THRESHOLD, -ZSCORE_ENTRY_THRESHOLD), color = "red", linetype = "dashed") + geom_hline(yintercept = c(ZSCORE_EXIT_THRESHOLD, -ZSCORE_EXIT_THRESHOLD), color = "blue", linetype = "dotted") + geom_hline(yintercept = 0, color = "grey") +
        geom_point(data = trade_entries, aes(color = entry_type), size = 2) + scale_color_manual(values = c("Long Spread" = "darkgreen", "Short Spread" = "darkred")) +
        labs(title = paste("Z-Score & Entries (Win:", ROLLING_WINDOW, " Entry:", ZSCORE_ENTRY_THRESHOLD, " Exit:", ZSCORE_EXIT_THRESHOLD, ")"), x = "Date", y = "Z-Score", color = NULL) + theme_minimal() + theme(legend.position = "bottom", plot.title = element_text(size=10))
    pair_plot_file_spread <- file.path(plot_dir, paste0("01_spread_z_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_spread, gridExtra::grid.arrange(plot_spread, plot_zscore, ncol = 1, heights = c(1, 2)), width=10, height=6); cat(" Spread/Z plot saved:", pair_plot_file_spread, "\n") }, error = function(e){cat(" Could not save spread/z plot:", e$message, "\n")})
    # Plot 2: Strategy Results Plot (Style like example)
    plot_data_strategy <- pair_plot_data %>% select(date, position, market_regime) %>% inner_join(pair_returns_data, by="date") %>% inner_join(btc_prices_backtest, by="date") %>%
        mutate(strategy_return = ifelse(is.na(strategy_return), 0, strategy_return), CumEQ = STARTING_BALANCE * cumprod(1 + strategy_return),
               trade_signal = case_when( sign(position) != 0 & lag(sign(position), default=0) == 0 ~ sign(position), sign(position) == 0 & lag(sign(position), default=0) != 0 ~ -lag(sign(position)), TRUE ~ 0 ),
               pos_start = date, pos_end = lead(date, default = END_DATE + days(1)), position_plot = ifelse(abs(position) < SD_OFFSET, NA, position))
    p_price <- ggplot(plot_data_strategy, aes(x=date, y=btc_price)) + geom_line() + labs(y=NULL, x=NULL) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + ggtitle(paste("Strategy Results: BTC vs", candidate_symbol)) # Remove y-axis label
    p_trades <- ggplot(plot_data_strategy %>% filter(trade_signal != 0), aes(x=date, y=0)) + geom_point(aes(shape=factor(sign(trade_signal)), color=factor(sign(trade_signal))), size=2.0) + scale_shape_manual(values=c("1"=17, "-1"=17), guide="none") + scale_color_manual(values=c("1"="darkgreen", "-1"="darkred"), guide="none") + labs(y="Trades", x=NULL) + ylim(-1, 1) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
    p_pos <- ggplot(plot_data_strategy %>% filter(!is.na(position_plot))) + geom_rect(aes(xmin = pos_start, xmax = pos_end, ymin = 0, ymax = position_plot, fill = factor(sign(position_plot))), alpha=0.8) + geom_hline(yintercept=0) + scale_fill_manual(values=c("1"="blue", "-1"="lightblue"), guide="none") + labs(y="Position", x=NULL) + theme_minimal(base_size = 9) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + ylim(-MAX_POSITION_SCALAR*1.1, MAX_POSITION_SCALAR*1.1)
    p_cumeq <- ggplot(plot_data_strategy, aes(x=date, y=CumEQ)) + geom_line(color="blue") + labs(y="Equity", x="Date") + theme_minimal(base_size = 9) + scale_y_continuous(labels = scales::comma)
    pair_plot_file_strategy <- file.path(plot_dir, paste0("02_strategy_results_", candidate_symbol, ".png"))
    tryCatch({ ggsave(pair_plot_file_strategy, gridExtra::grid.arrange(p_price, p_trades, p_pos, p_cumeq, ncol=1, heights=c(3, 0.5, 1.5, 2)), width=8, height=6); cat(" Strategy results plot saved:", pair_plot_file_strategy, "\n") }, error=function(e){cat(" Could not save strategy plot:", e$message, "\n")})
}

# --- 8. Notes on Bitcoin Fundamental Value Strategy ---
# (Code unchanged)
cat("\n--- Step 8: Notes on Bitcoin Fundamental Value Strategy --- \n")#...rest of notes

# --- 9. Notes on Dynamic Portfolio Rebalancing ---
# (Code unchanged)
cat("\n--- Step 9: Notes on Dynamic Portfolio Rebalancing ---\n") #...rest of notes

cat("\n--- Analysis, Optimization, Backtesting, and Plotting Finished ---\n")
```
